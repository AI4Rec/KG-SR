分析文件夹: /root/9-10/KG-SR
输出文件: code.txt
分析时间: 2025-09-09 15:35:03
============================================================

环境信息:
- 生成时间: 2025-09-09 15:35:02
- Python: 3.9.23 on linux
- pip freeze (partial):
absl-py==2.3.1
aiosignal==1.4.0
anyio==4.10.0
argon2-cffi==25.1.0
argon2-cffi-bindings==25.1.0
arrow==1.3.0
asttokens==3.0.0
async-lru==2.0.5
attrs==25.3.0
babel==2.17.0
beautifulsoup4==4.13.5
bleach==6.2.0
certifi==2025.8.3
cffi==1.17.1
charset-normalizer==3.4.3
click==8.1.8
colorama==0.4.4
colorlog==4.7.2
comm==0.2.3
contourpy==1.3.0
cycler==0.12.1
debugpy==1.8.16
decorator==5.2.1
defusedxml==0.7.1
exceptiongroup==1.3.0
executing==2.2.1
fastjsonschema==2.21.2
filelock==3.13.1
fonttools==4.59.2
fqdn==1.5.1
frozenlist==1.7.0
fsspec==2024.6.1
grpcio==1.74.0
h11==0.16.0
hf-xet==1.1.9
httpcore==1.0.9
httpx==0.28.1
huggingface-hub==0.34.4
idna==3.10
importlib_metadata==8.7.0
importlib_resources==6.5.2
ipykernel==6.30.1
ipython==8.18.1
isoduration==20.11.0
jedi==0.19.2
Jinja2==3.1.4
joblib==1.5.2
json5==0.12.1
jsonpointer==3.0.0
jsonschema==4.25.1
jsonschema-specifications==2025.4.1
jupyter-events==0.12.0
jupyter-lsp==2.3.0
jupyter_client==8.6.3
jupyter_core==5.8.1
jupyter_server==2.17.0
jupyter_server_terminals==0.5.3
jupyterlab==4.4.7
jupyterlab_pygments==0.3.0
jupyterlab_server==2.27.3
kiwisolver==1.4.7
lark==1.2.2
Markdown==3.8.2
MarkupSafe==2.1.5
matplotlib==3.9.4
matplotlib-inline==0.1.7
mistune==3.1.4
mpmath==1.3.0
msgpack==1.1.1
narwhals==2.3.0
nbclient==0.10.2
nbconvert==7.16.6
nbformat==5.10.4
nest-asyncio==1.6.0
networkx==3.2.1
NEU-ipgw-manager==3.2
notebook==7.4.5
notebook_shim==0.2.4
numpy==2.0.2
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu12==9.1.0.70
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu12==12.1.0.106
nvidia-ml-py==12.575.51
nvidia-ml-py3==7.352.0
nvidia-nccl-cu12==2.21.5
nvidia-nvjitlink-cu12==12.9.86
nvidia-nvtx-cu12==12.1.105
overrides==7.7.0
packaging==25.0
pandas==2.3.2
pandocfilters==1.5.1
parso==0.8.5
patsy==1.0.1
pexpect==4.9.0
pillow==11.0.0
platformdirs==4.4.0
plotly==6.3.0
prometheus_client==0.22.1
prompt_toolkit==3.0.52
protobuf==6.32.0
psutil==7.0.0
ptyprocess==0.7.0
pure_eval==0.2.3
pycparser==2.22
Pygments==2.19.2
pynvml==12.0.0
pyparsing==3.2.3
python-dateutil==2.9.0.post0
python-json-logger==3.3.0
pytz==2025.2
PyYAML==6.0.2
pyzmq==27.0.2
ray==2.6.3
recbole==1.2.1
referencing==0.36.2
regex==2025.9.1
requests==2.32.5
rfc3339-validator==0.1.4
rfc3986-validator==0.1.1
rfc3987-syntax==1.1.0
rpds-py==0.27.1
safetensors==0.6.2
scikit-learn==1.6.1
scipy==1.13.1
Send2Trash==1.8.3
six==1.17.0
sniffio==1.3.1
soupsieve==2.8
stack-data==0.6.3
statsmodels==0.14.5
sympy==1.13.1
tabulate==0.9.0
tensorboard==2.20.0
tensorboard-data-server==0.7.2
terminado==0.18.1
texttable==1.7.0
thop==0.1.1.post2209072238
threadpoolctl==3.6.0
tinycss2==1.4.0
tokenizers==0.22.0
tomli==2.2.1
torch==2.5.1+cu121
torchaudio==2.5.1+cu121
torchvision==0.20.1+cu121
tornado==6.5.2
tqdm==4.67.1
traitlets==5.14.3
transformers==4.56.0
triton==3.1.0
types-python-dateutil==2.9.0.20250822
typing_extensions==4.12.2
tzdata==2025.2
uri-template==1.3.0
urllib3==2.5.0
wcwidth==0.2.13
webcolors==24.11.1
webencodings==0.5.1
websocket-client==1.8.0
Werkzeug==3.1.3
zipp==3.23.0
- git commit: 4433f75d877d1611c7e1d904179389672d0eca48
- git status: clean
- GPU: GPU 0: NVIDIA RTX A6000 (UUID: GPU-52e42290-a934-c527-006e-bf6dc2a2d5bb)
GPU 1: NVIDIA RTX A6000 (UUID: GPU-74d662ea-724c-110e-7513-8011e46ab9ff)
GPU 2: NVIDIA RTX A6000 (UUID: GPU-d39a616b-cf6f-f72b-7a83-63032b2db4cf)
GPU 3: NVIDIA RTX A6000 (UUID: GPU-7f4ddc8a-8ae2-6547-3300-278e0dd1d384)

============================================================
文件夹结构:
└── KG-SR
    ├── configs
    │   └── model
    │       ├── bert4rec_ml1m.yaml
    │       ├── gru4rec_ml1m.yaml
    │       ├── katrec_ml1m.yaml
    │       ├── ksr_ml1m.yaml
    │       └── sasrec_ml1m.yaml
    ├── scripts
    │   └── run_experiments.sh
    ├── src
    │   ├── candidates
    │   │   ├── __init__.py
    │   │   ├── build.py
    │   │   ├── predictors.py
    │   │   └── successors.py
    │   ├── datasets
    │   │   ├── __init__.py
    │   │   ├── dataloader.py
    │   │   ├── kg_dataloader.py
    │   │   └── preprocess.py
    │   ├── models
    │   │   ├── components
    │   │   │   ├── __init__.py
    │   │   │   ├── augment.py
    │   │   │   ├── init_utils.py
    │   │   │   ├── mask_utils.py
    │   │   │   └── transformer.py
    │   │   ├── __init__.py
    │   │   ├── bert4rec.py
    │   │   ├── gru4rec.py
    │   │   ├── katrec.py
    │   │   ├── ksr.py
    │   │   └── sasrec.py
    │   ├── trainers
    │   │   ├── __init__.py
    │   │   └── trainer.py
    │   ├── utils
    │   │   ├── __init__.py
    │   │   ├── code_snapshot.py
    │   │   ├── gpu_utils.py
    │   │   ├── logging_utils.py
    │   │   ├── memory_monitor.py
    │   │   ├── metrics.py
    │   │   └── path_finder.py
    │   └── __init__.py
    ├── .gitignore
    ├── folder_analysis_root.txt
    ├── folder_structure_analyzer.py
    └── main.py

============================================================
文件内容分析:
============================================================

============================================================
文件位置: configs/model/bert4rec_ml1m.yaml
============================================================
# 运行：python main.py --config configs/experiments/bert4rec_ml1m.yaml
# tmux new -s bert4rec_ml1m ; tmux attach -t bert4rec_ml1m
model_name: BERT4Rec
dataset_name: ml-1m
seed: 42

# ===== Data & Window =====
window_size: 5

# ===== Train =====
epochs: 200
batch_size: 256
learning_rate: 0.001
weight_decay: 0.0
patience: 20
num_workers: 4

# ===== Transformer (Bi-Encoder) =====
embedding_dim: 64
dropout_prob: 0.2
num_blocks: 2
num_attention_heads: 2

# ===== BERT4Rec-specific (兼容，不改 Trainer) =====
pooling: last     # 可选: last / mean

============================================================
文件位置: configs/model/gru4rec_ml1m.yaml
============================================================
# python main.py --config configs/model/gru4rec_ml1m.yaml
# tmux new -s gru4rec_ml1m 
# --- Model & Dataset ---
model_name: GRU4Rec
dataset_name: ml-1m
seed: 42

window_size: 50

epochs: 200
batch_size: 256
learning_rate: 0.001
weight_decay: 0.0
patience: 20

embedding_dim: 64
hidden_size: 64
num_layers: 1
dropout_prob: 0.2
num_workers: 4



============================================================
文件位置: configs/model/katrec_ml1m.yaml
============================================================
# configs/model/katrec_ml1m.yaml
# 运行：python main.py --config configs/model/katrec_ml1m.yaml
# tmux new -s katrec_ml1m tmux attach -t katrec_ml1m
model_name: KATRec
dataset_name: ml-1m
seed: 42

# ===== Data & Window =====
window_size: 5

# ===== Train =====
epochs: 200
batch_size: 256
learning_rate: 0.001
weight_decay: 0.0
patience: 20
num_workers: 4

# ===== Transformer (Seq Encoder) =====
embedding_dim: 64
dropout_prob: 0.2
num_blocks: 2
num_attention_heads: 2

# ===== KG / Graph Propagation =====
kg_embedding_dim: 64
n_gcn_layers: 2
mess_dropout_prob: 0.1
kg_attn_alpha: 0.2    # KG 注意力偏置的缩放系数（可调 0.1~0.5）

# ===== KGE Loss =====
kg_margin: 1.0
kg_loss_lambda: 0.1   # Trainer 里: loss = CE + kg_loss_lambda * KGE

# ===== Misc (兼容 main.py) =====
freeze_kg: false
gamma: 0.5
regs: [1e-5, 1e-5]




============================================================
文件位置: configs/model/ksr_ml1m.yaml
============================================================
# python main.py --config configs/model/ksr_ml1m.yaml
# tmux new -s ksr_ml1m tmux attach -t ksr_ml1m
model_name: KSR
dataset_name: ml-1m
seed: 42

window_size: 10

epochs: 200
batch_size: 256
learning_rate: 0.001
weight_decay: 0.0
patience: 20
num_workers: 4

embedding_dim: 64
dropout_prob: 0.2
hidden_size: 64
num_layers: 1
kg_embedding_size: 64
gamma: 0.5




============================================================
文件位置: configs/model/sasrec_ml1m.yaml
============================================================
# python main.py --config configs/model/sasrec_ml1m.yaml
# tmux new -s sasrec_ml1m tmux attach -t sasrec_ml1m
model_name: SASRec
dataset_name: ml-1m
seed: 42

window_size: 50

epochs: 200
batch_size: 256
learning_rate: 0.001
weight_decay: 0.0
patience: 20

embedding_dim: 64
dropout_prob: 0.2
num_blocks: 2
num_attention_heads: 2

============================================================
文件位置: folder_analysis_root.txt
============================================================
分析文件夹: /root/KG-SR-main
输出文件: /root/KG-SR-main/folder_analysis_root.txt
分析时间: 2025-09-08 10:59:40
============================================================

文件夹结构:
└── 
    ├── configs
    │   └── model
    │       ├── bert4rec_ml1m.yaml
    │       ├── gru4rec_ml1m.yaml
    │       ├── katrec_ml1m.yaml
    │       ├── ksr_ml1m.yaml
    │       └── sasrec_ml1m.yaml
    ├── dataset
    │   ├── processed
    │   │   └── ml-1m
    │   │       ├── L_5
    │   │       │   ├── test.csv
    │   │       │   ├── train.csv
    │   │       │   └── valid.csv
    │   │       ├── ml-1m.inter
    │   │       ├── ml-1m.item
    │   │       ├── ml-1m.kg
    │   │       ├── ml-1m.link
    │   │       └── ml-1m.user
    │   ├── raw
    │   │   └── ml-1m
    │   │       ├── ml-1m.inter
    │   │       ├── ml-1m.item
    │   │       ├── ml-1m.kg
    │   │       ├── ml-1m.link
    │   │       └── ml-1m.user
    │   └── .gitkeep
    ├── results
    │   └── experiment_results.csv
    ├── src
    │   ├── candidates
    │   │   ├── __init__.py
    │   │   ├── build.py
    │   │   ├── predictors.py
    │   │   └── successors.py
    │   ├── datasets
    │   │   ├── __init__.py
    │   │   ├── dataloader.py
    │   │   ├── kg_dataloader.py
    │   │   └── preprocess.py
    │   ├── models
    │   │   ├── components
    │   │   │   ├── __init__.py
    │   │   │   ├── augment.py
    │   │   │   ├── init_utils.py
    │   │   │   ├── mask_utils.py
    │   │   │   └── transformer.py
    │   │   ├── __init__.py
    │   │   ├── bert4rec.py
    │   │   ├── gru4rec.py
    │   │   ├── katrec.py
    │   │   ├── ksr.py
    │   │   └── sasrec.py
    │   ├── trainers
    │   │   ├── __init__.py
    │   │   └── trainer.py
    │   ├── utils
    │   │   ├── __init__.py
    │   │   ├── code_snapshot.py
    │   │   ├── gpu_utils.py
    │   │   ├── logging_utils.py
    │   │   ├── memory_monitor.py
    │   │   ├── metrics.py
    │   │   └── path_finder.py
    │   └── __init__.py
    ├── .gitignore
    ├── folder_analysis_root.txt
    ├── folder_structure_analyzer.py
    └── main.py


============================================================
文件内容分析:
============================================================

============================================================
文件位置: /root/KG-SR-main/configs/model/sasrec_ml1m.yaml
============================================================
# python main.py --config configs/model/sasrec_ml1m.yaml
# tmux new -s sasrec_ml1m tmux attach -t sasrec_ml1m
model_name: SASRec
dataset_name: ml-1m
seed: 42

window_size: 50

epochs: 200
batch_size: 256
learning_rate: 0.001
weight_decay: 0.0
patience: 20

embedding_dim: 64
dropout_prob: 0.2
num_blocks: 2
num_attention_heads: 2

============================================================
文件位置: /root/KG-SR-main/configs/model/katrec_ml1m.yaml
============================================================
# configs/model/katrec_ml1m.yaml
# 运行：python main.py --config configs/model/katrec_ml1m.yaml
# tmux new -s katrec_ml1m tmux attach -t katrec_ml1m
model_name: KATRec
dataset_name: ml-1m
seed: 42

# ===== Data & Window =====
window_size: 5

# ===== Train =====
epochs: 200
batch_size: 256
learning_rate: 0.001
weight_decay: 0.0
patience: 20
num_workers: 4

# ===== Transformer (Seq Encoder) =====
embedding_dim: 64
dropout_prob: 0.2
num_blocks: 2
num_attention_heads: 2

# ===== KG / Graph Propagation =====
kg_embedding_dim: 64
n_gcn_layers: 2
mess_dropout_prob: 0.1
kg_attn_alpha: 0.2    # KG 注意力偏置的缩放系数（可调 0.1~0.5）

# ===== KGE Loss =====
kg_margin: 1.0
kg_loss_lambda: 0.1   # Trainer 里: loss = CE + kg_loss_lambda * KGE

# ===== Misc (兼容 main.py) =====
freeze_kg: false
gamma: 0.5
regs: [1e-5, 1e-5]




============================================================
文件位置: /root/KG-SR-main/configs/model/gru4rec_ml1m.yaml
============================================================
# python main.py --config configs/model/gru4rec_ml1m.yaml
# tmux new -s gru4rec_ml1m 
# --- Model & Dataset ---
model_name: GRU4Rec
dataset_name: ml-1m
seed: 42

window_size: 50

epochs: 200
batch_size: 256
learning_rate: 0.001
weight_decay: 0.0
patience: 20

embedding_dim: 64
hidden_size: 64
num_layers: 1
dropout_prob: 0.2
num_workers: 4



============================================================
文件位置: /root/KG-SR-main/configs/model/bert4rec_ml1m.yaml
============================================================
# 运行：python main.py --config configs/experiments/bert4rec_ml1m.yaml
# tmux new -s bert4rec_ml1m ; tmux attach -t bert4rec_ml1m
model_name: BERT4Rec
dataset_name: ml-1m
seed: 42

# ===== Data & Window =====
window_size: 5

# ===== Train =====
epochs: 200
batch_size: 256
learning_rate: 0.001
weight_decay: 0.0
patience: 20
num_workers: 4

# ===== Transformer (Bi-Encoder) =====
embedding_dim: 64
dropout_prob: 0.2
num_blocks: 2
num_attention_heads: 2

# ===== BERT4Rec-specific (兼容，不改 Trainer) =====
pooling: last     # 可选: last / mean

============================================================
文件位置: /root/KG-SR-main/configs/model/ksr_ml1m.yaml
============================================================
# python main.py --config configs/model/ksr_ml1m.yaml
# tmux new -s ksr_ml1m tmux attach -t ksr_ml1m
model_name: KSR
dataset_name: ml-1m
seed: 42

window_size: 10

epochs: 200
batch_size: 256
learning_rate: 0.001
weight_decay: 0.0
patience: 20
num_workers: 4

embedding_dim: 64
dropout_prob: 0.2
hidden_size: 64
num_layers: 1
kg_embedding_size: 64
gamma: 0.5




============================================================
文件位置: /root/KG-SR-main/src/utils/__init__.py
============================================================



============================================================
文件位置: /root/KG-SR-main/src/utils/path_finder.py
============================================================
from __future__ import annotations
from dataclasses import dataclass
from pathlib import Path
from typing import NamedTuple, Optional, Tuple

class DataPaths(NamedTuple):
    base_processed: Path
    split_dir: Path
    train_csv: Path
    valid_csv: Path
    test_csv: Path
    processed_link: Path
    processed_user: Path
    raw_inter: Path
    raw_link: Path
    raw_item: Path
    raw_kg: Path

@dataclass(frozen=True)
class PathFinder:
    dataset_name: str
    window_size: int
    project_root: Optional[Path] = None

    def root(self) -> Path:
        if self.project_root is not None:
            return self.project_root.resolve()
        return Path('.').resolve()

    def data_dir(self) -> Path:
        return self.root() / 'dataset'

    def raw_dir(self) -> Path:
        return self.data_dir() / 'raw' / self.dataset_name

    def processed_dir(self) -> Path:
        return self.data_dir() / 'processed' / self.dataset_name

    def split_dir_path(self) -> Path:
        return self.processed_dir() / f'L_{self.window_size}'

    def raw_inter_path(self) -> Path:
        return self.raw_dir() / f'{self.dataset_name}.inter'

    def raw_link_path(self) -> Path:
        return self.raw_dir() / f'{self.dataset_name}.link'

    def raw_item_path(self) -> Path:
        return self.raw_dir() / f'{self.dataset_name}.item'

    def raw_kg_path(self) -> Path:
        return self.raw_dir() / f'{self.dataset_name}.kg'

    def processed_link_path(self) -> Path:
        return self.processed_dir() / f'{self.dataset_name}.link'

    def processed_user_path(self) -> Path:
        return self.processed_dir() / f'{self.dataset_name}.user'

    def train_valid_test_paths(self, use_corrected_train: bool=False, corrected_train_filename: str='corrected_train.csv') -> Tuple[Path, Path, Path]:
        split = self.split_dir_path()
        corrected = split / corrected_train_filename
        train_default = split / 'train.csv'
        train = corrected if use_corrected_train and corrected.exists() else train_default
        return (train, split / 'valid.csv', split / 'test.csv')

    def data_paths(self, use_corrected_train: bool=False, corrected_train_filename: str='corrected_train.csv', ensure_dirs: bool=True) -> DataPaths:
        base_processed = self.processed_dir()
        split_dir = self.split_dir_path()
        if ensure_dirs:
            base_processed.mkdir(parents=True, exist_ok=True)
            split_dir.mkdir(parents=True, exist_ok=True)
        (train_csv, valid_csv, test_csv) = self.train_valid_test_paths(use_corrected_train=use_corrected_train, corrected_train_filename=corrected_train_filename)
        return DataPaths(base_processed=base_processed, split_dir=split_dir, train_csv=train_csv, valid_csv=valid_csv, test_csv=test_csv, processed_link=self.processed_link_path(), processed_user=self.processed_user_path(), raw_inter=self.raw_inter_path(), raw_link=self.raw_link_path(), raw_item=self.raw_item_path(), raw_kg=self.raw_kg_path())

    def save_dir(self, model_name: str) -> Path:
        out = Path('./saved') / self.dataset_name / model_name / f'L_{self.window_size}'
        out.mkdir(parents=True, exist_ok=True)
        return out


============================================================
文件位置: /root/KG-SR-main/src/utils/memory_monitor.py
============================================================
import psutil
import torch
import os
import gc
from pathlib import Path

class MemoryMonitor:

    def __init__(self, config):
        self.config = config
        self.initial_memory = self.get_memory_usage()

    def get_memory_usage(self):
        mem = psutil.virtual_memory()
        info = {'total_gb': mem.total / 1024 ** 3, 'available_gb': mem.available / 1024 ** 3, 'used_gb': mem.used / 1024 ** 3, 'percent': mem.percent}
        if torch.cuda.is_available():
            total = torch.cuda.get_device_properties(0).total_memory
            info.update({'gpu_total_gb': total / 1024 ** 3, 'gpu_allocated_gb': torch.cuda.memory_allocated(0) / 1024 ** 3, 'gpu_cached_gb': torch.cuda.memory_reserved(0) / 1024 ** 3, 'gpu_free_gb': (total - torch.cuda.memory_reserved(0)) / 1024 ** 3})
        return info

    def print_memory_stats(self, stage=''):
        m = self.get_memory_usage()
        print(f'\n=== 内存使用 {stage} ===')
        print(f"系统: {m['used_gb']:.2f}/{m['total_gb']:.2f}GB ({m['percent']:.1f}%), 可用 {m['available_gb']:.2f}GB")
        if 'gpu_total_gb' in m:
            print(f"GPU: alloc {m['gpu_allocated_gb']:.2f}GB, cached {m['gpu_cached_gb']:.2f}GB, free {m['gpu_free_gb']:.2f}/{m['gpu_total_gb']:.2f}GB")

    def suggest_config_adjustments(self):
        m = self.get_memory_usage()
        tips = []
        if m['percent'] > 80:
            cur_bs = getattr(self.config, 'batch_size', 256)
            tips += ['系统内存紧张，建议：', f'- 减少 batch_size（当前 {cur_bs}）', '- 将 num_workers 调低为 0~1', '- 关闭 pin_memory', f'- 建议 batch_size ≈ {max(16, cur_bs // 2)}']
        if 'gpu_free_gb' in m and m['gpu_free_gb'] < 2.0:
            tips += ['GPU 内存紧张，建议：', '- 进一步减少 batch_size', '- 降低 embedding_dim', '- 使用 AMP 或梯度累积']
        return tips or ['内存使用正常']

    def auto_adjust_config(self):
        m = self.get_memory_usage()
        if m['percent'] > 80 and hasattr(self.config, 'batch_size'):
            orig = self.config.batch_size
            self.config.optimized_batch_size = max(16, orig // 2)
            self.config.num_workers = min(getattr(self.config, 'num_workers', 0), 1)
            print(f'自动调整: batch_size {orig} -> {self.config.optimized_batch_size}, num_workers -> {self.config.num_workers}')
        if 'gpu_free_gb' in m and m['gpu_free_gb'] < 2.0:
            cur = getattr(self.config, 'optimized_batch_size', getattr(self.config, 'batch_size', 256))
            self.config.optimized_batch_size = max(8, cur // 2)
            print(f'GPU 紧张: 进一步减少 batch_size -> {self.config.optimized_batch_size}')

def setup_memory_efficient_environment():
    torch.set_num_threads(min(4, os.cpu_count() or 4))
    if torch.cuda.is_available():
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
        torch.cuda.empty_cache()
    os.environ['OMP_NUM_THREADS'] = '2'
    os.environ['MKL_NUM_THREADS'] = '2'
    print('已应用内存优化环境变量')

def monitor_training_memory(model, train_loader, device, max_batches=5):
    print('\n=== 训练内存监控 ===')
    model.eval()
    with torch.no_grad():
        for (i, batch) in enumerate(train_loader):
            if i >= max_batches:
                break
            if torch.cuda.is_available():
                torch.cuda.synchronize()
                before = torch.cuda.memory_allocated() / 1024 ** 3
            seq = batch['sequence'].to(device, non_blocking=True)
            tgt = batch['target_id'].to(device, non_blocking=True)
            _ = model(seq)
            if torch.cuda.is_available():
                torch.cuda.synchronize()
                after = torch.cuda.memory_allocated() / 1024 ** 3
                print(f'Batch {i + 1}: ΔGPU {after - before:.3f}GB')
            del seq, tgt
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    print('内存监控完成')


============================================================
文件位置: /root/KG-SR-main/src/utils/metrics.py
============================================================
import torch

class AllRankMetrics:

    def __init__(self, top_k=10):
        self.top_k = top_k
        self.reset()

    def reset(self):
        self._total_hits = 0.0
        self._total_ndcgs = 0.0
        self._total_mrrs = 0.0
        self._count = 0

    def __call__(self, logits, targets):
        targets = targets.to(logits.device)
        target_scores = logits.gather(1, targets.view(-1, 1))
        ranks = (logits > target_scores).sum(dim=1) + 1
        hits_at_k = (ranks <= self.top_k).float()
        in_top_k_mask = (ranks <= self.top_k).float()
        ndcg_at_k = in_top_k_mask * (1.0 / torch.log2(ranks.float() + 1))
        mrr = 1.0 / ranks.float()
        self._total_hits += hits_at_k.sum().item()
        self._total_ndcgs += ndcg_at_k.sum().item()
        self._total_mrrs += mrr.sum().item()
        self._count += targets.size(0)

    def summary(self):
        if self._count == 0:
            return {f'Hit@{self.top_k}': 0, f'NDCG@{self.top_k}': 0, 'MRR': 0}
        avg_hit = self._total_hits / self._count
        avg_ndcg = self._total_ndcgs / self._count
        avg_mrr = self._total_mrrs / self._count
        return {f'Hit@{self.top_k}': avg_hit, f'NDCG@{self.top_k}': avg_ndcg, 'MRR': avg_mrr}


============================================================
文件位置: /root/KG-SR-main/src/utils/gpu_utils.py
============================================================
from __future__ import annotations
import os
import math
from typing import List, Dict, Optional, Tuple
import torch
__all__ = ['get_gpu_memory', 'select_device', 'describe_devices']

def _try_init_nvml():
    try:
        import pynvml
        pynvml.nvmlInit()
        return pynvml
    except Exception:
        return None

def _shutdown_nvml(pynvml_mod) -> None:
    try:
        if pynvml_mod is not None:
            pynvml_mod.nvmlShutdown()
    except Exception:
        pass

def _parse_visible_device_indices() -> Optional[List[int]]:
    cvd = os.environ.get('CUDA_VISIBLE_DEVICES', '').strip()
    if not cvd:
        return None
    parts = [p.strip() for p in cvd.split(',') if p.strip() != '']
    idxs: List[int] = []
    for p in parts:
        if p.isdigit():
            idxs.append(int(p))
        else:
            return None
    return idxs if idxs else None

def get_gpu_memory() -> List[Dict[str, float]]:
    if not torch.cuda.is_available():
        return []
    infos: List[Dict[str, float]] = []
    pynvml = _try_init_nvml()
    try:
        visible_phys = _parse_visible_device_indices()
        if pynvml is not None:
            if visible_phys is None:
                phys_indices = list(range(pynvml.nvmlDeviceGetCount()))
                visible_map = list(range(len(phys_indices)))
            else:
                phys_indices = visible_phys
                visible_map = list(range(len(phys_indices)))
            torch_visible_cnt = torch.cuda.device_count()
            phys_indices = phys_indices[:torch_visible_cnt]
            visible_map = visible_map[:torch_visible_cnt]
            for (torch_idx, phys_idx) in zip(visible_map, phys_indices):
                handle = pynvml.nvmlDeviceGetHandleByIndex(phys_idx)
                mem = pynvml.nvmlDeviceGetMemoryInfo(handle)
                total = float(mem.total)
                used = float(mem.used)
                free = float(mem.free)
                util_ratio = used / total if total > 0 else 1.0
                try:
                    ur = pynvml.nvmlDeviceGetUtilizationRates(handle)
                    gpu_util_pct = float(ur.gpu)
                    mem_ctrl_util_pct = float(ur.memory)
                except Exception:
                    gpu_util_pct = float('nan')
                    mem_ctrl_util_pct = float('nan')
                infos.append({'id': torch_idx, 'total': total, 'used': used, 'free': free, 'util': util_ratio, 'gpu_util_pct': gpu_util_pct, 'mem_ctrl_util_pct': mem_ctrl_util_pct})
        else:
            count = torch.cuda.device_count()
            for i in range(count):
                props = torch.cuda.get_device_properties(i)
                total = float(props.total_memory)
                try:
                    reserved = float(torch.cuda.memory_reserved(i))
                except Exception:
                    reserved = 0.0
                used = reserved
                free = max(0.0, total - reserved)
                util_ratio = used / total if total > 0 else 1.0
                infos.append({'id': i, 'total': total, 'used': used, 'free': free, 'util': util_ratio, 'gpu_util_pct': float('nan'), 'mem_ctrl_util_pct': float('nan')})
    finally:
        _shutdown_nvml(pynvml)
    return infos

def _gpu_util_for_sort(g: Dict[str, float]) -> float:
    v = g.get('gpu_util_pct', float('nan'))
    if isinstance(v, float) and math.isfinite(v):
        return v
    return float(g.get('util', 1.0)) * 100.0

def select_device(prefer: str='auto', strategy: str='min_gpu_util', min_free_mem_gb: float=1.0, allow_cpu: bool=True, explicit_id: Optional[int]=None, verbose: bool=True) -> torch.device:
    if prefer == 'cpu' or not torch.cuda.is_available():
        if verbose:
            print('[GPU-Select] Using CPU (prefer=cpu or CUDA unavailable).')
        return torch.device('cpu')
    if explicit_id is not None:
        if verbose:
            print(f'[GPU-Select] Using explicit cuda:{explicit_id}.')
        return torch.device(f'cuda:{explicit_id}')
    gpus = get_gpu_memory()
    if not gpus:
        if allow_cpu:
            if verbose:
                print('[GPU-Select] No visible GPUs. Falling back to CPU.')
            return torch.device('cpu')
        return torch.device('cuda:0')
    min_free_bytes = int(min_free_mem_gb * 1024 ** 3)
    candidates = [g for g in gpus if g['free'] >= min_free_bytes] or gpus
    strat = strategy.lower()
    if strat in ('max_free', 'max_free_mem'):
        best = max(candidates, key=lambda g: g['free'])
        strat_name = 'max_free'
    elif strat in ('min_gpu_util', 'min_volatile', 'least_busy'):
        best = min(candidates, key=_gpu_util_for_sort)
        strat_name = 'min_gpu_util'
    else:
        best = min(candidates, key=lambda g: g.get('util', 1.0))
        strat_name = 'min_util'
    device = torch.device(f"cuda:{int(best['id'])}")
    if verbose:
        tot = best['total'] / 1024 ** 3
        fre = best['free'] / 1024 ** 3
        usd = best['used'] / 1024 ** 3
        gpuu = best.get('gpu_util_pct', float('nan'))
        if math.isfinite(gpuu):
            extra = f', gpu_util={gpuu:.0f}%'
        else:
            extra = ''
        print(f"[GPU-Select] Strategy={strat_name} -> pick cuda:{int(best['id'])} (free={fre:.2f}GB, used={usd:.2f}GB, total={tot:.2f}GB{extra}).")
    return device

def describe_devices() -> str:
    infos = get_gpu_memory()
    if not infos:
        return 'No CUDA GPUs visible.'
    lines = ['Visible GPUs:']
    for g in infos:
        free_gb = g['free'] / 1024 ** 3
        tot_gb = g['total'] / 1024 ** 3
        used_gb = g['used'] / 1024 ** 3
        util_pct = g.get('gpu_util_pct', float('nan'))
        memc_pct = g.get('mem_ctrl_util_pct', float('nan'))
        util_str = f'{util_pct:.0f}%' if math.isfinite(util_pct) else 'N/A'
        memc_str = f'{memc_pct:.0f}%' if math.isfinite(memc_pct) else 'N/A'
        lines.append(f"  cuda:{int(g['id'])}: free {free_gb:.2f}GB / total {tot_gb:.2f}GB (used {used_gb:.2f}GB), GPU-Util {util_str}, MemCtrl {memc_str}")
    return '\n'.join(lines)


============================================================
文件位置: /root/KG-SR-main/src/utils/logging_utils.py
============================================================
import logging
from pathlib import Path
from typing import Optional

def setup_logging(level: str='INFO', file_path: Optional[Path]=None, name: str='train'):
    logger = logging.getLogger(name)
    if logger.handlers:
        for h in list(logger.handlers):
            logger.removeHandler(h)
    logger.propagate = False
    logger.setLevel(getattr(logging, level.upper(), logging.INFO))
    formatter = logging.Formatter(fmt='%(asctime)s [%(levelname)s] %(name)s: %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
    ch = logging.StreamHandler()
    ch.setFormatter(formatter)
    logger.addHandler(ch)
    if file_path is not None:
        fh = logging.FileHandler(file_path)
        fh.setFormatter(formatter)
        logger.addHandler(fh)
    return logger


============================================================
文件位置: /root/KG-SR-main/src/utils/code_snapshot.py
============================================================
from __future__ import annotations
import os, sys, subprocess
from pathlib import Path
from datetime import datetime
from typing import Iterable
DEFAULT_INCLUDE_EXTS = {'.py', '.yaml', '.yml', '.sh', '.txt', '.md', '.cfg', '.ini'}
DEFAULT_PARTIAL_TEXT_EXTS = {'.csv', '.tsv', '.json', '.log', '.inter', '.kg', '.link'}
DEFAULT_EXCLUDE_DIRS = {'.git', '.idea', '.vscode', '__pycache__', '.pytest_cache', '.mypy_cache', 'dataset', 'saved', 'results', 'wandb', 'logs', '.DS_Store'}
DEFAULT_BINARY_EXTS = {'.pth', '.tar', '.pkl', '.npy', '.npz', '.pyc', '.so', '.dll', '.exe', '.bin', '.dat'}

def _is_binary_path(p: Path) -> bool:
    suf = ''.join(p.suffixes).lower()
    if suf.endswith('.pth.tar') or suf.endswith('.pkl.gz'):
        return True
    return p.suffix.lower() in DEFAULT_BINARY_EXTS

def _try_read_bytes(p: Path):
    try:
        return p.read_bytes()
    except Exception:
        return None

def _decode_text(b: bytes) -> str:
    for enc in ('utf-8', 'utf-16', 'gbk', 'gb2312', 'latin-1'):
        try:
            return b.decode(enc)
        except Exception:
            pass
    return b.decode('utf-8', errors='replace')

def _iter_project_files(root: Path, include_exts, partial_text_exts, exclude_dirs) -> Iterable[Path]:
    for (dirpath, dirnames, filenames) in os.walk(root):
        dirnames[:] = sorted([d for d in dirnames if d not in exclude_dirs and (not d.startswith('.')) or d == 'configs'], key=str.lower)
        for fname in sorted(filenames, key=str.lower):
            p = Path(dirpath) / fname
            if p.name == 'main.py' or 'configs' in p.parts:
                yield p
                continue
            suf = p.suffix.lower()
            if suf in include_exts or suf in partial_text_exts:
                yield p

def _build_tree(root: Path, exclude_dirs) -> str:
    lines = ['文件夹结构:', f"└── {root.name or '/'}"]

    def kids(path: Path):
        try:
            d = [c for c in path.iterdir() if c.is_dir() and c.name not in exclude_dirs and (not c.name.startswith('.'))]
            f = [c for c in path.iterdir() if c.is_file()]
            return (sorted(d, key=lambda x: x.name.lower()), sorted(f, key=lambda x: x.name.lower()))
        except PermissionError:
            return ([], [])

    def walk(path: Path, prefix: str):
        (d, f) = kids(path)
        all_ = d + f
        for (i, ch) in enumerate(all_):
            last = i == len(all_) - 1
            lines.append(f"{prefix}{('└── ' if last else '├── ')}{ch.name}")
            if ch.is_dir():
                walk(ch, prefix + ('    ' if last else '│   '))
    walk(root, '    ')
    return '\n'.join(lines)

def _gather_env_info(project_root: Path) -> str:
    info = ['环境信息:', f"- 生成时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", f'- Python: {sys.version.split()[0]} on {sys.platform}']
    try:
        out = subprocess.check_output([sys.executable, '-m', 'pip', 'freeze'], stderr=subprocess.STDOUT, timeout=10)
        info += ['- pip freeze (partial):', _decode_text(out)[:5000].strip()]
    except Exception:
        info.append('- pip freeze: N/A')
    try:
        commit = subprocess.check_output(['git', 'rev-parse', 'HEAD'], cwd=project_root, stderr=subprocess.STDOUT, timeout=3)
        status = subprocess.check_output(['git', 'status', '--porcelain'], cwd=project_root, stderr=subprocess.STDOUT, timeout=3)
        info.append(f'- git commit: {_decode_text(commit).strip()}')
        info.append(f"- git status: {('clean' if not _decode_text(status).strip() else 'dirty')}")
    except Exception:
        info.append('- git: N/A')
    try:
        out = subprocess.check_output(['nvidia-smi', '-L'], stderr=subprocess.STDOUT, timeout=2)
        info.append('- GPU: ' + _decode_text(out).strip())
    except Exception:
        info.append('- GPU: N/A')
    return '\n'.join(info)

def write_code_snapshot(save_dir: Path, project_root: Path | None=None, include_exts=None, partial_text_exts=None, exclude_dirs=None, max_bytes_per_file: int=1000000, partial_head_lines: int=50) -> Path:
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    out_path = save_dir / 'code.txt'
    project_root = Path(project_root) if project_root else Path.cwd()
    include_exts = include_exts or DEFAULT_INCLUDE_EXTS
    partial_text_exts = partial_text_exts or DEFAULT_PARTIAL_TEXT_EXTS
    exclude_dirs = exclude_dirs or DEFAULT_EXCLUDE_DIRS
    files = sorted(_iter_project_files(project_root, include_exts, partial_text_exts, exclude_dirs), key=lambda p: '/'.join(p.relative_to(project_root).parts).lower())
    tree_text = _build_tree(project_root, exclude_dirs)
    env_text = _gather_env_info(project_root)
    with out_path.open('w', encoding='utf-8', errors='replace') as f:
        f.write(f'分析文件夹: {project_root.resolve()}\n')
        f.write(f'输出文件: {out_path.name}\n')
        f.write(f"分析时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write('=' * 60 + '\n\n')
        f.write(env_text + '\n')
        f.write('\n' + '=' * 60 + '\n')
        f.write(tree_text + '\n')
        f.write('\n' + '=' * 60 + '\n')
        f.write('文件内容分析:\n')
        f.write('=' * 60 + '\n')
        for fp in files:
            if _is_binary_path(fp):
                continue
            rel = fp.relative_to(project_root)
            f.write('\n' + '=' * 60 + '\n')
            f.write(f'文件位置: {rel.as_posix()}\n')
            f.write('=' * 60 + '\n')
            raw = _try_read_bytes(fp)
            if raw is None:
                f.write('无法读取文件内容\n')
                continue
            truncated = False
            if len(raw) > max_bytes_per_file:
                raw = raw[:max_bytes_per_file]
                truncated = True
            text = _decode_text(raw)
            suf = fp.suffix.lower()
            if suf in include_exts:
                if truncated:
                    f.write('[TRUNCATED]\n')
                f.write(text + '\n')
            else:
                lines = text.splitlines()
                for (i, ln) in enumerate(lines[:partial_head_lines], 1):
                    f.write(f'{i:3d}: {ln}\n')
                if len(lines) > partial_head_lines:
                    f.write(f'... (文件共 {len(lines)} 行，仅显示前{partial_head_lines}行)\n')
        f.write('\n' + '=' * 60 + '\n')
        f.write('分析完成!\n')
        f.write(f'结果已保存到: {out_path.name}\n')
    return out_path


============================================================
文件位置: /root/KG-SR-main/src/__init__.py
============================================================



============================================================
文件位置: /root/KG-SR-main/src/trainers/__init__.py
============================================================



============================================================
文件位置: /root/KG-SR-main/src/trainers/trainer.py
============================================================
import torch
import torch.nn as nn
from tqdm import tqdm
from pathlib import Path
import yaml
import gc
import time
import pandas as pd
import random
from collections import defaultdict
from src.utils.metrics import AllRankMetrics
from src.utils.memory_monitor import setup_memory_efficient_environment, MemoryMonitor
from src.utils.logging_utils import setup_logging
from src.utils.gpu_utils import select_device

class Trainer:

    def __init__(self, model, train_loader, valid_loader, test_loader, config):
        setup_memory_efficient_environment()
        self.memory_monitor = MemoryMonitor(config)
        self.memory_monitor.print_memory_stats('Trainer initialization')
        self.config = config
        self.save_dir = Path(config.save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        self.log_file = self.save_dir / 'train.log'
        self.logger = setup_logging(level=getattr(config, 'log_level', 'INFO'), file_path=self.log_file, name='train')
        self.use_amp = bool(getattr(config, 'use_amp', False) and torch.cuda.is_available())
        self.allow_fast_kernels = bool(getattr(config, 'allow_fast_kernels', False) and torch.cuda.is_available())
        if torch.cuda.is_available():
            try:
                torch.backends.cuda.matmul.allow_tf32 = self.allow_fast_kernels
                torch.backends.cudnn.allow_tf32 = self.allow_fast_kernels
            except Exception:
                pass
            try:
                torch.backends.cuda.sdp_kernel(enable_flash=self.allow_fast_kernels, enable_mem_efficient=self.allow_fast_kernels, enable_math=not self.allow_fast_kernels)
            except Exception:
                pass
        self.scaler = torch.cuda.amp.GradScaler(enabled=self.use_amp)
        self.device = select_device(prefer=getattr(config, 'device_prefer', 'auto'), strategy=getattr(config, 'gpu_select', 'max_free'), min_free_mem_gb=float(getattr(config, 'min_free_mem_gb', 0.5)), allow_cpu=True, explicit_id=getattr(config, 'gpu_id') if hasattr(config, 'gpu_id') else None, verbose=True)
        self.logger.info(f'Using device: {self.device}')
        self.model = model.to(self.device)
        self.train_loader = train_loader
        self.valid_loader = valid_loader
        self.test_loader = test_loader
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)
        self.criterion = nn.CrossEntropyLoss()
        top_k = int(getattr(config, 'top_k', 10))
        self.metrics_calculator = AllRankMetrics(top_k=top_k)
        self.grad_clip = float(getattr(config, 'grad_clip', 1.0))
        self.best_metric = -1
        self.epochs_no_improve = 0
        self._prepare_eval_protocol_masks()
        if self.config.model_name.lower() == 'katrec':
            self._setup_katrec_training()

    def _safe_config_dict(self):
        blacklist = {'adj_matrix', 'item_relation_map_tensor', 'item_entity_map'}
        cfg = {}
        for (k, v) in vars(self.config).items():
            if k in blacklist:
                continue
            try:
                yaml.safe_dump({k: v})
                cfg[k] = v
            except Exception:
                cfg[k] = f'<non-serializable:{type(v).__name__}>'
        return cfg

    def _prepare_eval_protocol_masks(self):
        df = self.train_loader.dataset.data_df
        num_items = int(self.config.num_items)
        valid_items_set = set()
        user_seen = defaultdict(set)
        for (_, row) in df.iterrows():
            uid = int(row['user_id'])
            seq_str = row['sequence']
            if isinstance(seq_str, str):
                for tok in seq_str.split(','):
                    if not tok:
                        continue
                    it = int(tok)
                    if it > 0:
                        valid_items_set.add(it)
                        user_seen[uid].add(it)
            tgt = int(row['target_id'])
            if tgt > 0:
                valid_items_set.add(tgt)
                user_seen[uid].add(tgt)
        valid_flag = torch.zeros(num_items, dtype=torch.bool)
        for it in valid_items_set:
            if 1 <= it <= num_items:
                valid_flag[it - 1] = True
        invalid_items_mask_1d = ~valid_flag
        self._invalid_items_mask_1d = invalid_items_mask_1d.to(self.device)
        max_uid = int(df['user_id'].max()) if len(df) > 0 else -1
        self._user_seen_idx = [torch.empty(0, dtype=torch.long, device=self.device) for _ in range(max_uid + 1)]
        for (uid, items) in user_seen.items():
            idxs = [it - 1 for it in items if 1 <= it <= num_items]
            if idxs:
                self._user_seen_idx[uid] = torch.as_tensor(sorted(set(idxs)), dtype=torch.long, device=self.device)
        self._log(f'[EvalProto] valid_items={len(valid_items_set)}, users={len(self._user_seen_idx)} ready.')

    def _setup_katrec_training(self):
        self.logger.info('Setting up data for KATRec KG training...')
        from src.datasets.kg_dataloader import get_kg_data
        kg_data = get_kg_data(self.config)
        kg_df = kg_data['kg_triples_df']
        self.kg_triples = kg_df[['head', 'relation', 'tail']].astype(int).values
        self.num_entities = kg_data['num_entities']
        self.kg_dict = defaultdict(list)
        for (h, r, t) in self.kg_triples:
            self.kg_dict[h, r].append(t)
        self.logger.info(f'Prepared {len(self.kg_triples)} triples for KG loss calculation.')

    def _sample_kg_batch(self, batch_size):
        indices = torch.randint(0, len(self.kg_triples), (batch_size,))
        batch_triples = self.kg_triples[indices]
        (h, r, pos_t) = (batch_triples[:, 0], batch_triples[:, 1], batch_triples[:, 2])
        neg_t = []
        for i in range(batch_size):
            (head, rel) = (h[i], r[i])
            tries = 0
            while True:
                neg_tail_candidate = random.randint(0, self.num_entities - 1)
                if neg_tail_candidate not in self.kg_dict[head, rel]:
                    neg_t.append(neg_tail_candidate)
                    break
                tries += 1
                if tries > 50:
                    neg_t.append(neg_tail_candidate)
                    break
        return (torch.LongTensor(h).to(self.device), torch.LongTensor(r).to(self.device), torch.LongTensor(pos_t).to(self.device), torch.LongTensor(neg_t).to(self.device))

    def _log(self, message):
        print(message)
        self.logger.info(message)

    def _save_checkpoint(self, epoch, is_best=False):
        state = {'epoch': epoch, 'model_state_dict': self.model.state_dict(), 'optimizer_state_dict': self.optimizer.state_dict(), 'best_metric': self.best_metric, 'config': self._safe_config_dict()}
        if is_best:
            best_filename = self.save_dir / 'best_model.pth.tar'
            torch.save(state, best_filename)
            self._log(f'-> Found new best model on validation set, saving to {best_filename}')
        else:
            pass

    def _save_results_to_csv(self, final_metrics, best_epoch, duration_str):
        results_dir = Path('./results')
        results_dir.mkdir(parents=True, exist_ok=True)
        results_file = results_dir / 'experiment_results.csv'
        results_data = {'timestamp': pd.to_datetime('now').strftime('%Y-%m-%d %H:%M:%S'), 'model_name': self.config.model_name, 'dataset_name': self.config.dataset_name, f'hit@{self.metrics_calculator.top_k}': final_metrics.get(f'Hit@{self.metrics_calculator.top_k}', -1), f'ndcg@{self.metrics_calculator.top_k}': final_metrics.get(f'NDCG@{self.metrics_calculator.top_k}', -1), 'mrr': final_metrics.get('MRR', -1), 'best_epoch': best_epoch, 'total_epochs': self.config.epochs, 'training_time': duration_str, 'learning_rate': self.config.learning_rate, 'batch_size': getattr(self.config, 'optimized_batch_size', self.config.batch_size), 'embedding_dim': self.config.embedding_dim, 'dropout_prob': self.config.dropout_prob, 'window_size': self.config.window_size, 'save_dir': str(self.save_dir)}
        df_results = pd.DataFrame([results_data])
        if not results_file.exists():
            df_results.to_csv(results_file, index=False, header=True)
        else:
            df_results.to_csv(results_file, mode='a', index=False, header=False)

    def _train_epoch(self, epoch):
        self.model.train()
        (total_loss, total_rec_loss, total_kg_loss) = (0.0, 0.0, 0.0)
        progress_bar = tqdm(self.train_loader, desc=f'Epoch {epoch + 1}/{self.config.epochs} [Training]')
        for batch in progress_bar:
            sequences = batch['sequence'].to(self.device, non_blocking=True)
            targets = batch['target_id'].to(self.device, non_blocking=True)
            targets_0 = torch.clamp_min(targets, 1) - 1
            self.optimizer.zero_grad(set_to_none=True)
            if self.use_amp:
                with torch.cuda.amp.autocast():
                    logits = self.model(sequences)
                    rec_loss = self.criterion(logits, targets_0)
                    if self.config.model_name.lower() == 'katrec':
                        (h, r, pos_t, neg_t) = self._sample_kg_batch(sequences.size(0))
                        kg_loss = self.model.calculate_kg_loss(h, r, pos_t, neg_t)
                        loss = rec_loss + self.config.kg_loss_lambda * kg_loss
                    else:
                        kg_loss = torch.tensor(0.0, device=self.device)
                        loss = rec_loss
                self.scaler.scale(loss).backward()
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.grad_clip)
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                logits = self.model(sequences)
                rec_loss = self.criterion(logits, targets_0)
                if self.config.model_name.lower() == 'katrec':
                    (h, r, pos_t, neg_t) = self._sample_kg_batch(sequences.size(0))
                    kg_loss = self.model.calculate_kg_loss(h, r, pos_t, neg_t)
                    loss = rec_loss + self.config.kg_loss_lambda * kg_loss
                else:
                    kg_loss = torch.tensor(0.0, device=self.device)
                    loss = rec_loss
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.grad_clip)
                self.optimizer.step()
            total_loss += loss.item()
            total_rec_loss += rec_loss.item()
            total_kg_loss += kg_loss.item()
            postfix = {'loss': f'{loss.item():.4f}', 'rec': f'{rec_loss.item():.4f}'}
            if self.config.model_name.lower() == 'katrec':
                postfix['kg'] = f'{kg_loss.item():.4f}'
            progress_bar.set_postfix(postfix)
        avg_loss = total_loss / len(self.train_loader)
        avg_rec_loss = total_rec_loss / len(self.train_loader)
        avg_kg_loss = total_kg_loss / len(self.train_loader)
        log_msg = f'Epoch {epoch + 1} Training -> Avg Loss: {avg_loss:.4f} (Rec: {avg_rec_loss:.4f}'
        if self.config.model_name.lower() == 'katrec':
            log_msg += f', KG: {avg_kg_loss:.4f})'
        else:
            log_msg += ')'
        self._log(log_msg)

    def _evaluate(self, epoch, loader, phase='Validation'):
        if hasattr(self.model, 'precompute_kg'):
            try:
                self.model.precompute_kg(self.device)
            except Exception:
                pass
        self.model.eval()
        self.metrics_calculator.reset()
        progress_bar = tqdm(loader, desc=f'Epoch {epoch + 1}/{self.config.epochs} [{phase}]')
        with torch.inference_mode():
            for batch in progress_bar:
                sequences = batch['sequence'].to(self.device, non_blocking=True)
                targets = batch['target_id']
                targets_0 = torch.clamp_min(targets, 1) - 1
                logits = self.model(sequences)
                num_items = logits.size(1)
                base_mask = self._invalid_items_mask_1d.view(1, -1).expand(sequences.size(0), -1).clone()
                if hasattr(self, '_user_seen_idx') and self._user_seen_idx:
                    uids = batch['user_id'].tolist()
                    for (i, uid) in enumerate(uids):
                        if 0 <= uid < len(self._user_seen_idx):
                            seen_idx = self._user_seen_idx[uid]
                            if seen_idx.numel() > 0:
                                base_mask[i, seen_idx] = True
                gt = targets_0.to(logits.device).long()
                valid = (gt >= 0) & (gt < num_items)
                if valid.any():
                    bs_idx = torch.arange(gt.size(0), device=logits.device)[valid]
                    base_mask[bs_idx, gt[valid]] = False
                logits = logits.masked_fill(base_mask, float('-inf')).detach().cpu()
                keep = targets > 0
                if keep.any():
                    self.metrics_calculator(logits[keep], targets_0[keep].to(torch.long))
        metrics = self.metrics_calculator.summary()
        k = self.metrics_calculator.top_k
        log_str = f"Epoch {epoch + 1} {phase} Results -> Hit@{k}: {metrics[f'Hit@{k}']:.4f}, NDCG@{k}: {metrics[f'NDCG@{k}']:.4f}, MRR: {metrics['MRR']:.4f}"
        self._log(log_str)
        return metrics

    def fit(self):
        self._log('=' * 20 + ' Starting Training ' + '=' * 20)
        startup_cfg = getattr(self.config, '_startup_config', None)
        to_dump = startup_cfg if isinstance(startup_cfg, dict) else self._safe_config_dict()
        cfg_path = self.save_dir / 'config.yaml'
        with open(cfg_path, 'w', encoding='utf-8') as f:
            yaml.safe_dump(to_dump, f, default_flow_style=False, sort_keys=False, allow_unicode=True)
        start_time = time.time()
        best_epoch_num = 0
        try:
            for epoch in range(self.config.epochs):
                self._train_epoch(epoch)
                metrics = self._evaluate(epoch, loader=self.valid_loader, phase='Validation')
                current_metric = metrics[f'NDCG@{self.metrics_calculator.top_k}']
                if current_metric > self.best_metric:
                    self.best_metric = current_metric
                    self.epochs_no_improve = 0
                    best_epoch_num = epoch + 1
                    self._save_checkpoint(epoch, is_best=True)
                else:
                    self.epochs_no_improve += 1
                    self._log(f'-> No improvement in {self.epochs_no_improve} epochs.')
                    self._save_checkpoint(epoch, is_best=False)
                if self.epochs_no_improve >= self.config.patience:
                    self._log(f'Early stopping triggered after {self.config.patience} epochs.')
                    break
        finally:
            gc.collect()
        duration_str = time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))
        self._log(f'Total Training Time: {duration_str}')
        self._log('=' * 20 + ' Training Finished ' + '=' * 20)
        self._log('\n' + '=' * 20 + ' Final Evaluation on Test Set ' + '=' * 20)
        best_model_path = self.save_dir / 'best_model.pth.tar'
        if best_model_path.exists():
            checkpoint = torch.load(best_model_path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self._log(f'Loaded best model from epoch {best_epoch_num}')
            if hasattr(self.model, 'precompute_kg'):
                try:
                    self.model.precompute_kg(self.device)
                except Exception:
                    pass
            final_metrics = self._evaluate(checkpoint['epoch'], loader=self.test_loader, phase='Testing')
            self._save_results_to_csv(final_metrics, best_epoch_num, duration_str)
        else:
            self._log('No best model found to evaluate.')


============================================================
文件位置: /root/KG-SR-main/src/datasets/__init__.py
============================================================



============================================================
文件位置: /root/KG-SR-main/src/datasets/kg_dataloader.py
============================================================
import pandas as pd
from pathlib import Path
import torch
from collections import defaultdict
import numpy as np
import scipy.sparse as sp
from tqdm import tqdm
from src.utils.path_finder import PathFinder

def get_adj_matrix(dataset_name, num_users, num_entities, item_entity_map, kg_df):
    print('--- Building Adjacency Matrix for KATRec ---')
    t1 = pd.Timestamp.now()
    adj_mat = sp.lil_matrix((num_users + num_entities, num_users + num_entities), dtype=np.float32)
    inter_path = PathFinder(dataset_name, window_size=0).processed_dir() / f'{dataset_name}.inter'
    inter_df = pd.read_csv(inter_path, sep='\t', usecols=[0, 1], names=['user_id', 'item_id'], header=0)
    inter_df['entity_id'] = inter_df['item_id'].map(item_entity_map)
    inter_df.dropna(subset=['entity_id'], inplace=True)
    inter_df['entity_id'] = inter_df['entity_id'].astype(int)
    rows = inter_df['user_id'].values
    cols = inter_df['entity_id'].values + num_users
    adj_mat[rows, cols] = 1
    adj_mat[cols, rows] = 1
    print(f'Added {len(inter_df)} user-item interactions to matrix.')
    rows = kg_df['head'].values.astype(int) + num_users
    cols = kg_df['tail'].values.astype(int) + num_users
    adj_mat[rows, cols] = 1
    adj_mat[cols, rows] = 1
    print(f'Added {len(kg_df)} KG triples to matrix.')
    adj_mat = adj_mat.tocsr()
    rowsum = np.array(adj_mat.sum(axis=1))
    d_inv_sqrt = np.power(rowsum, -0.5).flatten()
    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.0
    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)
    norm_adj = adj_mat.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()
    indices = torch.from_numpy(np.vstack((norm_adj.row, norm_adj.col))).long()
    values = torch.from_numpy(norm_adj.data)
    shape = torch.Size(norm_adj.shape)
    norm_adj_tensor = torch.sparse.FloatTensor(indices, values, shape)
    t2 = pd.Timestamp.now()
    print(f'Adjacency matrix created and normalized in {(t2 - t1).total_seconds():.2f}s. Shape: {norm_adj.shape}')
    return norm_adj_tensor

def get_kg_data(config):
    dataset_name = config.dataset_name
    pf = PathFinder(dataset_name, getattr(config, 'window_size', 50))
    processed_dir = pf.processed_dir()
    link_path = processed_dir / f'{dataset_name}.link'
    user_path = processed_dir / f'{dataset_name}.user'
    kg_path = processed_dir / f'{dataset_name}.kg'
    if not link_path.exists() or not user_path.exists() or (not kg_path.exists()):
        raise FileNotFoundError(f'Required files not found. Please run preprocess.py first. Missing one of: {link_path}, {user_path}, {kg_path}')
    print('--- Loading KG and Link/User data ---')
    user_df = pd.read_csv(user_path, sep='\t')
    if 'user_id:token' in user_df.columns:
        num_users = int(user_df.shape[0])
    else:
        num_users = int(user_df.iloc[:, 0].nunique())
    link_df = pd.read_csv(link_path, sep='\t', dtype={'item_id:token': 'int32', 'entity_id:token': 'string'})
    inferred_num_items = int(link_df['item_id:token'].max())
    num_items_local = int(getattr(config, 'num_items', inferred_num_items))
    if num_items_local < inferred_num_items:
        print(f'[KG] Warning: config.num_items ({num_items_local}) < inferred ({inferred_num_items}). Using inferred value.')
        num_items_local = inferred_num_items
        try:
            setattr(config, 'num_items', num_items_local)
        except Exception:
            pass
    kg_df_raw = pd.read_csv(kg_path, sep='\t')
    kg_df_raw.columns = ['head', 'relation', 'tail']
    all_entities_str = sorted(list(set(kg_df_raw['head']).union(set(kg_df_raw['tail']))))
    entity_map = {e: i for (i, e) in enumerate(all_entities_str)}
    all_relations_str = sorted(list(set(kg_df_raw['relation'])))
    relation_map = {r: i + 1 for (i, r) in enumerate(all_relations_str)}
    num_entities = len(entity_map)
    num_relations = len(relation_map) + 1
    item_id_to_entity_str = pd.Series(link_df['entity_id:token'].values, index=link_df['item_id:token'].values).to_dict()
    item_entity_map = {int(item_id): entity_map[ent_str] for (item_id, ent_str) in item_id_to_entity_str.items() if isinstance(ent_str, str) and ent_str in entity_map}
    kg_df_mapped = kg_df_raw.copy()
    kg_df_mapped['head'] = kg_df_mapped['head'].map(entity_map)
    kg_df_mapped['relation'] = kg_df_mapped['relation'].map(relation_map)
    kg_df_mapped['tail'] = kg_df_mapped['tail'].map(entity_map)
    kg_df_mapped.dropna(inplace=True)
    kg_df_mapped[['head', 'relation', 'tail']] = kg_df_mapped[['head', 'relation', 'tail']].astype(int)
    R_MAX = getattr(config, 'max_relations_per_item', 8)
    if R_MAX < 1:
        R_MAX = 1
    head_rel_counts = kg_df_mapped.groupby(['head', 'relation']).size().reset_index(name='cnt')
    rel_list_by_head = {}
    for (head, grp) in head_rel_counts.groupby('head'):
        grp_sorted = grp.sort_values('cnt', ascending=False)
        rel_list_by_head[head] = grp_sorted['relation'].tolist()
    item_relation_map_tensor = torch.zeros(num_items_local, R_MAX, dtype=torch.long)
    filled_items = 0
    for (item_id_1, ent_id) in item_entity_map.items():
        zero_idx = int(item_id_1) - 1
        if zero_idx < 0 or zero_idx >= num_items_local:
            continue
        if ent_id < 0 or ent_id >= num_entities:
            continue
        rels = rel_list_by_head.get(ent_id, [])
        if not rels:
            continue
        rels = rels[:R_MAX]
        item_relation_map_tensor[zero_idx, :len(rels)] = torch.tensor(rels, dtype=torch.long)
        filled_items += 1
    print(f'[KSR] Built item_relation_map_tensor: shape={tuple(item_relation_map_tensor.shape)}, filled_items={filled_items}, R_MAX={R_MAX}')
    if config.model_name.lower() in ['ksr']:
        return {'num_users': num_users, 'num_entities': num_entities, 'num_relations': num_relations, 'item_entity_map': item_entity_map, 'item_relation_map_tensor': item_relation_map_tensor, 'kg_triples_df': kg_df_mapped}
    elif config.model_name.lower() == 'katrec':
        adj_matrix = get_adj_matrix(dataset_name, num_users, num_entities, item_entity_map, kg_df_mapped)
        return {'num_users': num_users, 'num_entities': num_entities, 'num_relations': num_relations, 'adj_matrix': adj_matrix, 'item_entity_map': item_entity_map, 'kg_triples_df': kg_df_mapped}
    return {}


============================================================
文件位置: /root/KG-SR-main/src/datasets/preprocess.py
============================================================
import pandas as pd
import argparse
from pathlib import Path
from tqdm import tqdm
from src.utils.path_finder import PathFinder

class DataPreprocessor:

    def __init__(self, config):
        self.config = config
        self.dataset_name = config.dataset_name
        self.window_size = config.window_size
        self.min_core = config.min_core
        self.max_len = config.max_len
        self.pf = PathFinder(self.dataset_name, self.window_size)
        dp = self.pf.data_paths(ensure_dirs=True)
        self.data_dir = self.pf.data_dir()
        self.raw_data_dir = self.pf.raw_dir()
        self.base_processed_dir = dp.base_processed
        self.split_data_dir = dp.split_dir
        self.raw_file_path = dp.raw_inter
        self.link_file_path = dp.raw_link
        self.item_file_path = dp.raw_item
        print('#' * 20, ' Configuration ', '#' * 20)
        print(f'Dataset Name: {self.dataset_name}')
        print(f'Raw data file: {self.raw_file_path}')
        print(f'Entity link file: {self.link_file_path}')
        print(f'Raw item file: {self.item_file_path}')
        print(f'Base processed data path: {self.base_processed_dir}')
        print(f'L-dependent splits path: {self.split_data_dir}')
        print(f'Sliding Window Size (L): {self.window_size}')
        print(f'Min-Core for Filtering (K): {self.min_core}')
        print(f'Max History Length (for truncation): {self.max_len}')
        print('#' * 75)

    def _apply_k_core_filtering(self, df, min_core):
        print(f'\n--- Step 1a: Applying {min_core}-Core Filtering ---')
        while True:
            initial_rows = len(df)
            user_counts = df['user_id'].value_counts(dropna=False)
            valid_users = user_counts[user_counts >= min_core].index
            df = df[df['user_id'].isin(valid_users)]
            item_counts = df['item_id'].value_counts(dropna=False)
            valid_items = item_counts[item_counts >= min_core].index
            df = df[df['item_id'].isin(valid_items)]
            if len(df) == initial_rows:
                break
        print(f'Filtering complete. Final data has {len(df)} interactions.')
        return df

    def run(self):
        print('\n--- Step 1: Loading Data ---')
        df = pd.read_csv(self.raw_file_path, sep='\t', header=0, engine='python')
        df.columns = ['user_id', 'item_id', 'rating', 'timestamp']
        df['user_id'] = df['user_id'].astype(str)
        df['item_id'] = df['item_id'].astype(str)
        print(f'Original data loaded: {len(df)} interactions.')
        df = self._apply_k_core_filtering(df, self.min_core)
        print(f"User count after filtering: {df['user_id'].nunique()}")
        print(f"Item count after filtering: {df['item_id'].nunique()}")
        df = df.sort_values(by=['user_id', 'timestamp'], ascending=True)
        print('\n--- Step 2: Create and Apply User ID Mapping ---')
        unique_users = df['user_id'].unique()
        sorted_unique_users = sorted(unique_users, key=lambda x: int(x) if x.isdigit() else x)
        user_map = {original_id: new_id for (new_id, original_id) in enumerate(sorted_unique_users)}
        df['user_id'] = df['user_id'].map(user_map)
        print(f'Created and applied mapping for {len(user_map)} users.')
        print('\n--- Step 3: Create and Apply Item ID Mapping ---')
        item_map = {}
        next_item_id = 1
        user_sequences_original_items = df.groupby('user_id')['item_id'].apply(list)
        sorted_mapped_user_ids = sorted(user_sequences_original_items.index.tolist())
        for mapped_user_id in tqdm(sorted_mapped_user_ids, desc='Creating Item Map'):
            sequence = user_sequences_original_items[mapped_user_id]
            for original_item_id in sequence:
                if original_item_id not in item_map:
                    item_map[original_item_id] = next_item_id
                    next_item_id += 1
        df['item_id'] = df['item_id'].map(item_map)
        print(f'Created and applied mapping for {len(item_map)} items (IDs from 1 to {next_item_id - 1}). Token 0 is reserved for padding.')
        print('\n--- Step 4: Saving Remapped Interaction File ---')
        remapped_inter_path = self.base_processed_dir / f'{self.dataset_name}.inter'
        custom_header = ['user_id:token', 'item_id:token', 'rating:float', 'timestamp:float']
        df.to_csv(remapped_inter_path, sep='\t', index=False, header=custom_header)
        print(f'Remapped interaction data saved to {remapped_inter_path}')
        print('\n--- Step 5: Generating and Truncating User Interaction Sequences ---')
        df = df.sort_values(by=['user_id', 'timestamp'], ascending=True)
        user_sequences_mapped = df.groupby('user_id')['item_id'].apply(list)
        user_sequences_mapped = user_sequences_mapped.apply(lambda seq: seq[-self.max_len:])
        print(f'Generated and truncated sequences for {len(user_sequences_mapped)} users (max length = {self.max_len}).')
        print('\n--- Step 6: Applying Padding and Generating Samples (Leave-One-Out) ---')
        (train_data, valid_data, test_data) = ([], [], [])
        PAD_TOKEN = 0
        for (user_id, sequence) in tqdm(user_sequences_mapped.items(), desc='Processing sequences'):
            if len(sequence) < 3:
                continue
            test_hist = sequence[-self.window_size - 1:-1]
            test_padded_hist = [PAD_TOKEN] * (self.window_size - len(test_hist)) + test_hist
            test_data.append({'user_id': user_id, 'sequence': ','.join(map(str, test_padded_hist)), 'target_id': sequence[-1]})
            valid_hist = sequence[-self.window_size - 2:-2]
            valid_padded_hist = [PAD_TOKEN] * (self.window_size - len(valid_hist)) + valid_hist
            valid_data.append({'user_id': user_id, 'sequence': ','.join(map(str, valid_padded_hist)), 'target_id': sequence[-2]})
            train_seq = sequence[:-2]
            if len(train_seq) < 2:
                continue
            for i in range(1, len(train_seq)):
                target_id = train_seq[i]
                history = train_seq[max(0, i - self.window_size):i]
                padded_history = [PAD_TOKEN] * (self.window_size - len(history)) + history
                train_data.append({'user_id': user_id, 'sequence': ','.join(map(str, padded_history)), 'target_id': target_id})
        print(f'Generated {len(train_data)} training samples.')
        print(f'Generated {len(valid_data)} validation samples.')
        print(f'Generated {len(test_data)} testing samples.')
        print('\n--- Step 7: Saving Processed Data and Artifacts ---')
        train_path = self.split_data_dir / 'train.csv'
        valid_path = self.split_data_dir / 'valid.csv'
        test_path = self.split_data_dir / 'test.csv'
        df_train = pd.DataFrame(train_data)
        df_valid = pd.DataFrame(valid_data)
        df_test = pd.DataFrame(test_data)
        df_train.to_csv(train_path, index=False)
        print(f'Train data saved to {train_path}')
        df_valid.to_csv(valid_path, index=False)
        print(f'Validation data saved to {valid_path}')
        df_test.to_csv(test_path, index=False)
        print(f'Test data saved to {test_path}')
        processed_user_path = self.base_processed_dir / f'{self.dataset_name}.user'
        df_user_out = pd.DataFrame({'user_id:token': [user_map[k] for k in sorted(user_map.keys(), key=lambda x: int(x) if str(x).isdigit() else str(x))], 'original_id:token': sorted(user_map.keys(), key=lambda x: int(x) if str(x).isdigit() else str(x))})
        df_user_out.to_csv(processed_user_path, sep='\t', index=False)
        print(f'User file (remapped) saved to {processed_user_path}')
        try:
            df_link_raw = pd.read_csv(self.link_file_path, sep='\t', header=0, names=['original_id', 'entity_id'], dtype={'original_id': str, 'entity_id': str})
            print('Loaded raw link file.')
        except FileNotFoundError:
            print(f'Warning: Entity link file not found at {self.link_file_path}.')
            df_link_raw = pd.DataFrame(columns=['original_id', 'entity_id'])
        df_item_map = pd.DataFrame(item_map.items(), columns=['original_id', 'mapped_id'])
        df_item_map = pd.merge(df_item_map, df_link_raw, on='original_id', how='left')
        print('\n--- Step 7a: Generating Placeholder Entity IDs for Missing Items ---')
        missing_entity_mask = df_item_map['entity_id'].isnull()
        num_missing = int(missing_entity_mask.sum())
        if num_missing > 0:
            mapped_ids_to_impute = df_item_map.loc[missing_entity_mask, 'mapped_id']
            placeholders = mapped_ids_to_impute.apply(lambda x: f'n.00{x}')
            df_item_map.loc[missing_entity_mask, 'entity_id'] = placeholders
            print(f'Found and filled {num_missing} missing entity IDs with placeholders.')
        else:
            print('No missing entity IDs found.')
        link_export_path = self.base_processed_dir / f'{self.dataset_name}.link'
        link_export_df = df_item_map[['mapped_id', 'entity_id']].copy()
        link_export_df.dropna(subset=['entity_id'], inplace=True)
        link_export_df.rename(columns={'mapped_id': 'item_id:token', 'entity_id': 'entity_id:token'}, inplace=True)
        link_export_df = link_export_df.sort_values('item_id:token')
        link_export_df.to_csv(link_export_path, sep='\t', index=False)
        print(f'Generated new link file at: {link_export_path}')
        print('\n--- Step 7b: Loading Knowledge Graph and filtering by head entities from link ---')
        kg_dest_path = self.base_processed_dir / f'{self.dataset_name}.kg'
        kg_src_path = self.pf.raw_kg_path()
        default_kg_columns = ['head_id:token', 'relation:token', 'tail_id:token']
        try:
            df_kg_raw = pd.read_csv(kg_src_path, sep='\t', header=0)
            kg_columns = df_kg_raw.columns.tolist()
            print(f'Loaded raw KG with {len(df_kg_raw)} triples. Columns: {kg_columns}')
        except FileNotFoundError as e:
            print(f'Warning: Raw KG file not found: {e}. Will fallback to fake triples only.')
            kg_columns = default_kg_columns
            df_kg_raw = pd.DataFrame(columns=kg_columns)
        except Exception as e:
            print(f'An error occurred when reading raw KG: {e}. Will fallback to fake triples only.')
            kg_columns = default_kg_columns
            df_kg_raw = pd.DataFrame(columns=kg_columns)
        head_col = df_kg_raw.columns[0] if not df_kg_raw.empty else default_kg_columns[0]
        allowed_heads = set(link_export_df['entity_id:token'].astype(str).unique())
        if not df_kg_raw.empty:
            df_kg_raw[head_col] = df_kg_raw[head_col].astype(str)
            before_cnt = len(df_kg_raw)
            df_kg_base = df_kg_raw[df_kg_raw[head_col].isin(allowed_heads)].copy()
            after_cnt = len(df_kg_base)
            print(f'Filtered KG by head entities: {before_cnt} -> {after_cnt} triples kept.')
        else:
            df_kg_base = pd.DataFrame(columns=df_kg_raw.columns if not df_kg_raw.empty else default_kg_columns)
        print('\n--- Step 7c: Updating Knowledge Graph with Fake Triples ---')
        all_triples = []
        if not df_kg_base.empty:
            all_triples.append(df_kg_base)
        items_with_placeholders = link_export_df[link_export_df['entity_id:token'].astype(str).str.startswith('n.')]
        fake_triples = []
        if not items_with_placeholders.empty:
            kg_columns_use = df_kg_base.columns.tolist() if not df_kg_base.empty else default_kg_columns
            for (_, row) in items_with_placeholders.iterrows():
                head_entity = str(row['entity_id:token'])
                tail_entity = head_entity.replace('n.', 'f.', 1)
                fake_triples.append({kg_columns_use[0]: head_entity, kg_columns_use[1]: 'fake', kg_columns_use[2]: tail_entity})
        if fake_triples:
            df_fake_triples = pd.DataFrame(fake_triples)
            all_triples.append(df_fake_triples)
            print(f'Generated {len(df_fake_triples)} fake triples to be added.')
        if all_triples:
            df_kg_updated = pd.concat(all_triples, ignore_index=True)
            cols = df_kg_updated.columns.tolist()
            if len(cols) >= 3:
                rename_map = {}
                expect = default_kg_columns
                for i in range(3):
                    if cols[i] != expect[i]:
                        rename_map[cols[i]] = expect[i]
                if rename_map:
                    df_kg_updated.rename(columns=rename_map, inplace=True)
            sort_cols = default_kg_columns[:2]
            for c in sort_cols:
                df_kg_updated[c] = df_kg_updated[c].astype(str)
            df_kg_updated = df_kg_updated.sort_values(by=sort_cols, ascending=True, kind='mergesort')
            df_kg_updated.to_csv(kg_dest_path, sep='\t', index=False, header=True)
            print(f'Final knowledge graph with {len(df_kg_updated)} triples saved to: {kg_dest_path}')
        else:
            print('No KG triples to save. KG file not created.')
        print('\n--- Step 7d: Remapping and Filtering Item Metadata (.item) ---')
        remapped_item_path = self.base_processed_dir / f'{self.dataset_name}.item'
        try:
            df_item_raw = pd.read_csv(self.item_file_path, sep='\t', header=0, engine='python')
            df_item_raw.columns = ['item_id:token', 'movie_title:token_seq', 'release_year:token', 'genre:token_seq']
            df_item_raw['item_id:token'] = df_item_raw['item_id:token'].astype(str)
            df_item_remapped = df_item_raw[df_item_raw['item_id:token'].isin(item_map.keys())].copy()
            df_item_remapped['item_id:token'] = df_item_remapped['item_id:token'].map(item_map)
            df_item_remapped = df_item_remapped.sort_values(by='item_id:token')
            df_item_remapped.to_csv(remapped_item_path, sep='\t', index=False)
            print(f'Remapped and filtered item metadata saved to {remapped_item_path}')
        except FileNotFoundError:
            print(f'Warning: Item metadata file not found at {self.item_file_path}.')
        print('\nPreprocessing finished successfully! 🎉')
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Data Preprocessing for Sequential Recommendation')
    parser.add_argument('--dataset_name', type=str, default='ml-1m', help='Name of the dataset')
    parser.add_argument('--window_size', type=int, default=5, help='The size of the sliding window (L)')
    parser.add_argument('--min_core', type=int, default=5, help='The K-core filtering threshold')
    parser.add_argument('--max_len', type=int, default=200, help='The maximum length of user history')
    args = parser.parse_args()
    if args.max_len < args.window_size:
        print('Warning: max_len is less than window_size. Setting max_len to window_size for truncation.')
        args.max_len = args.window_size
    preprocessor = DataPreprocessor(args)
    preprocessor.run()


============================================================
文件位置: /root/KG-SR-main/src/datasets/dataloader.py
============================================================
import torch
import pandas as pd
from torch.utils.data import Dataset, DataLoader
import gc
import numpy as np
import random
from src.utils.path_finder import PathFinder

class RecDenoisingDataset(Dataset):

    def __init__(self, data_path):
        dtypes = {'user_id': 'int32', 'sequence': 'string', 'target_id': 'int32'}
        self.data_df = pd.read_csv(data_path, dtype=dtypes)
        print(f'Loaded {len(self.data_df)} samples from {data_path}')
        gc.collect()

    def __len__(self):
        return len(self.data_df)

    def __getitem__(self, idx):
        row = self.data_df.iloc[idx]
        user_id = int(row['user_id'])
        sequence = list(map(int, row['sequence'].split(',')))
        target_id = int(row['target_id'])
        return {'user_id': torch.tensor(user_id, dtype=torch.long), 'sequence': torch.tensor(sequence, dtype=torch.long), 'target_id': torch.tensor(target_id, dtype=torch.long)}

def get_dataloaders(config):
    pf = PathFinder(config.dataset_name, config.window_size)
    dp = pf.data_paths(use_corrected_train=bool(getattr(config, 'use_corrected_train', False)), corrected_train_filename=getattr(config, 'corrected_train_filename', 'corrected_train.csv'), ensure_dirs=True)
    (train_path, valid_path, test_path) = (dp.train_csv, dp.valid_csv, dp.test_csv)
    if not all([train_path.exists(), valid_path.exists(), test_path.exists()]):
        raise FileNotFoundError(f'One or more processed data files not found in {dp.split_dir}. Please run preprocess.py first with --window_size {config.window_size}')
    train_dataset = RecDenoisingDataset(train_path)
    valid_dataset = RecDenoisingDataset(valid_path)
    test_dataset = RecDenoisingDataset(test_path)
    link_path = dp.processed_link
    if not link_path.exists():
        raise FileNotFoundError(f'Processed link file not found: {link_path}. Please run preprocess.py first.')
    link_df = pd.read_csv(link_path, sep='\t', dtype={'item_id:token': 'int32'})
    num_items = int(link_df['item_id:token'].max())
    del link_df
    gc.collect()
    optimized_num_workers = min(getattr(config, 'num_workers', 0), 2)
    optimized_batch_size = getattr(config, 'optimized_batch_size', getattr(config, 'batch_size', 256))
    seed = int(getattr(config, 'seed', 42))
    g = torch.Generator()
    g.manual_seed(seed)

    def _worker_init_fn(worker_id):
        s = seed + worker_id
        np.random.seed(s)
        random.seed(s)
        torch.manual_seed(s)
    common_loader_args = {'batch_size': optimized_batch_size, 'num_workers': optimized_num_workers, 'pin_memory': torch.cuda.is_available(), 'persistent_workers': optimized_num_workers > 0, 'worker_init_fn': _worker_init_fn if optimized_num_workers > 0 else None, 'generator': g}
    if optimized_num_workers > 0:
        common_loader_args['prefetch_factor'] = 2
    train_loader = DataLoader(train_dataset, shuffle=True, drop_last=True, **common_loader_args)
    valid_loader = DataLoader(valid_dataset, shuffle=False, drop_last=False, **common_loader_args)
    test_loader = DataLoader(test_dataset, shuffle=False, drop_last=False, **common_loader_args)
    print(f'Number of items (EXCLUDING padding): {num_items}')
    print(f'Optimized batch size: {optimized_batch_size}')
    print(f'Optimized num workers: {optimized_num_workers}')
    print(f'Train loader: {len(train_loader.dataset)} samples, {len(train_loader)} batches.')
    print(f'Validation loader: {len(valid_loader.dataset)} samples, {len(valid_loader)} batches.')
    print(f'Test loader: {len(test_loader.dataset)} samples, {len(test_loader)} batches.')
    return (train_loader, valid_loader, test_loader, num_items)


============================================================
文件位置: /root/KG-SR-main/src/models/__init__.py
============================================================



============================================================
文件位置: /root/KG-SR-main/src/models/ksr.py
============================================================
import torch
import torch.nn as nn
from torch.nn.init import xavier_uniform_, xavier_normal_

class KSR(nn.Module):

    def __init__(self, num_items: int, config):
        super().__init__()
        self.n_items = int(num_items)
        self.embedding_size = int(getattr(config, 'embedding_dim', 64))
        self.hidden_size = int(getattr(config, 'hidden_size', self.embedding_size))
        self.num_layers = int(getattr(config, 'num_layers', 1))
        self.dropout_prob = float(getattr(config, 'dropout_prob', 0.2))
        self.kg_embedding_size = int(getattr(config, 'kg_embedding_size', getattr(config, 'kg_embedding_dim', 64)))
        self.gamma = float(getattr(config, 'gamma', 0.5))
        self.freeze_kg = bool(getattr(config, 'freeze_kg', False))
        assert hasattr(config, 'num_entities'), 'KSR needs config.num_entities'
        assert hasattr(config, 'num_relations'), 'KSR needs config.num_relations (with padding 0)'
        self.n_entities = int(config.num_entities)
        self.n_relations_total = int(config.num_relations)
        self.n_relations = self.n_relations_total - 1
        assert self.n_relations > 0
        self.item_embedding = nn.Embedding(self.n_items, self.embedding_size)
        self.entity_embedding = nn.Embedding(self.n_entities, self.kg_embedding_size)
        if self.freeze_kg:
            self.entity_embedding.weight.requires_grad = False
        self.relation_embedding = nn.Embedding(self.n_relations_total, self.kg_embedding_size, padding_idx=0)
        self.emb_dropout = nn.Dropout(self.dropout_prob)
        self.gru_layers = nn.GRU(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=self.num_layers, bias=False, batch_first=True)
        self.dense_seq_to_k = nn.Linear(self.hidden_size, self.kg_embedding_size)
        self.dense_layer_u = nn.Linear(self.hidden_size + self.kg_embedding_size, self.embedding_size)
        self.dense_layer_i = nn.Linear(self.embedding_size + self.kg_embedding_size, self.embedding_size)
        item_to_entity = torch.full((self.n_items,), -1, dtype=torch.long)
        for (it_1, ent) in getattr(config, 'item_entity_map', {}).items():
            it_i = int(it_1) - 1
            ent_i = int(ent)
            if 0 <= it_i < self.n_items and 0 <= ent_i < self.n_entities:
                item_to_entity[it_i] = ent_i
        self.register_buffer('item_to_entity', item_to_entity)
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Embedding):
            xavier_normal_(module.weight)
        elif isinstance(module, nn.GRU):
            try:
                nn.init.xavier_uniform_(module.weight_hh_l0)
                nn.init.xavier_uniform_(module.weight_ih_l0)
            except AttributeError:
                for (name, param) in module.named_parameters():
                    if 'weight' in name:
                        xavier_uniform_(param.data)
        elif isinstance(module, nn.Linear):
            xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)

    def _get_kg_embedding(self, head_items: torch.Tensor):
        nonpad = (head_items > 0).unsqueeze(-1).float()
        idx0 = torch.clamp(head_items - 1, min=0)
        ent_ids = self.item_to_entity[idx0]
        valid_mask = (ent_ids >= 0).float().unsqueeze(-1)
        ent_ids_clamped = torch.clamp(ent_ids, min=0)
        head_e = self.entity_embedding(ent_ids_clamped) * valid_mask * nonpad
        rel_mat = self.relation_embedding.weight[1:]
        rel_mat = rel_mat.unsqueeze(0).expand(head_e.size(0), -1, -1)
        head_M = head_e.unsqueeze(1).expand(-1, self.n_relations, -1)
        tail_M = head_M + rel_mat
        return (head_e, tail_M)

    def _memory_update_cell(self, user_memory: torch.Tensor, update_memory: torch.Tensor):
        z = torch.sigmoid((user_memory * update_memory).sum(-1)).unsqueeze(-1)
        updated_user_memory = (1.0 - z) * user_memory + z * update_memory
        return updated_user_memory

    def memory_update(self, item_seq: torch.Tensor, item_seq_len: torch.Tensor):
        (B, L) = item_seq.size()
        device = item_seq.device
        last_idx = torch.clamp(item_seq_len - 1, min=0)
        user_memory = torch.zeros(B, self.n_relations, self.kg_embedding_size, device=device)
        last_user_memory = torch.zeros_like(user_memory)
        for i in range(L):
            (_, upd_mem) = self._get_kg_embedding(item_seq[:, i])
            user_memory = self._memory_update_cell(user_memory, upd_mem)
            mask = last_idx == i
            if mask.any():
                last_user_memory[mask] = user_memory[mask]
        return last_user_memory

    def memory_read(self, seq_output_H: torch.Tensor, user_memory: torch.Tensor):
        q_k = self.dense_seq_to_k(seq_output_H)
        attrs = self.relation_embedding.weight[1:]
        logits = self.gamma * torch.matmul(q_k, attrs.t()).float()
        attn = torch.softmax(logits, dim=-1)
        u_m = (user_memory * attn.unsqueeze(-1)).sum(1)
        return u_m

    @staticmethod
    def _last_hidden_from_gru(gru_out: torch.Tensor, seq_len: torch.Tensor):
        B = gru_out.size(0)
        idx = torch.arange(B, device=gru_out.device)
        last = gru_out[idx, torch.clamp(seq_len - 1, min=0)]
        return last

    def _get_item_comb_embedding(self, items: torch.Tensor):
        ent_ids = self.item_to_entity[items]
        valid = (ent_ids >= 0).float().unsqueeze(-1)
        ent_ids_clamped = torch.clamp(ent_ids, min=0)
        h_e = self.entity_embedding(ent_ids_clamped) * valid
        i_e = self.item_embedding(items)
        q_i = self.dense_layer_i(torch.cat((i_e, h_e), dim=-1))
        return q_i

    def forward(self, sequences: torch.Tensor):
        nonpad = (sequences > 0).unsqueeze(-1).float()
        seq_idx = torch.clamp(sequences - 1, min=0)
        emb = self.item_embedding(seq_idx) * nonpad
        emb = self.emb_dropout(emb)
        (gru_out, _) = self.gru_layers(emb)
        seq_len = (sequences != 0).sum(dim=1)
        seq_H = self._last_hidden_from_gru(gru_out, seq_len)
        user_memory = self.memory_update(sequences, seq_len)
        u_m = self.memory_read(seq_H, user_memory)
        p_u = self.dense_layer_u(torch.cat((seq_H, u_m), dim=-1))
        all_item_e = self.item_embedding.weight
        all_item_ent = self.item_to_entity
        all_ent_ids = torch.clamp(all_item_ent, min=0)
        all_valid = (all_item_ent >= 0).float().unsqueeze(-1)
        all_h_e = self.entity_embedding(all_ent_ids) * all_valid
        all_q_i = self.dense_layer_i(torch.cat((all_item_e, all_h_e), dim=-1))
        logits = torch.matmul(p_u, all_q_i.t())
        return logits


============================================================
文件位置: /root/KG-SR-main/src/models/sasrec.py
============================================================
import torch
import torch.nn as nn
from src.models.components.transformer import TransformerBlock
from src.models.components.mask_utils import build_causal_mask, build_key_padding_mask
from src.models.components.init_utils import init_linear_xavier_uniform, init_embedding_xavier_uniform, init_layernorm_default

class SASRec(nn.Module):

    def __init__(self, num_items, embedding_dim, max_len, num_blocks, num_attention_heads, dropout_prob):
        super().__init__()
        self.hidden_size = embedding_dim
        self.inner_size = self.hidden_size * 4
        self.max_len = max_len
        self.dropout_prob = dropout_prob
        self.layer_norm_eps = 1e-12
        self.item_embedding = nn.Embedding(num_items, self.hidden_size)
        self.position_embedding = nn.Embedding(self.max_len, self.hidden_size)
        self.transformer_blocks = nn.ModuleList([TransformerBlock(hidden_size=self.hidden_size, n_heads=num_attention_heads, inner_size=self.inner_size, hidden_dropout_prob=self.dropout_prob, attn_dropout_prob=self.dropout_prob, hidden_act='relu', layer_norm_eps=self.layer_norm_eps) for _ in range(num_blocks)])
        self.layer_norm = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)
        self.dropout = nn.Dropout(self.dropout_prob)
        self.apply(init_linear_xavier_uniform)
        self.apply(init_embedding_xavier_uniform)
        self.apply(init_layernorm_default)

    @staticmethod
    def _generate_attention_mask(sequences):
        key_padding_mask = build_key_padding_mask(sequences)
        causal_mask = build_causal_mask(sequences.size(1), sequences.device)
        return (causal_mask, key_padding_mask)

    def encode(self, sequences, override_item_emb=None):
        nonpad = (sequences > 0).unsqueeze(-1).float()
        seq_idx = torch.clamp(sequences - 1, min=0)
        if override_item_emb is None:
            item_emb = self.item_embedding(seq_idx)
        else:
            item_emb = override_item_emb
        item_emb = item_emb * nonpad
        position_ids = torch.arange(sequences.size(1), dtype=torch.long, device=sequences.device)
        position_ids = position_ids.unsqueeze(0).expand_as(sequences)
        position_emb = self.position_embedding(position_ids)
        x = item_emb + position_emb
        x = self.layer_norm(x)
        x = self.dropout(x)
        (causal_mask, key_padding_mask) = self._generate_attention_mask(sequences)
        for block in self.transformer_blocks:
            x = block(x, attn_mask=causal_mask, key_padding_mask=key_padding_mask)
            x[sequences == 0] = 0.0
        item_seq_len = (sequences != 0).sum(dim=1)
        b_idx = torch.arange(sequences.size(0), device=sequences.device)
        last_idx = torch.clamp(item_seq_len - 1, min=0)
        final_user_repr = x[b_idx, last_idx, :]
        return final_user_repr

    def forward(self, sequences, override_item_emb=None):
        final_user_repr = self.encode(sequences, override_item_emb=override_item_emb)
        logits = torch.matmul(final_user_repr, self.item_embedding.weight.transpose(0, 1))
        return logits


============================================================
文件位置: /root/KG-SR-main/src/models/gru4rec.py
============================================================
import torch
import torch.nn as nn
from torch.nn.init import xavier_uniform_, xavier_normal_

class GRU4Rec(nn.Module):

    def __init__(self, num_items, embedding_dim, hidden_size, num_layers=1, dropout_prob=0.5):
        super().__init__()
        self.num_items = int(num_items)
        self.embedding_dim = int(embedding_dim)
        self.hidden_size = int(hidden_size)
        self.num_layers = int(num_layers)
        self.dropout_prob = float(dropout_prob)
        self.item_embedding = nn.Embedding(self.num_items, self.embedding_dim)
        self.emb_dropout = nn.Dropout(self.dropout_prob)
        self.gru_layers = nn.GRU(input_size=self.embedding_dim, hidden_size=self.hidden_size, num_layers=self.num_layers, bias=False, batch_first=True)
        self.dense = nn.Linear(self.hidden_size, self.embedding_dim)
        self.out_dropout = nn.Dropout(0.0)
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Embedding):
            xavier_normal_(module.weight)
        elif isinstance(module, nn.GRU):
            for (name, param) in module.named_parameters():
                if 'weight_ih' in name or 'weight_hh' in name:
                    xavier_uniform_(param.data)
        elif isinstance(module, nn.Linear):
            xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)

    @staticmethod
    def _last_hidden_from_outputs(outputs: torch.Tensor, seq_len: torch.Tensor):
        B = outputs.size(0)
        idx = torch.arange(B, device=outputs.device)
        last = outputs[idx, torch.clamp(seq_len - 1, min=0)]
        return last

    def forward(self, sequences: torch.Tensor):
        nonpad = (sequences > 0).unsqueeze(-1).float()
        seq_idx = torch.clamp(sequences - 1, min=0)
        emb = self.item_embedding(seq_idx)
        emb = emb * nonpad
        emb = self.emb_dropout(emb)
        (gru_out, _) = self.gru_layers(emb)
        proj_out = self.dense(gru_out)
        seq_len = (sequences != 0).sum(dim=1)
        seq_output = self._last_hidden_from_outputs(proj_out, seq_len)
        seq_output = self.out_dropout(seq_output)
        logits = torch.matmul(seq_output, self.item_embedding.weight.transpose(0, 1))
        return logits


============================================================
文件位置: /root/KG-SR-main/src/models/components/__init__.py
============================================================



============================================================
文件位置: /root/KG-SR-main/src/models/components/mask_utils.py
============================================================
import torch
__all__ = ['build_causal_mask', 'build_key_padding_mask']

def build_causal_mask(seq_len: int, device: torch.device) -> torch.Tensor:
    mask = torch.full((seq_len, seq_len), -1000000000.0, device=device)
    return torch.triu(mask, diagonal=1)

def build_key_padding_mask(sequences: torch.Tensor) -> torch.Tensor:
    return sequences.eq(0)


============================================================
文件位置: /root/KG-SR-main/src/models/components/init_utils.py
============================================================
import torch.nn as nn
from torch.nn.init import xavier_uniform_, xavier_normal_, kaiming_uniform_, kaiming_normal_, constant_
__all__ = ['init_linear_xavier_uniform', 'init_embedding_xavier_uniform', 'init_layernorm_default', 'init_gru_xavier_uniform']

def init_linear_xavier_uniform(m: nn.Module):
    if isinstance(m, nn.Linear):
        xavier_uniform_(m.weight)
        if m.bias is not None:
            constant_(m.bias, 0)

def init_embedding_xavier_uniform(m: nn.Module):
    if isinstance(m, nn.Embedding):
        xavier_uniform_(m.weight)

def init_layernorm_default(m: nn.Module):
    if isinstance(m, nn.LayerNorm):
        constant_(m.bias, 0)
        constant_(m.weight, 1.0)

def init_gru_xavier_uniform(m: nn.Module):
    if isinstance(m, nn.GRU):
        for (name, param) in m.named_parameters():
            if 'weight_ih' in name or 'weight_hh' in name:
                xavier_uniform_(param.data)


============================================================
文件位置: /root/KG-SR-main/src/models/components/augment.py
============================================================
import torch
__all__ = ['safe_random_mask', 'random_crop', 'local_reorder', 'random_substitute', 'compose_augmentation']

def _nonzero_pos(row: torch.Tensor):
    return row.nonzero(as_tuple=False).flatten()

@torch.no_grad()
def safe_random_mask(seq: torch.Tensor, mask_ratio: float) -> torch.Tensor:
    if mask_ratio <= 0.0:
        return seq
    out = seq.clone()
    (B, L) = out.size()
    dev = out.device
    for i in range(B):
        pos = _nonzero_pos(out[i])
        n = int(pos.numel())
        if n <= 1:
            continue
        m = int(round(mask_ratio * n))
        m = max(0, min(m, n - 1))
        if m == 0:
            continue
        idx = torch.randperm(n, device=dev)[:m]
        out[i, pos[idx]] = 0
    return out

@torch.no_grad()
def random_crop(seq: torch.Tensor, min_keep_ratio: float) -> torch.Tensor:
    if min_keep_ratio >= 1.0:
        return seq
    out = seq.clone()
    (B, L) = out.size()
    dev = out.device
    valid_len = (out != 0).sum(dim=1)
    r = torch.empty(B, device=dev).uniform_(min_keep_ratio, 1.0)
    keep = torch.clamp((valid_len.float() * r).round().long(), min=1)
    for i in range(B):
        k = int(keep[i].item())
        pos = _nonzero_pos(out[i])
        if pos.numel() == 0:
            continue
        pos = pos[-k:]
        new_row = torch.zeros(L, dtype=out.dtype, device=dev)
        new_row[-k:] = out[i, pos]
        out[i] = new_row
    return out

@torch.no_grad()
def local_reorder(seq: torch.Tensor, reorder_prob: float, max_window: int=4) -> torch.Tensor:
    if reorder_prob <= 0.0 or max_window <= 1:
        return seq
    out = seq.clone()
    (B, L) = out.size()
    dev = out.device
    for i in range(B):
        if torch.rand(1, device=dev).item() > reorder_prob:
            continue
        pos = _nonzero_pos(out[i])
        if pos.numel() <= 2:
            continue
        w = int(torch.randint(2, min(max_window, pos.numel()) + 1, (1,), device=dev).item())
        start = int(torch.randint(0, pos.numel() - w + 1, (1,), device=dev).item())
        seg = pos[start:start + w]
        vals = out[i, seg].clone()
        out[i, seg] = vals[torch.randperm(w, device=dev)]
    return out

@torch.no_grad()
def random_substitute(seq: torch.Tensor, substitute_prob: float, num_items: int) -> torch.Tensor:
    if substitute_prob <= 0.0 or num_items <= 1:
        return seq
    out = seq.clone()
    (B, L) = out.size()
    dev = out.device
    prob = torch.full((B, L), substitute_prob, device=dev)
    take = torch.bernoulli(prob).bool() & out.ne(0)
    if take.any():
        out[take] = torch.randint(1, num_items + 1, (take.sum().item(),), device=dev, dtype=out.dtype)
    return out

@torch.no_grad()
def compose_augmentation(seq: torch.Tensor, *, num_items: int, mask_ratio: float=0.0, crop_min_ratio: float=1.0, reorder_prob: float=0.0, substitute_prob: float=0.0) -> torch.Tensor:
    orig = seq
    out = seq
    out = random_crop(out, crop_min_ratio)
    out = local_reorder(out, reorder_prob, max_window=4)
    out = random_substitute(out, substitute_prob, num_items)
    out = safe_random_mask(out, mask_ratio)
    (B, L) = out.size()
    dev = out.device
    for i in range(B):
        if out[i].nonzero(as_tuple=False).numel() == 0:
            pos = _nonzero_pos(orig[i])
            if pos.numel() > 0:
                last_val = int(orig[i, pos[-1]].item())
                new_row = torch.zeros(L, dtype=out.dtype, device=dev)
                new_row[-1] = last_val
                out[i] = new_row
    return out


============================================================
文件位置: /root/KG-SR-main/src/models/components/transformer.py
============================================================
import torch
import torch.nn as nn
from src.models.components.init_utils import init_linear_xavier_uniform, init_embedding_xavier_uniform, init_layernorm_default
_ACT_MAP = {'relu': nn.ReLU, 'gelu': nn.GELU}

def get_activation(name: str):
    name = (name or 'relu').lower()
    return _ACT_MAP.get(name, nn.ReLU)()

def build_causal_mask(seq_len: int, device: torch.device) -> torch.Tensor:
    mask = torch.full((seq_len, seq_len), -1000000000.0, device=device)
    return torch.triu(mask, diagonal=1)

class TransformerBlock(nn.Module):

    def __init__(self, hidden_size: int, n_heads: int, inner_size: int, hidden_dropout_prob: float, attn_dropout_prob: float, hidden_act: str='relu', layer_norm_eps: float=1e-12):
        super().__init__()
        self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=n_heads, dropout=attn_dropout_prob, batch_first=True)
        self.ln1 = nn.LayerNorm(hidden_size, eps=layer_norm_eps)
        self.do1 = nn.Dropout(hidden_dropout_prob)
        self.ffn = nn.Sequential(nn.Linear(hidden_size, inner_size), get_activation(hidden_act), nn.Linear(inner_size, hidden_size))
        self.ln2 = nn.LayerNorm(hidden_size, eps=layer_norm_eps)
        self.do2 = nn.Dropout(hidden_dropout_prob)
        self.apply(init_linear_xavier_uniform)
        self.apply(init_embedding_xavier_uniform)
        self.apply(init_layernorm_default)

    def forward(self, x, attn_mask=None, key_padding_mask=None):
        (attn_out, _) = self.attention(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask)
        x = self.ln1(x + self.do1(attn_out))
        ff = self.ffn(x)
        x = self.ln2(x + self.do2(ff))
        return x


============================================================
文件位置: /root/KG-SR-main/src/models/katrec.py
============================================================
import math
import torch
import torch.nn as nn
from torch.nn.init import xavier_uniform_, xavier_normal_
from src.models.components.transformer import TransformerBlock
from src.models.components.mask_utils import build_causal_mask, build_key_padding_mask
from src.models.components.init_utils import init_linear_xavier_uniform, init_embedding_xavier_uniform, init_layernorm_default

class KATRec(nn.Module):

    def __init__(self, num_items: int, config):
        super().__init__()
        self.num_items = int(num_items)
        self.embedding_dim = int(getattr(config, 'embedding_dim', 64))
        self.hidden_size = self.embedding_dim
        self.num_blocks = int(getattr(config, 'num_blocks', 2))
        self.num_heads = int(getattr(config, 'num_attention_heads', 2))
        assert self.embedding_dim % self.num_heads == 0, 'embedding_dim 必须能被 num_attention_heads 整除'
        self.head_dim = self.embedding_dim // self.num_heads
        self.dropout_prob = float(getattr(config, 'dropout_prob', 0.2))
        self.max_len = int(getattr(config, 'window_size', 50))
        self.kg_dim = int(getattr(config, 'kg_embedding_dim', getattr(config, 'kg_embedding_size', 64)))
        self.n_gcn_layers = int(getattr(config, 'n_gcn_layers', 2))
        self.mess_dropout_prob = float(getattr(config, 'mess_dropout_prob', 0.1))
        self.kg_margin = float(getattr(config, 'kg_margin', 1.0))
        self.kg_attn_alpha = float(getattr(config, 'kg_attn_alpha', 0.2))
        self.num_users = int(config.num_users)
        self.num_entities = int(config.num_entities)
        self.num_relations = int(config.num_relations)
        self._raw_adj = config.adj_matrix.coalesce()
        self.register_buffer('adj_indices', self._raw_adj.indices())
        self.register_buffer('adj_values', self._raw_adj.values())
        self.adj_shape = self._raw_adj.shape
        item_to_ent = torch.full((self.num_items,), -1, dtype=torch.long)
        for (it_1, ent) in getattr(config, 'item_entity_map', {}).items():
            (it_i, ent_i) = (int(it_1) - 1, int(ent))
            if 0 <= it_i < self.num_items and 0 <= ent_i < self.num_entities:
                item_to_ent[it_i] = ent_i
        self.register_buffer('item_to_entity', item_to_ent)
        self.item_embedding = nn.Embedding(self.num_items, self.embedding_dim)
        self.position_embedding = nn.Embedding(self.max_len, self.embedding_dim)
        self.dropout = nn.Dropout(self.dropout_prob)
        self.layer_norm = nn.LayerNorm(self.embedding_dim, eps=1e-12)
        self.transformer_blocks = nn.ModuleList([TransformerBlock(hidden_size=self.embedding_dim, n_heads=self.num_heads, inner_size=self.embedding_dim * 4, hidden_dropout_prob=self.dropout_prob, attn_dropout_prob=self.dropout_prob, hidden_act='relu', layer_norm_eps=1e-12) for _ in range(self.num_blocks)])
        self.user_embedding_kg = nn.Embedding(self.num_users, self.kg_dim)
        self.entity_embedding_kg = nn.Embedding(self.num_entities, self.kg_dim)
        self.relation_embedding = nn.Embedding(self.num_relations, self.kg_dim, padding_idx=0)
        self.msg_dropout = nn.Dropout(self.mess_dropout_prob)
        self.kg2e_token = nn.Linear(self.kg_dim, self.embedding_dim)
        self.kg_q_proj = nn.Linear(self.embedding_dim, self.embedding_dim, bias=False)
        self.kg_k_proj = nn.Linear(self.embedding_dim, self.embedding_dim, bias=False)
        self.kg2e_item = nn.Linear(self.kg_dim, self.embedding_dim)
        self.item_fuse = nn.Linear(self.embedding_dim + self.embedding_dim, self.embedding_dim)
        self.apply(self._init_weights)
        self.apply(init_linear_xavier_uniform)
        self.apply(init_embedding_xavier_uniform)
        self.apply(init_layernorm_default)
        self.register_buffer('z_entity_cached', None, persistent=False)

    def _init_weights(self, module):
        if isinstance(module, nn.Embedding):
            xavier_normal_(module.weight.data)
        elif isinstance(module, nn.Linear):
            xavier_uniform_(module.weight.data)
            if module.bias is not None:
                nn.init.zeros_(module.bias)

    def _sparse_adj_on_device(self, device):
        return torch.sparse.FloatTensor(self.adj_indices.to(device), self.adj_values.to(device), torch.Size(self.adj_shape)).coalesce()

    def _graph_propagate(self, device):
        adj = self._sparse_adj_on_device(device)
        x0_user = self.user_embedding_kg.weight
        x0_ent = self.entity_embedding_kg.weight
        x = torch.cat([x0_user, x0_ent], dim=0)
        outs = [x]
        for _ in range(self.n_gcn_layers):
            x = torch.sparse.mm(adj, x)
            x = self.msg_dropout(x)
            outs.append(x)
        x_final = torch.stack(outs, dim=0).mean(0)
        z_user = x_final[:self.num_users]
        z_entity = x_final[self.num_users:]
        return (z_user, z_entity)

    def precompute_kg(self, device):
        with torch.no_grad():
            (_, z_entity) = self._graph_propagate(device)
        self.z_entity_cached = z_entity

    def _build_kga_bias(self, sequences, z_entity):
        device = sequences.device
        (B, L) = sequences.size()
        nonpad = (sequences > 0).unsqueeze(-1)
        idx0 = torch.clamp(sequences - 1, min=0)
        ent_ids = self.item_to_entity[idx0]
        valid_mask = (ent_ids >= 0) & nonpad.squeeze(-1)
        ent_ids_clamped = torch.clamp(ent_ids, min=0)
        ent_k = z_entity[ent_ids_clamped]
        ent_k = ent_k * valid_mask.unsqueeze(-1).float()
        ent_e = self.kg2e_token(ent_k)
        q = self.kg_q_proj(ent_e).view(B, L, self.num_heads, self.head_dim)
        k = self.kg_k_proj(ent_e).view(B, L, self.num_heads, self.head_dim)
        q = q.permute(0, 2, 1, 3)
        k = k.permute(0, 2, 1, 3)
        scale = 1.0 / math.sqrt(self.head_dim)
        bias = torch.matmul(q, k.transpose(-2, -1)) * scale
        key_pad = ~valid_mask
        key_pad_exp = key_pad.unsqueeze(1).unsqueeze(1)
        qry_pad_exp = key_pad.unsqueeze(1).unsqueeze(2)
        neg_inf = torch.tensor(-1000000000.0, device=device, dtype=bias.dtype)
        bias = bias.masked_fill(key_pad_exp, neg_inf)
        bias = bias.masked_fill(qry_pad_exp, neg_inf)
        bias = bias.reshape(B * self.num_heads, L, L)
        return bias

    def forward(self, sequences: torch.Tensor):
        device = sequences.device
        z_entity = self.z_entity_cached
        if z_entity is None:
            (_, z_entity) = self._graph_propagate(device)
        (B, L) = sequences.size()
        nonpad = (sequences > 0).unsqueeze(-1).float()
        seq_idx = torch.clamp(sequences - 1, min=0)
        item_emb = self.item_embedding(seq_idx) * nonpad
        pos_ids = torch.arange(L, device=device).unsqueeze(0).expand(B, L)
        pos_emb = self.position_embedding(pos_ids)
        x = self.layer_norm(item_emb + pos_emb)
        x = self.dropout(x)
        causal_mask = build_causal_mask(L, device)
        key_padding_mask = build_key_padding_mask(sequences)
        kg_bias = self._build_kga_bias(sequences, z_entity)
        causal_mask_expanded = causal_mask.unsqueeze(0).expand(B * self.num_heads, -1, -1)
        attn_mask = causal_mask_expanded + self.kg_attn_alpha * kg_bias
        for block in self.transformer_blocks:
            x = block(x, attn_mask=attn_mask, key_padding_mask=key_padding_mask)
            x[sequences == 0] = 0.0
        seq_len = (sequences != 0).sum(dim=1).clamp(min=1)
        b_idx = torch.arange(B, device=device)
        user_vec = x[b_idx, seq_len - 1, :]
        all_item_e = self.item_embedding.weight
        all_item_ent = self.item_to_entity
        all_ent_ids = torch.clamp(all_item_ent, min=0)
        all_valid = (all_item_ent >= 0).float().unsqueeze(-1)
        all_ent_k = z_entity[all_ent_ids] * all_valid
        all_ent_e = self.kg2e_item(all_ent_k)
        all_item_vec = self.item_fuse(torch.cat([all_item_e, all_ent_e], dim=-1))
        logits = torch.matmul(user_vec, all_item_vec.t())
        return logits

    @staticmethod
    def _transE_score(h, r, t):
        return -torch.norm(h + r - t, p=2, dim=-1)

    def calculate_kg_loss(self, h_idx, r_idx, pos_t_idx, neg_t_idx):
        device = h_idx.device
        h = self.entity_embedding_kg(h_idx).to(device)
        r = self.relation_embedding(r_idx).to(device)
        t_pos = self.entity_embedding_kg(pos_t_idx).to(device)
        t_neg = self.entity_embedding_kg(neg_t_idx).to(device)
        pos_score = self._transE_score(h, r, t_pos)
        neg_score = self._transE_score(h, r, t_neg)
        target = torch.ones_like(pos_score, device=device)
        loss_fn = nn.MarginRankingLoss(margin=self.kg_margin)
        loss = loss_fn(pos_score, neg_score, target)
        return loss


============================================================
文件位置: /root/KG-SR-main/src/models/bert4rec.py
============================================================
import torch
import torch.nn as nn
from src.models.components.transformer import TransformerBlock
from src.models.components.mask_utils import build_key_padding_mask
from src.models.components.init_utils import init_linear_xavier_uniform, init_embedding_xavier_uniform, init_layernorm_default

class BERT4Rec(nn.Module):

    def __init__(self, num_items: int, embedding_dim: int, max_len: int, num_blocks: int, num_attention_heads: int, dropout_prob: float, pooling: str='last', layer_norm_eps: float=1e-12):
        super().__init__()
        assert embedding_dim % num_attention_heads == 0, 'embedding_dim 必须能被 num_attention_heads 整除'
        self.num_items = int(num_items)
        self.hidden_size = int(embedding_dim)
        self.inner_size = self.hidden_size * 4
        self.max_len = int(max_len)
        self.dropout_prob = float(dropout_prob)
        self.layer_norm_eps = float(layer_norm_eps)
        self.num_blocks = int(num_blocks)
        self.num_heads = int(num_attention_heads)
        self.pooling = str(pooling).lower()
        self.item_embedding = nn.Embedding(self.num_items, self.hidden_size)
        self.position_embedding = nn.Embedding(self.max_len, self.hidden_size)
        self.transformer_blocks = nn.ModuleList([TransformerBlock(hidden_size=self.hidden_size, n_heads=self.num_heads, inner_size=self.inner_size, hidden_dropout_prob=self.dropout_prob, attn_dropout_prob=self.dropout_prob, hidden_act='gelu', layer_norm_eps=self.layer_norm_eps) for _ in range(self.num_blocks)])
        self.layer_norm = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)
        self.dropout = nn.Dropout(self.dropout_prob)
        self.apply(init_linear_xavier_uniform)
        self.apply(init_embedding_xavier_uniform)
        self.apply(init_layernorm_default)

    @staticmethod
    def _key_padding_mask(sequences: torch.Tensor) -> torch.Tensor:
        return build_key_padding_mask(sequences)

    @staticmethod
    def _last_hidden(x: torch.Tensor, sequences: torch.Tensor) -> torch.Tensor:
        seq_len = (sequences != 0).sum(dim=1).clamp(min=1)
        b_idx = torch.arange(sequences.size(0), device=sequences.device)
        return x[b_idx, seq_len - 1, :]

    @staticmethod
    def _mean_hidden(x: torch.Tensor, sequences: torch.Tensor) -> torch.Tensor:
        mask = (sequences != 0).float().unsqueeze(-1)
        summed = (x * mask).sum(dim=1)
        denom = mask.sum(dim=1).clamp(min=1.0)
        return summed / denom

    def encode(self, sequences: torch.Tensor, override_item_emb: torch.Tensor=None) -> torch.Tensor:
        nonpad = (sequences > 0).unsqueeze(-1).float()
        seq_idx = torch.clamp(sequences - 1, min=0)
        if override_item_emb is None:
            item_emb = self.item_embedding(seq_idx)
        else:
            item_emb = override_item_emb
        item_emb = item_emb * nonpad
        pos_ids = torch.arange(sequences.size(1), device=sequences.device).unsqueeze(0).expand_as(sequences)
        pos_emb = self.position_embedding(pos_ids)
        x = item_emb + pos_emb
        x = self.layer_norm(x)
        x = self.dropout(x)
        key_padding_mask = self._key_padding_mask(sequences)
        for block in self.transformer_blocks:
            x = block(x, attn_mask=None, key_padding_mask=key_padding_mask)
            x[sequences == 0] = 0.0
        if self.pooling == 'mean':
            final_user_repr = self._mean_hidden(x, sequences)
        else:
            final_user_repr = self._last_hidden(x, sequences)
        return final_user_repr

    def forward(self, sequences: torch.Tensor, override_item_emb: torch.Tensor=None) -> torch.Tensor:
        user_repr = self.encode(sequences, override_item_emb=override_item_emb)
        logits = torch.matmul(user_repr, self.item_embedding.weight.transpose(0, 1))
        return logits


============================================================
文件位置: /root/KG-SR-main/src/candidates/build.py
============================================================
from __future__ import annotations
from typing import Dict, List, Optional
import pandas as pd
from tqdm import tqdm
from src.candidates.predictors import predict_topk_for_train_sequences
from src.candidates.successors import build_successor_topn_for_train

def _merge_dedup_lists(a: List[int], b: List[int]) -> List[int]:
    seen = set()
    out: List[int] = []
    for x in a + b:
        if x not in seen:
            out.append(x)
            seen.add(x)
    return out

def build_candidates_for_train(dataset_name: str, window_size: int, model_name: Optional[str], K: int, N: int, save_csv: bool=True):
    pred_dict: Dict[str, pd.DataFrame] = predict_topk_for_train_sequences(dataset_name=dataset_name, window_size=window_size, model_name=model_name, K=K, save_csv=False)
    succ_df = build_successor_topn_for_train(dataset_name=dataset_name, window_size=window_size, N=N, save_csv=False)
    succ_col = succ_df['succ_topN'].tolist()
    results: Dict[str, pd.DataFrame] = {}
    for (mname, pdf) in pred_dict.items():
        assert len(pdf) == len(succ_df), '两种候选的行数不一致，请检查数据'
        merged_col: List[str] = []
        for i in tqdm(range(len(pdf)), desc=f'Merge[{mname}]'):
            topk = list(map(int, str(pdf.iloc[i]['topk_pred']).split(',')))
            succ = list(map(int, str(succ_col[i]).split(',')))
            merged = _merge_dedup_lists(topk, succ)
            merged_col.append(','.join(map(str, merged)))
        out = pdf.copy()
        out['succ_topN'] = succ_df['succ_topN']
        out['merged_candidates'] = merged_col
        if save_csv:
            from pathlib import Path
            mdir = Path('./saved') / dataset_name / mname / f'L_{window_size}'
            mdir.mkdir(parents=True, exist_ok=True)
            out_path = mdir / f'train_candidates_top{K}_succ{N}_merged.csv'
            out.to_csv(out_path, index=False)
            print(f'已保存：{out_path}')
        results[mname] = out
    return results


============================================================
文件位置: /root/KG-SR-main/src/candidates/__init__.py
============================================================



============================================================
文件位置: /root/KG-SR-main/src/candidates/successors.py
============================================================
from __future__ import annotations
from pathlib import Path
from typing import Dict, List, Tuple, Set
import pandas as pd
import numpy as np
from collections import defaultdict, Counter
from tqdm import tqdm
from src.utils.path_finder import PathFinder

def _load_user_sequences(processed_inter: Path) -> Dict[int, List[int]]:
    df = pd.read_csv(processed_inter, sep='\t', header=0)
    df.columns = ['user_id', 'item_id', 'rating', 'timestamp']
    df = df.sort_values(['user_id', 'timestamp'], kind='mergesort')
    seqs = df.groupby('user_id')['item_id'].apply(list).to_dict()
    return {int(u): list(map(int, items)) for (u, items) in seqs.items()}

def _build_training_transitions(user_seqs: Dict[int, List[int]]) -> Tuple[Dict[int, Dict[int, Counter]], Dict[int, Counter], Dict[int, Set[int]]]:
    user_to_succ: Dict[int, Dict[int, Counter]] = defaultdict(lambda : defaultdict(Counter))
    global_succ: Dict[int, Counter] = defaultdict(Counter)
    user_train_seen: Dict[int, Set[int]] = defaultdict(set)
    for (uid, seq) in user_seqs.items():
        if len(seq) < 3:
            continue
        train_seq = seq[:-2]
        if len(train_seq) < 2:
            user_train_seen[uid].update(train_seq)
            continue
        user_train_seen[uid].update(train_seq)
        for i in range(len(train_seq) - 1):
            t = int(train_seq[i])
            nxt = int(train_seq[i + 1])
            user_to_succ[uid][t][nxt] += 1
            global_succ[t][nxt] += 1
    return (user_to_succ, global_succ, user_train_seen)

def _rank_take(counter: Counter, limit: int, exclude: set) -> List[int]:
    out: List[int] = []
    for (item, _cnt) in counter.most_common():
        if item in exclude:
            continue
        out.append(item)
        if len(out) >= limit:
            break
    return out

def build_successor_topn_for_train(dataset_name: str, window_size: int, N: int, seed: int=42, save_csv: bool=True):
    rng = np.random.default_rng(seed)
    pf = PathFinder(dataset_name, window_size)
    processed_inter = pf.processed_dir() / f'{dataset_name}.inter'
    assert processed_inter.exists(), f'未找到：{processed_inter}（请先跑 preprocess.py）'
    user_seqs = _load_user_sequences(processed_inter)
    (user_to_succ, global_succ, user_train_seen) = _build_training_transitions(user_seqs)
    train_csv = pf.split_dir_path() / 'train.csv'
    df_train = pd.read_csv(train_csv)
    assert 'user_id' in df_train.columns and 'target_id' in df_train.columns, f'train.csv 需包含列：user_id, target_id；当前列={list(df_train.columns)}'
    link_path = pf.processed_dir() / f'{dataset_name}.link'
    link_df = pd.read_csv(link_path, sep='\t', dtype={'item_id:token': 'int32'})
    num_items = int(link_df['item_id:token'].max())
    all_items = set(range(1, num_items + 1))
    succ_lists: List[List[int]] = []
    for (_, row) in tqdm(df_train.iterrows(), total=len(df_train), desc='SuccessorTopN'):
        uid = int(row['user_id'])
        target = int(row['target_id'])
        chosen: List[int] = []
        used = {target}
        if uid in user_to_succ and target in user_to_succ[uid]:
            picked = _rank_take(user_to_succ[uid][target], N, used)
            chosen.extend(picked)
            used.update(picked)
        if len(chosen) < N and target in global_succ:
            fill = _rank_take(global_succ[target], N - len(chosen), used)
            chosen.extend(fill)
            used.update(fill)
        if len(chosen) < N:
            seen = user_train_seen.get(uid, set())
            cand = all_items - used - seen
            if not cand:
                cand = all_items - used
            cand_pool = list(cand)
            rng.shuffle(cand_pool)
            need = N - len(chosen)
            chosen.extend(cand_pool[:need])
        succ_lists.append(chosen[:N])
    out = df_train.copy()
    out['succ_topN'] = [','.join(map(str, xs)) for xs in succ_lists]
    if save_csv:
        save_dir = Path('./saved') / dataset_name / 'SuccessorTopN' / f'L_{window_size}'
        save_dir.mkdir(parents=True, exist_ok=True)
        out_path = save_dir / f'train_succ_top{N}.csv'
        out.to_csv(out_path, index=False)
        print(f'已保存：{out_path}')
    return out


============================================================
文件位置: /root/KG-SR-main/src/candidates/predictors.py
============================================================
from __future__ import annotations
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Iterable, Union
import yaml
import torch
from torch.utils.data import DataLoader
import pandas as pd
from tqdm import tqdm
from src.utils.gpu_utils import select_device
from src.utils.path_finder import PathFinder
from src.datasets.dataloader import RecDenoisingDataset
from src.datasets.kg_dataloader import get_kg_data
from src.models.sasrec import SASRec
from src.models.gru4rec import GRU4Rec
from src.models.ksr import KSR
from src.models.katrec import KATRec
MODEL_BUILDERS = {'sasrec': lambda cfg, n_items: SASRec(num_items=n_items, embedding_dim=cfg.embedding_dim, max_len=cfg.window_size, num_attention_heads=cfg.num_attention_heads, num_blocks=cfg.num_blocks, dropout_prob=cfg.dropout_prob), 'gru4rec': lambda cfg, n_items: GRU4Rec(num_items=n_items, embedding_dim=cfg.embedding_dim, hidden_size=cfg.hidden_size, num_layers=cfg.num_layers, dropout_prob=cfg.dropout_prob), 'ksr': lambda cfg, n_items: KSR(num_items=n_items, config=cfg), 'katrec': lambda cfg, n_items: KATRec(num_items=n_items, config=cfg)}

def _load_yaml(path: Path) -> dict:
    with open(path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def _load_checkpoint(ckpt_path: Path, map_location: Union[str, torch.device]):
    state = torch.load(ckpt_path, map_location=map_location)
    return state

def _rebuild_config_from_yaml(cfg_dict: dict):

    class C:
        ...
    c = C()
    for (k, v) in cfg_dict.items():
        setattr(c, k, v)
    return c

def _find_model_dirs(dataset_name: str, window_size: int, model_name: Optional[str]) -> List[Path]:
    base = Path('./saved') / dataset_name
    if not base.exists():
        return []
    dirs: List[Path] = []
    if model_name and model_name.upper() != 'ALL':
        cand = base / model_name / f'L_{window_size}'
        if (cand / 'best_model.pth.tar').exists():
            dirs.append(cand)
        return dirs
    for sub in base.iterdir():
        if not sub.is_dir():
            continue
        d = sub / f'L_{window_size}'
        if (d / 'best_model.pth.tar').exists():
            dirs.append(d)
    return dirs

def _build_model_from_dir(model_dir: Path, device: torch.device):
    cfg_yaml = model_dir / 'config.yaml'
    ckpt = model_dir / 'best_model.pth.tar'
    assert cfg_yaml.exists() and ckpt.exists(), f'缺少文件：{cfg_yaml} 或 {ckpt}'
    cfg_dict = _load_yaml(cfg_yaml)
    cfg = _rebuild_config_from_yaml(cfg_dict)
    pf = PathFinder(cfg.dataset_name, cfg.window_size)
    link_path = pf.processed_dir() / f'{cfg.dataset_name}.link'
    link_df = pd.read_csv(link_path, sep='\t', dtype={'item_id:token': 'int32'})
    num_items = int(link_df['item_id:token'].max())
    need_kg = str(cfg.model_name).lower() in ['ksr', 'katrec']
    if need_kg:
        kg_data = get_kg_data(cfg)
        cfg.num_entities = kg_data['num_entities']
        cfg.num_relations = kg_data['num_relations']
        if 'item_entity_map' in kg_data:
            cfg.item_entity_map = kg_data['item_entity_map']
        if str(cfg.model_name).lower() == 'katrec':
            cfg.num_users = kg_data['num_users']
            cfg.adj_matrix = kg_data['adj_matrix']
        elif str(cfg.model_name).lower() == 'ksr':
            cfg.item_relation_map_tensor = kg_data['item_relation_map_tensor']
    builder = MODEL_BUILDERS[str(cfg.model_name).lower()]
    model = builder(cfg, num_items)
    state = _load_checkpoint(ckpt, map_location=device)
    model.load_state_dict(state['model_state_dict'], strict=True)
    model.to(device).eval()
    return (model, cfg, num_items)

@torch.no_grad()
def predict_topk_for_train_sequences(dataset_name: str, window_size: int, model_name: Optional[str], K: int, batch_size: int=512, num_workers: int=2, save_csv: bool=True) -> Dict[str, pd.DataFrame]:
    device = select_device(prefer='auto', strategy='max_free', min_free_mem_gb=0.5, allow_cpu=True, verbose=True)
    pf = PathFinder(dataset_name, window_size)
    train_csv = pf.split_dir_path() / 'train.csv'
    assert train_csv.exists(), f'未找到训练集：{train_csv}（请先运行 preprocess.py）'
    dataset = RecDenoisingDataset(train_csv)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=torch.cuda.is_available())
    model_dirs = _find_model_dirs(dataset_name, window_size, model_name)
    assert model_dirs, f'未发现已训练模型（saved/{dataset_name}/**/L_{window_size}/best_model.pth.tar）'
    outputs: Dict[str, pd.DataFrame] = {}
    for mdir in model_dirs:
        mname = mdir.parent.name
        print(f'==> 载入模型：{mname} from {mdir}')
        (model, cfg, num_items) = _build_model_from_dir(mdir, device)
        all_preds: List[List[int]] = []
        progress = tqdm(loader, desc=f'Predict[{mname}]')
        for batch in progress:
            seq: torch.Tensor = batch['sequence'].to(device, non_blocking=True)
            logits: torch.Tensor = model(seq)
            with torch.no_grad():
                seq_items = torch.clamp(seq, min=0)
                mask = torch.zeros((seq.size(0), num_items), dtype=torch.bool, device=device)
                nonzero = seq_items > 0
                if nonzero.any():
                    idx_0based = torch.clamp(seq_items[nonzero] - 1, min=0)
                    row_idx = nonzero.nonzero(as_tuple=False)[:, 0]
                    mask[row_idx, idx_0based] = True
                logits = logits.masked_fill(mask, float('-inf'))
                k = min(K, num_items)
                topk_idx = torch.topk(logits, k=k, dim=1, largest=True, sorted=True).indices
                topk_items = (topk_idx + 1).detach().cpu().tolist()
                all_preds.extend(topk_items)
        df_in = pd.read_csv(train_csv)
        assert len(df_in) == len(all_preds), '预测条数与训练集不一致'
        out = df_in.copy()
        out['topk_pred'] = [','.join(map(str, x)) for x in all_preds]
        if save_csv:
            out_path = mdir / f'train_top{K}_pred.csv'
            out.to_csv(out_path, index=False)
            print(f'已保存：{out_path}')
        outputs[mname] = out
        del model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    return outputs


============================================================
文件位置: /root/KG-SR-main/main.py
============================================================
import argparse
import torch
import random
import numpy as np
import yaml
from datetime import datetime
from pathlib import Path
from src.datasets.dataloader import get_dataloaders
from src.datasets.kg_dataloader import get_kg_data
from src.models.sasrec import SASRec
from src.models.gru4rec import GRU4Rec
from src.models.ksr import KSR
from src.models.katrec import KATRec
from src.models.bert4rec import BERT4Rec
from src.trainers.trainer import Trainer
from src.utils.logging_utils import setup_logging
from src.utils.memory_monitor import setup_memory_efficient_environment, MemoryMonitor
from src.utils.code_snapshot import write_code_snapshot
from src.utils.path_finder import PathFinder

def main(config):
    startup_config = {k: v for (k, v) in vars(config).items()}
    setattr(config, '_startup_config', startup_config)
    torch.manual_seed(config.seed)
    random.seed(config.seed)
    np.random.seed(config.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(config.seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    setup_memory_efficient_environment()
    _mm = MemoryMonitor(config)
    _mm.print_memory_stats('Before dataloader')
    _mm.auto_adjust_config()
    pf = PathFinder(config.dataset_name, config.window_size)
    config.save_dir = pf.save_dir(config.model_name)
    try:
        project_root = Path(__file__).resolve().parent
        write_code_snapshot(save_dir=config.save_dir, project_root=project_root)
    except Exception as e:
        print(f'[WARN] Code snapshot failed: {e}')
    logger = setup_logging(level=getattr(config, 'log_level', 'INFO'), file_path=None, name='main')
    logger.info('===== Loading Data =====')
    (train_loader, valid_loader, test_loader, num_items) = get_dataloaders(config)
    config.num_items = num_items
    kg_data = None
    if config.model_name.lower() in ['ksr', 'katrec']:
        logger.info(f"Model '{config.model_name}' requires Knowledge Graph data. Loading...")
        kg_data = get_kg_data(config)
        config.num_entities = kg_data['num_entities']
        config.num_relations = kg_data['num_relations']
        if 'item_entity_map' in kg_data:
            config.item_entity_map = kg_data['item_entity_map']
        if config.model_name.lower() == 'katrec':
            config.num_users = kg_data['num_users']
            config.adj_matrix = kg_data['adj_matrix']
        elif config.model_name.lower() == 'ksr':
            config.item_relation_map_tensor = kg_data['item_relation_map_tensor']
    else:
        logger.info('Model does not require KG data.')
    logger.info('===== Initializing Model =====')
    model_name_lower = config.model_name.lower()
    if model_name_lower == 'sasrec':
        model = SASRec(num_items=num_items, embedding_dim=config.embedding_dim, max_len=config.window_size, num_attention_heads=config.num_attention_heads, num_blocks=config.num_blocks, dropout_prob=config.dropout_prob)
    elif model_name_lower == 'gru4rec':
        model = GRU4Rec(num_items=num_items, embedding_dim=config.embedding_dim, hidden_size=config.hidden_size, num_layers=config.num_layers, dropout_prob=config.dropout_prob)
    elif model_name_lower == 'ksr':
        model = KSR(num_items=num_items, config=config)
    elif model_name_lower == 'katrec':
        model = KATRec(num_items=num_items, config=config)
    elif model_name_lower == 'bert4rec':
        model = BERT4Rec(num_items=num_items, embedding_dim=config.embedding_dim, max_len=config.window_size, num_blocks=config.num_blocks, num_attention_heads=config.num_attention_heads, dropout_prob=config.dropout_prob, pooling=getattr(config, 'pooling', 'last'))
    else:
        raise ValueError(f'Unknown model name: {config.model_name}')
    total_params = sum((p.numel() for p in model.parameters() if p.requires_grad))
    logger.info(f'Model: {config.model_name}, Total Parameters: {total_params:,}')
    logger.info('===== Initializing Trainer =====')
    trainer = Trainer(model, train_loader, valid_loader, test_loader, config)
    trainer.fit()
    logger.info('===== Process Finished =====')
    logger.info(f'Results and logs saved in: {config.save_dir}')
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Main script for training sequential recommendation models.')
    parser.add_argument('--config', type=str, help='Path to a YAML config file.')
    parser.add_argument('--model_name', type=str, default='SASRec')
    parser.add_argument('--dataset_name', type=str, default='ml-1m')
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--log_level', type=str, default='INFO', help='DEBUG/INFO/WARNING/ERROR')
    parser.add_argument('--window_size', type=int, default=5)
    parser.add_argument('--epochs', type=int, default=200)
    parser.add_argument('--batch_size', type=int, default=256)
    parser.add_argument('--learning_rate', type=float, default=0.001)
    parser.add_argument('--weight_decay', type=float, default=0.0)
    parser.add_argument('--patience', type=int, default=10)
    parser.add_argument('--num_workers', type=int, default=4)
    parser.add_argument('--embedding_dim', type=int, default=64)
    parser.add_argument('--dropout_prob', type=float, default=0.2)
    parser.add_argument('--hidden_size', type=int, default=64)
    parser.add_argument('--num_layers', type=int, default=1)
    parser.add_argument('--num_blocks', type=int, default=2)
    parser.add_argument('--num_attention_heads', type=int, default=2)
    parser.add_argument('--kg_embedding_dim', type=int, default=64)
    parser.add_argument('--freeze_kg', type=bool, default=False)
    parser.add_argument('--gamma', type=float, default=0.5)
    parser.add_argument('--n_gcn_layers', type=int, default=2)
    parser.add_argument('--mess_dropout_prob', type=float, default=0.1)
    parser.add_argument('--kg_loss_lambda', type=float, default=0.1)
    parser.add_argument('--regs', type=str, default='[1e-5, 1e-5]')
    args = parser.parse_args()
    if args.config:
        with open(args.config, 'r') as f:
            config_dict = yaml.safe_load(f)
        parser.set_defaults(**config_dict)
        args = parser.parse_args()
    if isinstance(args.regs, str):
        args.regs = eval(args.regs)
    print('=' * 20 + ' Configuration ' + '=' * 20)
    for (k, v) in vars(args).items():
        print(f'{k}: {v}')
    print('=' * 55)
    main(args)


============================================================
文件位置: /root/KG-SR-main/dataset/processed/ml-1m/L_5/train.csv
============================================================
  1: user_id,sequence,target_id
  2: 0,"0,0,0,0,1",2
  3: 0,"0,0,0,1,2",3
  4: 0,"0,0,1,2,3",4
  5: 0,"0,1,2,3,4",5
... (文件共 642720 行，仅显示前5行)

============================================================
文件位置: /root/KG-SR-main/dataset/processed/ml-1m/L_5/valid.csv
============================================================
  1: user_id,sequence,target_id
  2: 0,"47,48,49,50,51",52
  3: 1,"169,170,171,172,173",174
  4: 2,"128,204,48,205,206",207
  5: 3,"214,138,215,216,217",218
... (文件共 6041 行，仅显示前5行)

============================================================
文件位置: /root/KG-SR-main/dataset/processed/ml-1m/L_5/test.csv
============================================================
  1: user_id,sequence,target_id
  2: 0,"48,49,50,51,52",53
  3: 1,"170,171,172,173,174",175
  4: 2,"204,48,205,206,207",208
  5: 3,"138,215,216,217,218",89
... (文件共 6041 行，仅显示前5行)

============================================================
文件位置: /root/KG-SR-main/dataset/processed/ml-1m/ml-1m.inter
============================================================
  1: user_id:token	item_id:token	rating:float	timestamp:float
  2: 0	1	4	978300019
  3: 0	2	5	978300055
  4: 0	3	4	978300055
  5: 0	4	5	978300055
... (文件共 999612 行，仅显示前5行)

============================================================
文件位置: /root/KG-SR-main/dataset/processed/ml-1m/ml-1m.kg
============================================================
  1: head_id:token	relation:token	tail_id:token
  2: m.011_mj	film.film.actor	m.08m_rw
  3: m.011_mj	film.film.actor	m.03d36rf
  4: m.011_mj	film.film.actor	m.02x2cys
  5: m.011_mj	film.film.actor	m.08lk66
... (文件共 192611 行，仅显示前5行)

============================================================
文件位置: /root/KG-SR-main/dataset/processed/ml-1m/ml-1m.link
============================================================
  1: item_id:token	entity_id:token
  2: 1	m.098s2w
  3: 2	m.0bt4g
  4: 3	m.0dr_4
  5: 4	m.023p33
... (文件共 3417 行，仅显示前5行)

============================================================
文件位置: /root/KG-SR-main/dataset/raw/ml-1m/ml-1m.inter
============================================================
  1: user_id:token	item_id:token	rating:float	timestamp:float
  2: 1	1193	5	978300760
  3: 1	661	3	978302109
  4: 1	914	3	978301968
  5: 1	3408	4	978300275
... (文件共 1000210 行，仅显示前5行)

============================================================
文件位置: /root/KG-SR-main/dataset/raw/ml-1m/ml-1m.kg
============================================================
  1: head_id:token	relation_id:token	tail_id:token
  2: m.016kpn	film.film.actor	m.0v9lwzh
  3: m.0v9lwzh	film.actor.film	m.016kpn
  4: m.016kpn	film.film.actor	m.01msrs
  5: m.01msrs	film.actor.film	m.016kpn
... (文件共 392422 行，仅显示前5行)

============================================================
文件位置: /root/KG-SR-main/dataset/raw/ml-1m/ml-1m.link
============================================================
  1: item_id:token	entity_id:token
  2: 1	m.0dyb1
  3: 2	m.09w353
  4: 3	m.0676dr
  5: 4	m.03vny7
... (文件共 3812 行，仅显示前5行)

============================================================
文件位置: /root/KG-SR-main/results/experiment_results.csv
============================================================
  1: timestamp,model_name,dataset_name,hit@10,ndcg@10,mrr,best_epoch,total_epochs,training_time,learning_rate,batch_size,embedding_dim,dropout_prob,window_size,save_dir
  2: 2025-09-08 10:05:16,KSR,ml-1m,0.26986754966887416,0.15385149883118687,0.13219623518305898,16,200,00:32:19,0.001,256,64,0.2,5,saved/ml-1m/KSR/L_5
  3: 2025-09-08 10:18:20,SASRec,ml-1m,0.29288079470198675,0.16125277967642473,0.13579717850843012,47,200,00:45:27,0.001,256,64,0.2,5,saved/ml-1m/SASRec/L_5
  4: 2025-09-08 10:21:09,BERT4Rec,ml-1m,0.28774834437086094,0.1619557428044199,0.1388408264576994,51,200,00:48:13,0.001,256,64,0.2,5,saved/ml-1m/BERT4Rec/L_5
  5: 2025-09-08 10:26:25,GRU4Rec,ml-1m,0.2731788079470199,0.15598864902723705,0.13480485196145164,61,200,00:53:29,0.001,256,64,0.2,5,saved/ml-1m/GRU4Rec/L_5

============================================================
分析完成!
结果已保存到: /root/KG-SR-main/folder_analysis_root.txt


============================================================
文件位置: folder_structure_analyzer.py
============================================================
import os
from pathlib import Path
import sys
from datetime import datetime
import io
import tokenize
from fnmatch import fnmatch

# ============ 基础输出 ============

class DualOutput:
    """双重输出：同时输出到控制台和文件"""
    def __init__(self, output_file):
        self.output_file = output_file
        self.file_handle = None
    
    def __enter__(self):
        self.file_handle = open(self.output_file, 'w', encoding='utf-8')
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.file_handle:
            self.file_handle.close()
    
    def print(self, *args, **kwargs):
        print(*args, **kwargs)
        if self.file_handle:
            print(*args, **kwargs, file=self.file_handle)

# ============ 忽略目录规则 ============

def should_ignore_dir(name: str, ignore_dirs_lower: set, ignore_patterns: set) -> bool:
    """
    判断目录名是否应被忽略：
    - 大小写不敏感的精确匹配（ignore_dirs_lower）
    - 通配符模式（ignore_patterns，大小写不敏感）
    """
    n = name.lower()
    if n in ignore_dirs_lower:
        return True
    # 通配符匹配不自带大小写不敏感，这里将模式和目标都降为小写再比
    for pat in ignore_patterns:
        if fnmatch(n, pat.lower()):
            return True
    return False

# ============ 树形结构 ============

def print_tree_structure(
    root_path, 
    prefix="", 
    ignore_dirs=None, 
    ignore_patterns=None, 
    is_last=True, 
    output=None
):
    # 默认忽略（统一小写）
    if ignore_dirs is None:
        ignore_dirs = {"saved", "scripts", "bert", "__pycache__"}
    if ignore_patterns is None:
        ignore_patterns = {"saved*", "*scripts*", "bert*"}
    ignore_dirs_lower = {d.lower() for d in ignore_dirs}

    root_path = Path(root_path)
    # 若根节点本身命中忽略，直接返回（一般不会把忽略目录当根）
    if root_path.is_dir() and should_ignore_dir(root_path.name, ignore_dirs_lower, ignore_patterns):
        return

    connector = "└── " if is_last else "├── "
    line = f"{prefix}{connector}{root_path.name}"
    (output.print if output else print)(line)
    
    if root_path.is_file():
        return
    
    try:
        children = []
        for c in root_path.iterdir():
            if c.is_dir() and should_ignore_dir(c.name, ignore_dirs_lower, ignore_patterns):
                continue
            children.append(c)
        # 目录优先
        children.sort(key=lambda x: (x.is_file(), x.name.lower()))
    except PermissionError:
        return
    
    extension = "    " if is_last else "│   "
    new_prefix = prefix + extension
    
    for i, child in enumerate(children):
        print_tree_structure(
            child, 
            new_prefix, 
            ignore_dirs=ignore_dirs, 
            ignore_patterns=ignore_patterns, 
            is_last=(i == len(children) - 1), 
            output=output
        )

# ============ 工具函数 ============

def is_binary_file(file_path):
    binary_extensions = {'.pth', '.tar', '.pkl', '.npy', '.npz', '.pyc', '.pt',
                         '.so', '.dll', '.exe', '.bin', '.dat'}
    file_path = Path(file_path)
    if file_path.suffix.lower() in binary_extensions:
        return True
    if len(file_path.suffixes) > 1:
        combined = ''.join(file_path.suffixes).lower()
        if any(ext in combined for ext in ['.pth.tar', '.pkl.gz']):
            return True
    return False


def read_file_content(file_path, encoding_list=None):
    if encoding_list is None:
        encoding_list = ['utf-8', 'gbk', 'gb2312', 'utf-16', 'latin-1']
    
    for enc in encoding_list:
        try:
            with open(file_path, 'r', encoding=enc) as f:
                return f.read()
        except (UnicodeDecodeError, UnicodeError):
            continue
        except Exception as e:
            return f"读取文件时发生错误: {e}"
    return "无法使用常见编码读取文件"


def _normalize_newlines_and_blank_lines(text: str) -> str:
    """统一换行并压缩连续空行为单个空行，去除行尾空白。"""
    text = text.replace('\r\n', '\n').replace('\r', '\n')
    lines = [ln.rstrip() for ln in text.split('\n')]
    cleaned = []
    blank_run = 0
    for ln in lines:
        if ln.strip() == "":
            blank_run += 1
            if blank_run > 1:
                continue
        else:
            blank_run = 0
        cleaned.append(ln)
    # 末尾留一个换行，阅读更舒服
    return ("\n".join(cleaned)).rstrip() + "\n"


def strip_py_comments_and_docstrings(source_code: str) -> str:
    """
    去掉 .py 文件中的注释与 docstring。
    首选 AST 重建（能彻底移除注释/文档字符串且避免奇怪换行），
    若失败则回退到 tokenize 方案，并做后处理以减少空行与“\ + 换行”现象。
    """
    # ---- 方案一：AST ----
    try:
        import ast

        class _RmDoc(ast.NodeTransformer):
            def _strip_doc(self, node):
                if node.body and isinstance(node.body[0], ast.Expr):
                    val = node.body[0].value
                    if isinstance(val, ast.Constant) and isinstance(val.value, str):
                        node.body = node.body[1:]
                return node

            def visit_Module(self, node):
                self.generic_visit(node)
                return self._strip_doc(node)

            def visit_FunctionDef(self, node):
                self.generic_visit(node)
                return self._strip_doc(node)

            def visit_AsyncFunctionDef(self, node):
                self.generic_visit(node)
                return self._strip_doc(node)

            def visit_ClassDef(self, node):
                self.generic_visit(node)
                return self._strip_doc(node)

        tree = ast.parse(source_code)
        tree = _RmDoc().visit(tree)
        ast.fix_missing_locations(tree)
        try:
            # Python 3.9+
            code = ast.unparse(tree)
        except Exception:
            # 某些环境可能没有 ast.unparse
            raise
        # AST 输出本身不含注释；再做统一换行与空行压缩
        return _normalize_newlines_and_blank_lines(code)
    except Exception:
        # ---- 方案二：tokenize 兜底 ----
        try:
            io_obj = io.StringIO(source_code)
            out = io.StringIO()
            prev_toktype = tokenize.INDENT
            last_lineno = -1
            last_col = 0

            for tok in tokenize.generate_tokens(io_obj.readline):
                tok_type, tok_str, start, end, line = tok

                # 跳过 # 注释
                if tok_type == tokenize.COMMENT:
                    continue

                # 跳过独立成句的字符串（docstring）
                if tok_type == tokenize.STRING:
                    if prev_toktype in (tokenize.INDENT, tokenize.NEWLINE):
                        if start[1] == 0:
                            prev_toktype = tok_type
                            continue

                if start[0] > last_lineno:
                    last_col = 0
                if start[1] > last_col:
                    out.write(" " * (start[1] - last_col))
                out.write(tok_str)
                prev_toktype = tok_type
                last_lineno, last_col = end

            cleaned = out.getvalue()
            # 统一行尾与空行；尽量避免奇怪的“\ + 换行”表现
            cleaned = _normalize_newlines_and_blank_lines(cleaned)
            return cleaned
        except Exception:
            # 实在失败就原样返回
            return source_code

# ============ 内容打印 ============

def print_file_content(file_path, file_type="full", output=None):
    file_path = Path(file_path)
    sep = f"\n{'='*60}"
    loc = f"文件位置: {file_path}"
    (output.print if output else print)(sep)
    (output.print if output else print)(loc)
    (output.print if output else print)("="*60)
    
    content = read_file_content(file_path)
    if content is None or (isinstance(content, str) and "错误" in content):
        (output.print if output else print)(content if content else "无法读取文件内容")
        return
    
    # 对 .py 文件做清洗
    if file_path.suffix.lower() == ".py":
        content = strip_py_comments_and_docstrings(content)
    
    if file_type == "full":
        (output.print if output else print)(content)
    else:
        lines = content.splitlines()
        for i, line in enumerate(lines[:5]):
            (output.print if output else print)(f"{i+1:3d}: {line}")
        if len(lines) > 5:
            (output.print if output else print)(f"... (文件共 {len(lines)} 行，仅显示前5行)")

# ============ 主流程 ============

def analyze_folder(target_folder, output_file=None):
    target_path = Path(target_folder)
    if not target_path.exists():
        print(f"错误: 路径 '{target_folder}' 不存在")
        return
    if not target_path.is_dir():
        print(f"错误: '{target_folder}' 不是一个文件夹")
        return
    
    if output_file is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        folder_name = target_path.name if target_path.name else "root"
        output_file = f"folder_analysis_{folder_name}.txt"

    # 解析“本脚本路径”和“输出文件路径”，用于排除
    try:
        self_script_path = Path(__file__).resolve()
    except NameError:
        # 某些打包/嵌入环境可能没有 __file__
        self_script_path = Path(sys.argv[0]).resolve()
    output_file_path = Path(output_file).resolve()

    # 统一的忽略配置（可按需修改/扩展）
    ignore_dirs = {"scripts", "bert",  ".git","__pycache__"}     # 精确名（大小写不敏感）
    ignore_patterns = {"saved*", ".git*", "*scripts*", "bert*"}            # 通配

    ignore_dirs_lower = {d.lower() for d in ignore_dirs}

    with DualOutput(output_file_path) as output:
        output.print(f"分析文件夹: {target_path.absolute()}")
        output.print(f"输出文件: {output_file_path}")
        output.print(f"分析时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        output.print("="*60)
        
        output.print("\n文件夹结构:")
        print_tree_structure(
            target_path, 
            output=output, 
            ignore_dirs=ignore_dirs, 
            ignore_patterns=ignore_patterns
        )
        
        output.print("\n\n" + "="*60)
        output.print("文件内容分析:")
        output.print("="*60)
        
        def process_dir(cur: Path):
            try:
                for item in cur.iterdir():
                    # ---- 目录处理：命中忽略则跳过整个子树 ----
                    if item.is_dir():
                        if should_ignore_dir(item.name, ignore_dirs_lower, ignore_patterns):
                            continue
                        process_dir(item)
                        continue

                    # ---- 文件过滤：排除本脚本与输出文件自身 ----
                    try:
                        item_real = item.resolve()
                    except Exception:
                        item_real = item

                    if item_real == self_script_path or item_real == output_file_path:
                        continue

                    # 二进制直接跳过
                    if is_binary_file(item_real):
                        continue

                    suffix = item_real.suffix.lower()
                    if suffix in ('.yaml', '.yml', '.py'):
                        print_file_content(item_real, "full", output)
                    elif suffix in ('.csv', '.inter', '.kg', '.link', '.txt', '.md', '.json', '.log'):
                        print_file_content(item_real, "partial", output)
            except PermissionError:
                output.print(f"权限错误: 无法访问 {cur}")
        
        process_dir(target_path)
        output.print("\n" + "="*60)
        output.print("分析完成!")
        output.print(f"结果已保存到: {output_file_path}")
    
    print(f"\n✓ 分析完成！结果已保存到: {output_file_path}")

# ============ CLI ============

def main():
    if len(sys.argv) < 2:
        print("使用方法: python folder_structure_analyzer.py <目标文件夹路径> [输出文件路径]")
        return
    target = sys.argv[1]
    output_file = sys.argv[2] if len(sys.argv) > 2 else None
    analyze_folder(target, output_file)

if __name__ == "__main__":
    if len(sys.argv) == 1:
        print("未指定目标文件夹，将分析当前目录...")
        analyze_folder(".")
    else:
        main()

============================================================
文件位置: main.py
============================================================
import argparse
import torch
import random
import numpy as np
import yaml
from datetime import datetime
from pathlib import Path
from src.datasets.dataloader import get_dataloaders
from src.datasets.kg_dataloader import get_kg_data
from src.models.sasrec import SASRec
from src.models.gru4rec import GRU4Rec
from src.models.ksr import KSR
from src.models.katrec import KATRec
from src.models.bert4rec import BERT4Rec
from src.trainers.trainer import Trainer
from src.utils.logging_utils import setup_logging
from src.utils.memory_monitor import setup_memory_efficient_environment, MemoryMonitor
from src.utils.code_snapshot import write_code_snapshot
from src.utils.path_finder import PathFinder

def main(config):
    startup_config = {k: v for (k, v) in vars(config).items()}
    setattr(config, '_startup_config', startup_config)

    torch.manual_seed(config.seed)
    random.seed(config.seed)
    np.random.seed(config.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(config.seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    setup_memory_efficient_environment()
    _mm = MemoryMonitor(config)
    _mm.print_memory_stats('Before dataloader')
    _mm.auto_adjust_config()

    pf = PathFinder(config.dataset_name, config.window_size)
    config.save_dir = pf.save_dir(config.model_name)

    try:
        project_root = Path(__file__).resolve().parent
        write_code_snapshot(save_dir=config.save_dir, project_root=project_root)
    except Exception as e:
        print(f'[WARN] Code snapshot failed: {e}')

    # 仅控制台日志，不写 main.log 文件
    logger = setup_logging(level=getattr(config, 'log_level', 'INFO'),
                           file_path=None,  # <<< 不再写入 main.log
                           name='main')

    logger.info('===== Loading Data =====')
    (train_loader, valid_loader, test_loader, num_items) = get_dataloaders(config)
    config.num_items = num_items

    kg_data = None
    if config.model_name.lower() in ['ksr', 'katrec']:
        logger.info(f"Model '{config.model_name}' requires Knowledge Graph data. Loading...")
        kg_data = get_kg_data(config)
        config.num_entities = kg_data['num_entities']
        config.num_relations = kg_data['num_relations']
        if 'item_entity_map' in kg_data:
            config.item_entity_map = kg_data['item_entity_map']
        if config.model_name.lower() == 'katrec':
            config.num_users = kg_data['num_users']
            config.adj_matrix = kg_data['adj_matrix']
        elif config.model_name.lower() == 'ksr':
            config.item_relation_map_tensor = kg_data['item_relation_map_tensor']
    else:
        logger.info('Model does not require KG data.')

    logger.info('===== Initializing Model =====')
    model_name_lower = config.model_name.lower()

    
    if model_name_lower == 'sasrec':
        model = SASRec(num_items=num_items,
                       embedding_dim=config.embedding_dim,
                       max_len=config.window_size,
                       num_attention_heads=config.num_attention_heads,
                       num_blocks=config.num_blocks,
                       dropout_prob=config.dropout_prob)
    elif model_name_lower == 'gru4rec':
        model = GRU4Rec(num_items=num_items,
                        embedding_dim=config.embedding_dim,
                        hidden_size=config.hidden_size,
                        num_layers=config.num_layers,
                        dropout_prob=config.dropout_prob)
    elif model_name_lower == 'ksr':
        model = KSR(num_items=num_items, config=config)
    elif model_name_lower == 'katrec':
        model = KATRec(num_items=num_items, config=config)
    elif model_name_lower == 'bert4rec':
            model = BERT4Rec(
            num_items=num_items,
            embedding_dim=config.embedding_dim,
            max_len=config.window_size,
            num_blocks=config.num_blocks,
            num_attention_heads=config.num_attention_heads,
            dropout_prob=config.dropout_prob,
            pooling=getattr(config, 'pooling', 'last')
        )
    else:
        raise ValueError(f'Unknown model name: {config.model_name}')

    total_params = sum((p.numel() for p in model.parameters() if p.requires_grad))
    logger.info(f'Model: {config.model_name}, Total Parameters: {total_params:,}')

    logger.info('===== Initializing Trainer =====')
    trainer = Trainer(model, train_loader, valid_loader, test_loader, config)
    trainer.fit()

    logger.info('===== Process Finished =====')
    logger.info(f'Results and logs saved in: {config.save_dir}')

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Main script for training sequential recommendation models.')
    parser.add_argument('--config', type=str, help='Path to a YAML config file.')
    parser.add_argument('--model_name', type=str, default='SASRec')
    parser.add_argument('--dataset_name', type=str, default='ml-1m')
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--log_level', type=str, default='INFO', help='DEBUG/INFO/WARNING/ERROR')
    parser.add_argument('--window_size', type=int, default=5)
    parser.add_argument('--epochs', type=int, default=200)
    parser.add_argument('--batch_size', type=int, default=256)
    parser.add_argument('--learning_rate', type=float, default=0.001)
    parser.add_argument('--weight_decay', type=float, default=0.0)
    parser.add_argument('--patience', type=int, default=10)
    parser.add_argument('--num_workers', type=int, default=4)
    parser.add_argument('--embedding_dim', type=int, default=64)
    parser.add_argument('--dropout_prob', type=float, default=0.2)
    parser.add_argument('--hidden_size', type=int, default=64)
    parser.add_argument('--num_layers', type=int, default=1)
    parser.add_argument('--num_blocks', type=int, default=2)
    parser.add_argument('--num_attention_heads', type=int, default=2)
    parser.add_argument('--kg_embedding_dim', type=int, default=64)
    parser.add_argument('--freeze_kg', type=bool, default=False)
    parser.add_argument('--gamma', type=float, default=0.5)
    parser.add_argument('--n_gcn_layers', type=int, default=2)
    parser.add_argument('--mess_dropout_prob', type=float, default=0.1)
    parser.add_argument('--kg_loss_lambda', type=float, default=0.1)
    parser.add_argument('--regs', type=str, default='[1e-5, 1e-5]')

    args = parser.parse_args()
    if args.config:
        with open(args.config, 'r') as f:
            config_dict = yaml.safe_load(f)
        parser.set_defaults(**config_dict)
        args = parser.parse_args()
    if isinstance(args.regs, str):
        args.regs = eval(args.regs)

    print('=' * 20 + ' Configuration ' + '=' * 20)
    for (k, v) in vars(args).items():
        print(f'{k}: {v}')
    print('=' * 55)
    main(args)

============================================================
文件位置: scripts/run_experiments.sh
============================================================
#!/usr/bin/env bash
set -euo pipefail

# ==============================
# KG-Corrector 并行实验启动（tmux 稳健版）
# 版本说明（去掉 GPU 相关逻辑）
# ==============================

PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$PROJECT_ROOT"

# ===== 可配置区域 =====
CONDA_ENV="recbole_env"      # 为空则不激活 conda
WL=5                      # <<< 手动设置：window_size 数值（示例为 128）
SESSION="mine${WL}"          # tmux 会话名，带上 WL 便于区分

# 固定顺序的模型与配置（按下标对齐）
MODELS=(SASRec GRU4Rec KSR KATRec BERT4Rec)
CONFIGS=(
  "configs/model/sasrec_ml1m.yaml"
  "configs/model/gru4rec_ml1m.yaml"
  "configs/model/ksr_ml1m.yaml"
  "configs/model/katrec_ml1m.yaml"
  "configs/model/bert4rec_ml1m.yaml"
)
# =====================

# 1) 长度一致性 + 配置文件存在性检查
if [[ "${#MODELS[@]}" -ne "${#CONFIGS[@]}" ]]; then
  echo "!! 配置错误：MODELS 数量(${#MODELS[@]}) 与 CONFIGS 数量(${#CONFIGS[@]}) 不一致。" >&2
  exit 1
fi
for i in "${!MODELS[@]}"; do
  [[ -n "${CONFIGS[$i]}" && -f "${CONFIGS[$i]}" ]] || {
    echo "!! Missing config for ${MODELS[$i]} -> ${CONFIGS[$i]}" >&2
    exit 1
  }
done

# 2) 解析 conda.sh 路径（如果安装了 conda）
CONDA_SH=""
if [[ -n "${CONDA_ENV}" ]] && command -v conda >/dev/null 2>&1; then
  CONDA_BASE="$(conda info --base 2>/dev/null || true)"
  if [[ -n "${CONDA_BASE}" && -f "${CONDA_BASE}/etc/profile.d/conda.sh" ]]; then
    CONDA_SH="${CONDA_BASE}/etc/profile.d/conda.sh"
  fi
fi

# 3) 必须有 tmux
if ! command -v tmux >/dev/null 2>&1; then
  echo "!! 未检测到 tmux，请安装或加入 PATH 后重试。" >&2
  exit 1
fi

# 4) 若会话已存在则清理（确保干净重启）
if tmux has-session -t "${SESSION}" 2>/dev/null; then
  tmux kill-session -t "${SESSION}"
fi

# 5) 创建会话 & 第一个窗口
tmux new-session -d -s "${SESSION}" -n "${MODELS[0]}" -c "${PROJECT_ROOT}"

# 6) 自检会话存在
if ! tmux has-session -t "${SESSION}" 2>/dev/null; then
  echo "!! tmux 会话创建失败（可能是 tmux socket 权限或环境问题）。" >&2
  exit 1
fi

# 7) 向某个窗口注入一套标准命令
send_job () {
  local win="$1"
  local cfg="$2"
  local wl="$3"
  tmux send-keys -t "${win}" "cd '${PROJECT_ROOT}'" C-m

  # Conda 激活（若需要）
  if [[ -n "${CONDA_ENV}" ]]; then
    if [[ -n "${CONDA_SH}" ]]; then
      tmux send-keys -t "${win}" "source '${CONDA_SH}'" C-m
      tmux send-keys -t "${win}" "conda activate '${CONDA_ENV}' || true" C-m
    else
      tmux send-keys -t "${win}" "[[ -f ~/.bashrc ]] && source ~/.bashrc" C-m
      tmux send-keys -t "${win}" "command -v conda >/dev/null 2>&1 && conda activate '${CONDA_ENV}' || true" C-m
    fi
  fi

  # 打印信息 & 执行训练；结束后保持交互
  tmux send-keys -t "${win}" 'echo "[info] PWD=$(pwd)"' C-m
  tmux send-keys -t "${win}" "echo \"[run] python main.py --config ${cfg} --window_size ${wl}\"" C-m
  tmux send-keys -t "${win}" "python main.py --config '${cfg}' --window_size ${wl}; ec=\$?; echo \"[exit] code=\$ec\"" C-m
  tmux send-keys -t "${win}" "exec bash" C-m
}

# 8) 注入第一个窗口
send_job "${SESSION}:0" "${CONFIGS[0]}" "${WL}"

# 9) 其余窗口
for i in "${!MODELS[@]}"; do
  [[ "$i" -eq 0 ]] && continue
  tmux new-window -t "${SESSION}" -n "${MODELS[$i]}" -c "${PROJECT_ROOT}"
  send_job "${SESSION}:$i" "${CONFIGS[$i]}" "${WL}"
done

# 10) 输出提示
echo "tmux 会话 '${SESSION}' 已启动（WL=${WL}），共 ${#MODELS[@]} 个窗口：${MODELS[*]}"
echo "进入会话：tmux attach -t ${SESSION}"

============================================================
文件位置: src/__init__.py
============================================================
# marks "src" as a package

============================================================
文件位置: src/candidates/__init__.py
============================================================
# 空文件即可，保证包可导入

============================================================
文件位置: src/candidates/build.py
============================================================
from __future__ import annotations
from typing import Dict, List, Optional
import pandas as pd
from tqdm import tqdm

from src.candidates.predictors import predict_topk_for_train_sequences
from src.candidates.successors import build_successor_topn_for_train


def _merge_dedup_lists(a: List[int], b: List[int]) -> List[int]:
    """按顺序去重合并（先保留 a 顺序，再补 b 中未出现的）"""
    seen = set()
    out: List[int] = []
    for x in a + b:
        if x not in seen:
            out.append(x)
            seen.add(x)
    return out


def build_candidates_for_train(
    dataset_name: str,
    window_size: int,
    model_name: Optional[str],  # 具体名或 'ALL'
    K: int,
    N: int,
    save_csv: bool = True,
):
    """
    同时构建两种候选（TopK 预测 & 后继 TopN），并按行合并去重：
      - model_name 支持 'ALL'：会分别输出每个模型的合并候选；
      - 返回 {模型名: DataFrame[user_id, sequence, target_id, topk_pred, succ_topN, merged_candidates]}
    """
    # 1) 预测候选
    pred_dict: Dict[str, pd.DataFrame] = predict_topk_for_train_sequences(
        dataset_name=dataset_name, window_size=window_size, model_name=model_name, K=K, save_csv=False
    )

    # 2) 后继候选（只算一次复用）
    succ_df = build_successor_topn_for_train(dataset_name=dataset_name, window_size=window_size, N=N, save_csv=False)
    succ_col = succ_df['succ_topN'].tolist()

    results: Dict[str, pd.DataFrame] = {}

    for mname, pdf in pred_dict.items():
        assert len(pdf) == len(succ_df), '两种候选的行数不一致，请检查数据'
        merged_col: List[str] = []

        for i in tqdm(range(len(pdf)), desc=f'Merge[{mname}]'):
            topk = list(map(int, str(pdf.iloc[i]['topk_pred']).split(',')))
            succ = list(map(int, str(succ_col[i]).split(',')))
            merged = _merge_dedup_lists(topk, succ)
            merged_col.append(','.join(map(str, merged)))

        out = pdf.copy()
        out['succ_topN'] = succ_df['succ_topN']
        out['merged_candidates'] = merged_col

        if save_csv:
            # 保存在各自模型目录下，便于追溯
            # NOTE: predictors.py 默认把预测保存到 saved/{dataset}/{Model}/L_{L}/train_topK_pred.csv
            # 这里保持同一目录，文件名标注 merged
            from pathlib import Path
            mdir = Path('./saved') / dataset_name / mname / f'L_{window_size}'
            mdir.mkdir(parents=True, exist_ok=True)
            out_path = mdir / f'train_candidates_top{K}_succ{N}_merged.csv'
            out.to_csv(out_path, index=False)
            print(f'已保存：{out_path}')

        results[mname] = out

    return results

============================================================
文件位置: src/candidates/predictors.py
============================================================
from __future__ import annotations
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Iterable, Union
import yaml
import torch
from torch.utils.data import DataLoader
import pandas as pd
from tqdm import tqdm
from src.utils.gpu_utils import select_device
from src.utils.path_finder import PathFinder
from src.datasets.dataloader import RecDenoisingDataset
from src.datasets.kg_dataloader import get_kg_data
from src.models.sasrec import SASRec
from src.models.gru4rec import GRU4Rec
from src.models.ksr import KSR
from src.models.katrec import KATRec

MODEL_BUILDERS = {
    'sasrec': lambda cfg, n_items: SASRec(
        num_items=n_items,
        embedding_dim=cfg.embedding_dim,
        max_len=cfg.window_size,
        num_attention_heads=cfg.num_attention_heads,
        num_blocks=cfg.num_blocks,
        dropout_prob=cfg.dropout_prob
    ),
    'gru4rec': lambda cfg, n_items: GRU4Rec(
        num_items=n_items,
        embedding_dim=cfg.embedding_dim,
        hidden_size=cfg.hidden_size,
        num_layers=cfg.num_layers,
        dropout_prob=cfg.dropout_prob
    ),
    'ksr': lambda cfg, n_items: KSR(num_items=n_items, config=cfg),
    'katrec': lambda cfg, n_items: KATRec(num_items=n_items, config=cfg)
}

def _load_yaml(path: Path) -> dict:
    with open(path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def _load_checkpoint(ckpt_path: Path, map_location: Union[str, torch.device]):
    state = torch.load(ckpt_path, map_location=map_location)
    return state

def _rebuild_config_from_yaml(cfg_dict: dict):
    class C:
        ...
    c = C()
    for (k, v) in cfg_dict.items():
        setattr(c, k, v)
    return c

def _find_model_dirs(dataset_name: str, window_size: int, model_name: Optional[str]) -> List[Path]:
    base = Path('./saved') / dataset_name
    if not base.exists():
        return []
    dirs: List[Path] = []
    if model_name and model_name.upper() != 'ALL':
        cand = base / model_name / f'L_{window_size}'
        if (cand / 'best_model.pth.tar').exists():
            dirs.append(cand)
        return dirs
    for sub in base.iterdir():
        if not sub.is_dir():
            continue
        d = sub / f'L_{window_size}'
        if (d / 'best_model.pth.tar').exists():
            dirs.append(d)
    return dirs

def _build_model_from_dir(model_dir: Path, device: torch.device):
    cfg_yaml = model_dir / 'config.yaml'
    ckpt = model_dir / 'best_model.pth.tar'
    assert cfg_yaml.exists() and ckpt.exists(), f'缺少文件：{cfg_yaml} 或 {ckpt}'
    cfg_dict = _load_yaml(cfg_yaml)
    cfg = _rebuild_config_from_yaml(cfg_dict)

    pf = PathFinder(cfg.dataset_name, cfg.window_size)

    # === 用 link 统计物品数（替代 item_map.csv） ===
    link_path = pf.processed_dir() / f'{cfg.dataset_name}.link'
    link_df = pd.read_csv(link_path, sep='\t', dtype={'item_id:token': 'int32'})
    num_items = int(link_df['item_id:token'].max())

    need_kg = str(cfg.model_name).lower() in ['ksr', 'katrec']
    if need_kg:
        kg_data = get_kg_data(cfg)
        cfg.num_entities = kg_data['num_entities']
        cfg.num_relations = kg_data['num_relations']
        if 'item_entity_map' in kg_data:
            cfg.item_entity_map = kg_data['item_entity_map']
        if str(cfg.model_name).lower() == 'katrec':
            cfg.num_users = kg_data['num_users']
            cfg.adj_matrix = kg_data['adj_matrix']
        elif str(cfg.model_name).lower() == 'ksr':
            cfg.item_relation_map_tensor = kg_data['item_relation_map_tensor']

    builder = MODEL_BUILDERS[str(cfg.model_name).lower()]
    model = builder(cfg, num_items)
    state = _load_checkpoint(ckpt, map_location=device)
    model.load_state_dict(state['model_state_dict'], strict=True)
    model.to(device).eval()
    return (model, cfg, num_items)

@torch.no_grad()
def predict_topk_for_train_sequences(
    dataset_name: str,
    window_size: int,
    model_name: Optional[str],
    K: int,
    batch_size: int=512,
    num_workers: int=2,
    save_csv: bool=True
) -> Dict[str, pd.DataFrame]:
    device = select_device(prefer='auto', strategy='max_free', min_free_mem_gb=0.5, allow_cpu=True, verbose=True)
    pf = PathFinder(dataset_name, window_size)
    train_csv = pf.split_dir_path() / 'train.csv'
    assert train_csv.exists(), f'未找到训练集：{train_csv}（请先运行 preprocess.py）'

    dataset = RecDenoisingDataset(train_csv)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=torch.cuda.is_available())

    model_dirs = _find_model_dirs(dataset_name, window_size, model_name)
    assert model_dirs, f'未发现已训练模型（saved/{dataset_name}/**/L_{window_size}/best_model.pth.tar）'

    outputs: Dict[str, pd.DataFrame] = {}
    for mdir in model_dirs:
        mname = mdir.parent.name
        print(f'==> 载入模型：{mname} from {mdir}')
        (model, cfg, num_items) = _build_model_from_dir(mdir, device)

        all_preds: List[List[int]] = []
        progress = tqdm(loader, desc=f'Predict[{mname}]')
        for batch in progress:
            seq: torch.Tensor = batch['sequence'].to(device, non_blocking=True)
            logits: torch.Tensor = model(seq)

            with torch.no_grad():
                seq_items = torch.clamp(seq, min=0)
                mask = torch.zeros((seq.size(0), num_items), dtype=torch.bool, device=device)
                nonzero = seq_items > 0
                if nonzero.any():
                    idx_0based = torch.clamp(seq_items[nonzero] - 1, min=0)
                    row_idx = nonzero.nonzero(as_tuple=False)[:, 0]
                    mask[row_idx, idx_0based] = True
                logits = logits.masked_fill(mask, float('-inf'))

                k = min(K, num_items)
                topk_idx = torch.topk(logits, k=k, dim=1, largest=True, sorted=True).indices
                topk_items = (topk_idx + 1).detach().cpu().tolist()
                all_preds.extend(topk_items)

        df_in = pd.read_csv(train_csv)
        assert len(df_in) == len(all_preds), '预测条数与训练集不一致'
        out = df_in.copy()
        out['topk_pred'] = [','.join(map(str, x)) for x in all_preds]

        if save_csv:
            out_path = mdir / f'train_top{K}_pred.csv'
            out.to_csv(out_path, index=False)
            print(f'已保存：{out_path}')

        outputs[mname] = out

        del model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    return outputs

============================================================
文件位置: src/candidates/successors.py
============================================================
from __future__ import annotations
from pathlib import Path
from typing import Dict, List, Tuple, Set
import pandas as pd
import numpy as np
from collections import defaultdict, Counter
from tqdm import tqdm
from src.utils.path_finder import PathFinder

def _load_user_sequences(processed_inter: Path) -> Dict[int, List[int]]:
    df = pd.read_csv(processed_inter, sep='\t', header=0)
    df.columns = ['user_id', 'item_id', 'rating', 'timestamp']
    df = df.sort_values(['user_id', 'timestamp'], kind='mergesort')
    seqs = df.groupby('user_id')['item_id'].apply(list).to_dict()
    return {int(u): list(map(int, items)) for (u, items) in seqs.items()}

def _build_training_transitions(user_seqs: Dict[int, List[int]]) -> Tuple[Dict[int, Dict[int, Counter]], Dict[int, Counter], Dict[int, Set[int]]]:
    user_to_succ: Dict[int, Dict[int, Counter]] = defaultdict(lambda : defaultdict(Counter))
    global_succ: Dict[int, Counter] = defaultdict(Counter)
    user_train_seen: Dict[int, Set[int]] = defaultdict(set)
    for (uid, seq) in user_seqs.items():
        if len(seq) < 3:
            continue
        train_seq = seq[:-2]
        if len(train_seq) < 2:
            user_train_seen[uid].update(train_seq)
            continue
        user_train_seen[uid].update(train_seq)
        for i in range(len(train_seq) - 1):
            t = int(train_seq[i])
            nxt = int(train_seq[i + 1])
            user_to_succ[uid][t][nxt] += 1
            global_succ[t][nxt] += 1
    return (user_to_succ, global_succ, user_train_seen)

def _rank_take(counter: Counter, limit: int, exclude: set) -> List[int]:
    out: List[int] = []
    for (item, _cnt) in counter.most_common():
        if item in exclude:
            continue
        out.append(item)
        if len(out) >= limit:
            break
    return out

def build_successor_topn_for_train(dataset_name: str, window_size: int, N: int, seed: int=42, save_csv: bool=True):
    rng = np.random.default_rng(seed)
    pf = PathFinder(dataset_name, window_size)
    processed_inter = pf.processed_dir() / f'{dataset_name}.inter'
    assert processed_inter.exists(), f'未找到：{processed_inter}（请先跑 preprocess.py）'

    user_seqs = _load_user_sequences(processed_inter)
    (user_to_succ, global_succ, user_train_seen) = _build_training_transitions(user_seqs)

    train_csv = pf.split_dir_path() / 'train.csv'
    df_train = pd.read_csv(train_csv)
    assert 'user_id' in df_train.columns and 'target_id' in df_train.columns, f'train.csv 需包含列：user_id, target_id；当前列={list(df_train.columns)}'

    # === 用 link 统计 num_items（替代 item_map.csv） ===
    link_path = pf.processed_dir() / f'{dataset_name}.link'
    link_df = pd.read_csv(link_path, sep='\t', dtype={'item_id:token': 'int32'})
    num_items = int(link_df['item_id:token'].max())
    all_items = set(range(1, num_items + 1))

    succ_lists: List[List[int]] = []
    for (_, row) in tqdm(df_train.iterrows(), total=len(df_train), desc='SuccessorTopN'):
        uid = int(row['user_id'])
        target = int(row['target_id'])
        chosen: List[int] = []
        used = {target}

        if uid in user_to_succ and target in user_to_succ[uid]:
            picked = _rank_take(user_to_succ[uid][target], N, used)
            chosen.extend(picked)
            used.update(picked)

        if len(chosen) < N and target in global_succ:
            fill = _rank_take(global_succ[target], N - len(chosen), used)
            chosen.extend(fill)
            used.update(fill)

        if len(chosen) < N:
            seen = user_train_seen.get(uid, set())
            cand = all_items - used - seen
            if not cand:
                cand = all_items - used
            cand_pool = list(cand)
            rng.shuffle(cand_pool)
            need = N - len(chosen)
            chosen.extend(cand_pool[:need])

        succ_lists.append(chosen[:N])

    out = df_train.copy()
    out['succ_topN'] = [','.join(map(str, xs)) for xs in succ_lists]

    if save_csv:
        save_dir = Path('./saved') / dataset_name / 'SuccessorTopN' / f'L_{window_size}'
        save_dir.mkdir(parents=True, exist_ok=True)
        out_path = save_dir / f'train_succ_top{N}.csv'
        out.to_csv(out_path, index=False)
        print(f'已保存：{out_path}')

    return out

============================================================
文件位置: src/datasets/__init__.py
============================================================
# datasets package

============================================================
文件位置: src/datasets/dataloader.py
============================================================
import torch
import pandas as pd
from torch.utils.data import Dataset, DataLoader
import gc
import numpy as np
import random
from src.utils.path_finder import PathFinder

class RecDenoisingDataset(Dataset):

    def __init__(self, data_path):
        dtypes = {'user_id': 'int32', 'sequence': 'string', 'target_id': 'int32'}
        self.data_df = pd.read_csv(data_path, dtype=dtypes)
        print(f'Loaded {len(self.data_df)} samples from {data_path}')
        gc.collect()

    def __len__(self):
        return len(self.data_df)

    def __getitem__(self, idx):
        row = self.data_df.iloc[idx]
        user_id = int(row['user_id'])
        sequence = list(map(int, row['sequence'].split(',')))
        target_id = int(row['target_id'])
        return {
            'user_id': torch.tensor(user_id, dtype=torch.long),
            'sequence': torch.tensor(sequence, dtype=torch.long),
            'target_id': torch.tensor(target_id, dtype=torch.long)
        }

def get_dataloaders(config):
    pf = PathFinder(config.dataset_name, config.window_size)
    dp = pf.data_paths(
        use_corrected_train=bool(getattr(config, 'use_corrected_train', False)),
        corrected_train_filename=getattr(config, 'corrected_train_filename', 'corrected_train.csv'),
        ensure_dirs=True
    )
    (train_path, valid_path, test_path) = (dp.train_csv, dp.valid_csv, dp.test_csv)

    if not all([train_path.exists(), valid_path.exists(), test_path.exists()]):
        raise FileNotFoundError(
            f'One or more processed data files not found in {dp.split_dir}. '
            f'Please run preprocess.py first with --window_size {config.window_size}'
        )

    train_dataset = RecDenoisingDataset(train_path)
    valid_dataset = RecDenoisingDataset(valid_path)
    test_dataset = RecDenoisingDataset(test_path)

    # === 用 processed link 统计物品数（取代 item_map.csv） ===
    link_path = dp.processed_link
    if not link_path.exists():
        raise FileNotFoundError(f'Processed link file not found: {link_path}. Please run preprocess.py first.')

    link_df = pd.read_csv(link_path, sep='\t', dtype={'item_id:token': 'int32'})
    num_items = int(link_df['item_id:token'].max())  # 1..N，0 为 PAD
    del link_df
    gc.collect()

    optimized_num_workers = min(getattr(config, 'num_workers', 0), 2)
    optimized_batch_size = getattr(config, 'optimized_batch_size', getattr(config, 'batch_size', 256))
    seed = int(getattr(config, 'seed', 42))
    g = torch.Generator()
    g.manual_seed(seed)

    def _worker_init_fn(worker_id):
        s = seed + worker_id
        np.random.seed(s)
        random.seed(s)
        torch.manual_seed(s)

    common_loader_args = {
        'batch_size': optimized_batch_size,
        'num_workers': optimized_num_workers,
        'pin_memory': torch.cuda.is_available(),
        'persistent_workers': optimized_num_workers > 0,
        'worker_init_fn': _worker_init_fn if optimized_num_workers > 0 else None,
        'generator': g
    }
    if optimized_num_workers > 0:
        common_loader_args['prefetch_factor'] = 2

    train_loader = DataLoader(train_dataset, shuffle=True, drop_last=True, **common_loader_args)
    valid_loader = DataLoader(valid_dataset, shuffle=False, drop_last=False, **common_loader_args)
    test_loader = DataLoader(test_dataset, shuffle=False, drop_last=False, **common_loader_args)

    print(f'Number of items (EXCLUDING padding): {num_items}')
    print(f'Optimized batch size: {optimized_batch_size}')
    print(f'Optimized num workers: {optimized_num_workers}')
    print(f'Train loader: {len(train_loader.dataset)} samples, {len(train_loader)} batches.')
    print(f'Validation loader: {len(valid_loader.dataset)} samples, {len(valid_loader)} batches.')
    print(f'Test loader: {len(test_loader.dataset)} samples, {len(test_loader)} batches.')

    return (train_loader, valid_loader, test_loader, num_items)

============================================================
文件位置: src/datasets/kg_dataloader.py
============================================================
import pandas as pd
from pathlib import Path
import torch
from collections import defaultdict
import numpy as np
import scipy.sparse as sp
from tqdm import tqdm
from src.utils.path_finder import PathFinder

def get_adj_matrix(dataset_name, num_users, num_entities, item_entity_map, kg_df):
    print('--- Building Adjacency Matrix for KATRec ---')
    t1 = pd.Timestamp.now()

    adj_mat = sp.lil_matrix((num_users + num_entities, num_users + num_entities), dtype=np.float32)

    inter_path = PathFinder(dataset_name, window_size=0).processed_dir() / f'{dataset_name}.inter'
    inter_df = pd.read_csv(inter_path, sep='\t', usecols=[0, 1], names=['user_id', 'item_id'], header=0)
    inter_df['entity_id'] = inter_df['item_id'].map(item_entity_map)
    inter_df.dropna(subset=['entity_id'], inplace=True)
    inter_df['entity_id'] = inter_df['entity_id'].astype(int)

    rows = inter_df['user_id'].values
    cols = inter_df['entity_id'].values + num_users
    adj_mat[rows, cols] = 1
    adj_mat[cols, rows] = 1
    print(f'Added {len(inter_df)} user-item interactions to matrix.')

    rows = kg_df['head'].values.astype(int) + num_users
    cols = kg_df['tail'].values.astype(int) + num_users
    adj_mat[rows, cols] = 1
    adj_mat[cols, rows] = 1
    print(f'Added {len(kg_df)} KG triples to matrix.')

    adj_mat = adj_mat.tocsr()
    rowsum = np.array(adj_mat.sum(axis=1))
    d_inv_sqrt = np.power(rowsum, -0.5).flatten()
    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.0
    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)
    norm_adj = adj_mat.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()

    indices = torch.from_numpy(np.vstack((norm_adj.row, norm_adj.col))).long()
    values = torch.from_numpy(norm_adj.data)
    shape = torch.Size(norm_adj.shape)
    norm_adj_tensor = torch.sparse.FloatTensor(indices, values, shape)

    t2 = pd.Timestamp.now()
    print(f'Adjacency matrix created and normalized in {(t2 - t1).total_seconds():.2f}s. Shape: {norm_adj.shape}')
    return norm_adj_tensor

def get_kg_data(config):
    dataset_name = config.dataset_name
    pf = PathFinder(dataset_name, getattr(config, 'window_size', 50))
    processed_dir = pf.processed_dir()

    # 以 link / user 作为唯一真相（替代 item_map.csv / user_map.csv）
    link_path = processed_dir / f'{dataset_name}.link'
    user_path = processed_dir / f'{dataset_name}.user'
    kg_path = processed_dir / f'{dataset_name}.kg'

    if not link_path.exists() or not user_path.exists() or not kg_path.exists():
        raise FileNotFoundError(
            f'Required files not found. Please run preprocess.py first. '
            f'Missing one of: {link_path}, {user_path}, {kg_path}'
        )

    print('--- Loading KG and Link/User data ---')

    # 用户数
    user_df = pd.read_csv(user_path, sep='\t')
    # 兼容仅一列或两列
    if 'user_id:token' in user_df.columns:
        num_users = int(user_df.shape[0])  # 连续 0..N-1
    else:
        # 回退：若无列名，尝试第一列
        num_users = int(user_df.iloc[:, 0].nunique())

    # 物品-实体映射
    link_df = pd.read_csv(link_path, sep='\t', dtype={'item_id:token': 'int32', 'entity_id:token': 'string'})
    inferred_num_items = int(link_df['item_id:token'].max())
    num_items_local = int(getattr(config, 'num_items', inferred_num_items))
    if num_items_local < inferred_num_items:
        print(f'[KG] Warning: config.num_items ({num_items_local}) < inferred ({inferred_num_items}). Using inferred value.')
        num_items_local = inferred_num_items
        try:
            setattr(config, 'num_items', num_items_local)
        except Exception:
            pass

    # 读取 processed KG
    kg_df_raw = pd.read_csv(kg_path, sep='\t')
    # 统一列名到 head/relation/tail（原文件头：head_id:token / relation:token / tail_id:token）
    kg_df_raw.columns = ['head', 'relation', 'tail']

    # 建实体/关系词典
    all_entities_str = sorted(list(set(kg_df_raw['head']).union(set(kg_df_raw['tail']))))
    entity_map = {e: i for (i, e) in enumerate(all_entities_str)}
    all_relations_str = sorted(list(set(kg_df_raw['relation'])))
    relation_map = {r: i + 1 for (i, r) in enumerate(all_relations_str)}  # 0 预留 padding
    num_entities = len(entity_map)
    num_relations = len(relation_map) + 1

    # item_id(1-based) -> 实体id(int)，来自 link
    item_id_to_entity_str = pd.Series(
        link_df['entity_id:token'].values,
        index=link_df['item_id:token'].values
    ).to_dict()
    item_entity_map = {
        int(item_id): entity_map[ent_str]
        for (item_id, ent_str) in item_id_to_entity_str.items()
        if isinstance(ent_str, str) and ent_str in entity_map
    }

    # KG 映射为 index
    kg_df_mapped = kg_df_raw.copy()
    kg_df_mapped['head'] = kg_df_mapped['head'].map(entity_map)
    kg_df_mapped['relation'] = kg_df_mapped['relation'].map(relation_map)
    kg_df_mapped['tail'] = kg_df_mapped['tail'].map(entity_map)
    kg_df_mapped.dropna(inplace=True)
    kg_df_mapped[['head', 'relation', 'tail']] = kg_df_mapped[['head', 'relation', 'tail']].astype(int)

    # 为 KSR 构建 item->topR relations（可选）
    R_MAX = getattr(config, 'max_relations_per_item', 8)
    if R_MAX < 1:
        R_MAX = 1

    head_rel_counts = kg_df_mapped.groupby(['head', 'relation']).size().reset_index(name='cnt')
    rel_list_by_head = {}
    for (head, grp) in head_rel_counts.groupby('head'):
        grp_sorted = grp.sort_values('cnt', ascending=False)
        rel_list_by_head[head] = grp_sorted['relation'].tolist()

    item_relation_map_tensor = torch.zeros(num_items_local, R_MAX, dtype=torch.long)
    filled_items = 0
    for (item_id_1, ent_id) in item_entity_map.items():
        zero_idx = int(item_id_1) - 1
        if zero_idx < 0 or zero_idx >= num_items_local:
            continue
        if ent_id < 0 or ent_id >= num_entities:
            continue
        rels = rel_list_by_head.get(ent_id, [])
        if not rels:
            continue
        rels = rels[:R_MAX]
        item_relation_map_tensor[zero_idx, :len(rels)] = torch.tensor(rels, dtype=torch.long)
        filled_items += 1
    print(f'[KSR] Built item_relation_map_tensor: shape={tuple(item_relation_map_tensor.shape)}, filled_items={filled_items}, R_MAX={R_MAX}')

    # 打包不同模型需求
    if config.model_name.lower() in ['ksr']:
        return {
            'num_users': num_users,
            'num_entities': num_entities,
            'num_relations': num_relations,
            'item_entity_map': item_entity_map,
            'item_relation_map_tensor': item_relation_map_tensor,
            'kg_triples_df': kg_df_mapped
        }
    elif config.model_name.lower() == 'katrec':
        adj_matrix = get_adj_matrix(dataset_name, num_users, num_entities, item_entity_map, kg_df_mapped)
        return {
            'num_users': num_users,
            'num_entities': num_entities,
            'num_relations': num_relations,
            'adj_matrix': adj_matrix,
            'item_entity_map': item_entity_map,
            'kg_triples_df': kg_df_mapped
        }
    return {}

============================================================
文件位置: src/datasets/preprocess.py
============================================================
import pandas as pd
import argparse
from pathlib import Path
from tqdm import tqdm
from src.utils.path_finder import PathFinder

class DataPreprocessor:

    def __init__(self, config):
        self.config = config
        self.dataset_name = config.dataset_name
        self.window_size = config.window_size
        self.min_core = config.min_core
        self.max_len = config.max_len

        self.pf = PathFinder(self.dataset_name, self.window_size)
        dp = self.pf.data_paths(ensure_dirs=True)

        self.data_dir = self.pf.data_dir()
        self.raw_data_dir = self.pf.raw_dir()
        self.base_processed_dir = dp.base_processed
        self.split_data_dir = dp.split_dir

        self.raw_file_path = dp.raw_inter
        self.link_file_path = dp.raw_link
        self.item_file_path = dp.raw_item

        print('#' * 20, ' Configuration ', '#' * 20)
        print(f'Dataset Name: {self.dataset_name}')
        print(f'Raw data file: {self.raw_file_path}')
        print(f'Entity link file: {self.link_file_path}')
        print(f'Raw item file: {self.item_file_path}')
        print(f'Base processed data path: {self.base_processed_dir}')
        print(f'L-dependent splits path: {self.split_data_dir}')
        print(f'Sliding Window Size (L): {self.window_size}')
        print(f'Min-Core for Filtering (K): {self.min_core}')
        print(f'Max History Length (for truncation): {self.max_len}')
        print('#' * 75)

    def _apply_k_core_filtering(self, df, min_core):
        print(f'\n--- Step 1a: Applying {min_core}-Core Filtering ---')
        while True:
            initial_rows = len(df)
            user_counts = df['user_id'].value_counts(dropna=False)
            valid_users = user_counts[user_counts >= min_core].index
            df = df[df['user_id'].isin(valid_users)]

            item_counts = df['item_id'].value_counts(dropna=False)
            valid_items = item_counts[item_counts >= min_core].index
            df = df[df['item_id'].isin(valid_items)]

            if len(df) == initial_rows:
                break
        print(f'Filtering complete. Final data has {len(df)} interactions.')
        return df

    def run(self):
        print('\n--- Step 1: Loading Data ---')
        df = pd.read_csv(self.raw_file_path, sep='\t', header=0, engine='python')
        df.columns = ['user_id', 'item_id', 'rating', 'timestamp']
        df['user_id'] = df['user_id'].astype(str)
        df['item_id'] = df['item_id'].astype(str)
        print(f'Original data loaded: {len(df)} interactions.')

        # k-core
        df = self._apply_k_core_filtering(df, self.min_core)
        print(f"User count after filtering: {df['user_id'].nunique()}")
        print(f"Item count after filtering: {df['item_id'].nunique()}")

        df = df.sort_values(by=['user_id', 'timestamp'], ascending=True)

        # --- User remap ---
        print('\n--- Step 2: Create and Apply User ID Mapping ---')
        unique_users = df['user_id'].unique()
        sorted_unique_users = sorted(unique_users, key=lambda x: int(x) if x.isdigit() else x)
        user_map = {original_id: new_id for (new_id, original_id) in enumerate(sorted_unique_users)}
        df['user_id'] = df['user_id'].map(user_map)
        print(f'Created and applied mapping for {len(user_map)} users.')

        # --- Item remap (顺序出现分配，保证紧凑 1..N；0 为 PAD) ---
        print('\n--- Step 3: Create and Apply Item ID Mapping ---')
        item_map = {}
        next_item_id = 1
        user_sequences_original_items = df.groupby('user_id')['item_id'].apply(list)
        sorted_mapped_user_ids = sorted(user_sequences_original_items.index.tolist())
        for mapped_user_id in tqdm(sorted_mapped_user_ids, desc='Creating Item Map'):
            sequence = user_sequences_original_items[mapped_user_id]
            for original_item_id in sequence:
                if original_item_id not in item_map:
                    item_map[original_item_id] = next_item_id
                    next_item_id += 1
        df['item_id'] = df['item_id'].map(item_map)
        print(f'Created and applied mapping for {len(item_map)} items (IDs from 1 to {next_item_id - 1}). Token 0 is reserved for padding.')

        # --- 存 remapped inter ---
        print('\n--- Step 4: Saving Remapped Interaction File ---')
        remapped_inter_path = self.base_processed_dir / f'{self.dataset_name}.inter'
        custom_header = ['user_id:token', 'item_id:token', 'rating:float', 'timestamp:float']
        df.to_csv(remapped_inter_path, sep='\t', index=False, header=custom_header)
        print(f'Remapped interaction data saved to {remapped_inter_path}')

        # --- LOO 样本 ---
        print('\n--- Step 5: Generating and Truncating User Interaction Sequences ---')
        df = df.sort_values(by=['user_id', 'timestamp'], ascending=True)
        user_sequences_mapped = df.groupby('user_id')['item_id'].apply(list)
        user_sequences_mapped = user_sequences_mapped.apply(lambda seq: seq[-self.max_len:])
        print(f'Generated and truncated sequences for {len(user_sequences_mapped)} users (max length = {self.max_len}).')

        print('\n--- Step 6: Applying Padding and Generating Samples (Leave-One-Out) ---')
        (train_data, valid_data, test_data) = ([], [], [])
        PAD_TOKEN = 0
        for (user_id, sequence) in tqdm(user_sequences_mapped.items(), desc='Processing sequences'):
            if len(sequence) < 3:
                continue
            # test
            test_hist = sequence[-self.window_size - 1:-1]
            test_padded_hist = [PAD_TOKEN] * (self.window_size - len(test_hist)) + test_hist
            test_data.append({
                'user_id': user_id,
                'sequence': ','.join(map(str, test_padded_hist)),
                'target_id': sequence[-1]
            })
            # valid
            valid_hist = sequence[-self.window_size - 2:-2]
            valid_padded_hist = [PAD_TOKEN] * (self.window_size - len(valid_hist)) + valid_hist
            valid_data.append({
                'user_id': user_id,
                'sequence': ','.join(map(str, valid_padded_hist)),
                'target_id': sequence[-2]
            })
            # train
            train_seq = sequence[:-2]
            if len(train_seq) < 2:
                continue
            for i in range(1, len(train_seq)):
                target_id = train_seq[i]
                history = train_seq[max(0, i - self.window_size):i]
                padded_history = [PAD_TOKEN] * (self.window_size - len(history)) + history
                train_data.append({
                    'user_id': user_id,
                    'sequence': ','.join(map(str, padded_history)),
                    'target_id': target_id
                })

        print(f'Generated {len(train_data)} training samples.')
        print(f'Generated {len(valid_data)} validation samples.')
        print(f'Generated {len(test_data)} testing samples.')

        # --- 保存 split & 新产物 ---
        print('\n--- Step 7: Saving Processed Data and Artifacts ---')
        train_path = self.split_data_dir / 'train.csv'
        valid_path = self.split_data_dir / 'valid.csv'
        test_path = self.split_data_dir / 'test.csv'
        df_train = pd.DataFrame(train_data)
        df_valid = pd.DataFrame(valid_data)
        df_test = pd.DataFrame(test_data)
        df_train.to_csv(train_path, index=False); print(f'Train data saved to {train_path}')
        df_valid.to_csv(valid_path, index=False); print(f'Validation data saved to {valid_path}')
        df_test.to_csv(test_path, index=False); print(f'Test data saved to {test_path}')

        # === 取代 user_map.csv：输出重映射后的 .user 文件 ===
        processed_user_path = self.base_processed_dir / f'{self.dataset_name}.user'
        df_user_out = pd.DataFrame({
            'user_id:token': [user_map[k] for k in sorted(user_map.keys(), key=lambda x: int(x) if str(x).isdigit() else str(x))],
            'original_id:token': sorted(user_map.keys(), key=lambda x: int(x) if str(x).isdigit() else str(x))
        })
        df_user_out.to_csv(processed_user_path, sep='\t', index=False)
        print(f'User file (remapped) saved to {processed_user_path}')

        # === 继续生成 link（取代 item_map.csv 的功能） ===
        try:
            df_link_raw = pd.read_csv(
                self.link_file_path, sep='\t', header=0,
                names=['original_id', 'entity_id'],
                dtype={'original_id': str, 'entity_id': str}
            )
            print('Loaded raw link file.')
        except FileNotFoundError:
            print(f'Warning: Entity link file not found at {self.link_file_path}.')
            df_link_raw = pd.DataFrame(columns=['original_id', 'entity_id'])

        # merge 映射，构造 entity_id，占位补齐
        df_item_map = pd.DataFrame(item_map.items(), columns=['original_id', 'mapped_id'])
        df_item_map = pd.merge(df_item_map, df_link_raw, on='original_id', how='left')

        print('\n--- Step 7a: Generating Placeholder Entity IDs for Missing Items ---')
        missing_entity_mask = df_item_map['entity_id'].isnull()
        num_missing = int(missing_entity_mask.sum())
        if num_missing > 0:
            mapped_ids_to_impute = df_item_map.loc[missing_entity_mask, 'mapped_id']
            placeholders = mapped_ids_to_impute.apply(lambda x: f'n.00{x}')
            df_item_map.loc[missing_entity_mask, 'entity_id'] = placeholders
            print(f'Found and filled {num_missing} missing entity IDs with placeholders.')
        else:
            print('No missing entity IDs found.')

        link_export_path = self.base_processed_dir / f'{self.dataset_name}.link'
        link_export_df = df_item_map[['mapped_id', 'entity_id']].copy()
        link_export_df.dropna(subset=['entity_id'], inplace=True)
        link_export_df.rename(columns={'mapped_id': 'item_id:token', 'entity_id': 'entity_id:token'}, inplace=True)
        link_export_df = link_export_df.sort_values('item_id:token')
        link_export_df.to_csv(link_export_path, sep='\t', index=False)
        print(f'Generated new link file at: {link_export_path}')

        # === KG ===
        print('\n--- Step 7b: Loading Knowledge Graph and filtering by head entities from link ---')
        kg_dest_path = self.base_processed_dir / f'{self.dataset_name}.kg'
        kg_src_path = self.pf.raw_kg_path()
        default_kg_columns = ['head_id:token', 'relation:token', 'tail_id:token']

        try:
            df_kg_raw = pd.read_csv(kg_src_path, sep='\t', header=0)
            kg_columns = df_kg_raw.columns.tolist()
            print(f'Loaded raw KG with {len(df_kg_raw)} triples. Columns: {kg_columns}')
        except FileNotFoundError as e:
            print(f'Warning: Raw KG file not found: {e}. Will fallback to fake triples only.')
            kg_columns = default_kg_columns
            df_kg_raw = pd.DataFrame(columns=kg_columns)
        except Exception as e:
            print(f'An error occurred when reading raw KG: {e}. Will fallback to fake triples only.')
            kg_columns = default_kg_columns
            df_kg_raw = pd.DataFrame(columns=kg_columns)

        head_col = (df_kg_raw.columns[0] if not df_kg_raw.empty else default_kg_columns[0])
        allowed_heads = set(link_export_df['entity_id:token'].astype(str).unique())

        if not df_kg_raw.empty:
            df_kg_raw[head_col] = df_kg_raw[head_col].astype(str)
            before_cnt = len(df_kg_raw)
            df_kg_base = df_kg_raw[df_kg_raw[head_col].isin(allowed_heads)].copy()
            after_cnt = len(df_kg_base)
            print(f'Filtered KG by head entities: {before_cnt} -> {after_cnt} triples kept.')
        else:
            df_kg_base = pd.DataFrame(columns=df_kg_raw.columns if not df_kg_raw.empty else default_kg_columns)

        print('\n--- Step 7c: Updating Knowledge Graph with Fake Triples ---')
        all_triples = []
        if not df_kg_base.empty:
            all_triples.append(df_kg_base)

        # 为占位实体加 fake 边
        items_with_placeholders = link_export_df[link_export_df['entity_id:token'].astype(str).str.startswith('n.')]
        fake_triples = []
        if not items_with_placeholders.empty:
            kg_columns_use = (df_kg_base.columns.tolist() if not df_kg_base.empty else default_kg_columns)
            for _, row in items_with_placeholders.iterrows():
                head_entity = str(row['entity_id:token'])
                tail_entity = head_entity.replace('n.', 'f.', 1)
                fake_triples.append({
                    kg_columns_use[0]: head_entity,
                    kg_columns_use[1]: 'fake',
                    kg_columns_use[2]: tail_entity
                })
        if fake_triples:
            df_fake_triples = pd.DataFrame(fake_triples)
            all_triples.append(df_fake_triples)
            print(f'Generated {len(df_fake_triples)} fake triples to be added.')

        if all_triples:
            df_kg_updated = pd.concat(all_triples, ignore_index=True)
            # 规范列名与排序
            cols = df_kg_updated.columns.tolist()
            if len(cols) >= 3:
                rename_map = {}
                expect = default_kg_columns
                for i in range(3):
                    if cols[i] != expect[i]:
                        rename_map[cols[i]] = expect[i]
                if rename_map:
                    df_kg_updated.rename(columns=rename_map, inplace=True)
            sort_cols = default_kg_columns[:2]
            for c in sort_cols:
                df_kg_updated[c] = df_kg_updated[c].astype(str)
            df_kg_updated = df_kg_updated.sort_values(by=sort_cols, ascending=True, kind='mergesort')
            df_kg_updated.to_csv(kg_dest_path, sep='\t', index=False, header=True)
            print(f'Final knowledge graph with {len(df_kg_updated)} triples saved to: {kg_dest_path}')
        else:
            print('No KG triples to save. KG file not created.')

        # --- Remap .item (可选，沿用原逻辑) ---
        print('\n--- Step 7d: Remapping and Filtering Item Metadata (.item) ---')
        remapped_item_path = self.base_processed_dir / f'{self.dataset_name}.item'
        try:
            df_item_raw = pd.read_csv(self.item_file_path, sep='\t', header=0, engine='python')
            df_item_raw.columns = ['item_id:token', 'movie_title:token_seq', 'release_year:token', 'genre:token_seq']
            df_item_raw['item_id:token'] = df_item_raw['item_id:token'].astype(str)
            df_item_remapped = df_item_raw[df_item_raw['item_id:token'].isin(item_map.keys())].copy()
            df_item_remapped['item_id:token'] = df_item_remapped['item_id:token'].map(item_map)
            df_item_remapped = df_item_remapped.sort_values(by='item_id:token')
            df_item_remapped.to_csv(remapped_item_path, sep='\t', index=False)
            print(f'Remapped and filtered item metadata saved to {remapped_item_path}')
        except FileNotFoundError:
            print(f'Warning: Item metadata file not found at {self.item_file_path}.')

        print('\nPreprocessing finished successfully! 🎉')

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Data Preprocessing for Sequential Recommendation')
    parser.add_argument('--dataset_name', type=str, default='ml-1m', help='Name of the dataset')
    parser.add_argument('--window_size', type=int, default=5, help='The size of the sliding window (L)')
    parser.add_argument('--min_core', type=int, default=5, help='The K-core filtering threshold')
    parser.add_argument('--max_len', type=int, default=200, help='The maximum length of user history')
    args = parser.parse_args()

    if args.max_len < args.window_size:
        print('Warning: max_len is less than window_size. Setting max_len to window_size for truncation.')
        args.max_len = args.window_size

    preprocessor = DataPreprocessor(args)
    preprocessor.run()

============================================================
文件位置: src/models/__init__.py
============================================================
# datasets package

============================================================
文件位置: src/models/bert4rec.py
============================================================
# 文件路径：src/models/bert4rec.py
import torch
import torch.nn as nn
from src.models.components.transformer import TransformerBlock
from src.models.components.mask_utils import build_key_padding_mask
from src.models.components.init_utils import (
    init_linear_xavier_uniform,
    init_embedding_xavier_uniform,
    init_layernorm_default,
)

class BERT4Rec(nn.Module):
    """
    适配当前项目训练协议（next-item 预测）的 BERT4Rec 变体：
    - 使用双向 Transformer（无因果掩码），只对 padding 做 key padding mask；
    - 用户表示采用最后一个非 PAD 位置的 token 表示（last pooling）；
    - 输出对所有 items 的打分 logits（与 Trainer 的 CE loss 接口一致）。
    """
    def __init__(
        self,
        num_items: int,
        embedding_dim: int,
        max_len: int,
        num_blocks: int,
        num_attention_heads: int,
        dropout_prob: float,
        pooling: str = "last",   # 可选: last / mean
        layer_norm_eps: float = 1e-12,
    ):
        super().__init__()
        assert embedding_dim % num_attention_heads == 0, \
            "embedding_dim 必须能被 num_attention_heads 整除"

        self.num_items = int(num_items)
        self.hidden_size = int(embedding_dim)
        self.inner_size = self.hidden_size * 4
        self.max_len = int(max_len)
        self.dropout_prob = float(dropout_prob)
        self.layer_norm_eps = float(layer_norm_eps)
        self.num_blocks = int(num_blocks)
        self.num_heads = int(num_attention_heads)
        self.pooling = str(pooling).lower()

        # 与现有模型保持一致：物品 ID 从 1..num_items；索引时需减 1；0 保留为 PAD
        self.item_embedding = nn.Embedding(self.num_items, self.hidden_size)
        self.position_embedding = nn.Embedding(self.max_len, self.hidden_size)

        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(
                hidden_size=self.hidden_size,
                n_heads=self.num_heads,
                inner_size=self.inner_size,
                hidden_dropout_prob=self.dropout_prob,
                attn_dropout_prob=self.dropout_prob,
                hidden_act='gelu',            # BERT 常用 GELU
                layer_norm_eps=self.layer_norm_eps,
            ) for _ in range(self.num_blocks)
        ])

        self.layer_norm = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)
        self.dropout = nn.Dropout(self.dropout_prob)

        # 初始化策略与本仓库其他模型保持一致
        self.apply(init_linear_xavier_uniform)
        self.apply(init_embedding_xavier_uniform)
        self.apply(init_layernorm_default)

    @staticmethod
    def _key_padding_mask(sequences: torch.Tensor) -> torch.Tensor:
        # True 表示需要 mask 的位置（即 PAD=0）
        return build_key_padding_mask(sequences)

    @staticmethod
    def _last_hidden(x: torch.Tensor, sequences: torch.Tensor) -> torch.Tensor:
        # 取每条序列最后一个非 PAD 的位置表示
        seq_len = (sequences != 0).sum(dim=1).clamp(min=1)
        b_idx = torch.arange(sequences.size(0), device=sequences.device)
        return x[b_idx, seq_len - 1, :]

    @staticmethod
    def _mean_hidden(x: torch.Tensor, sequences: torch.Tensor) -> torch.Tensor:
        # 对非 PAD 位置取平均
        mask = (sequences != 0).float().unsqueeze(-1)  # (B, L, 1)
        summed = (x * mask).sum(dim=1)                # (B, D)
        denom = mask.sum(dim=1).clamp(min=1.0)        # (B, 1)
        return summed / denom

    def encode(self, sequences: torch.Tensor, override_item_emb: torch.Tensor = None) -> torch.Tensor:
        """
        双向编码，无因果 mask，仅做 padding mask。
        """
        # (B, L) -> (B, L, D)
        nonpad = (sequences > 0).unsqueeze(-1).float()
        seq_idx = torch.clamp(sequences - 1, min=0)

        if override_item_emb is None:
            item_emb = self.item_embedding(seq_idx)   # (B, L, D)
        else:
            # 兼容外部传入的预编码 item embedding（与 SASRec 接口一致）
            item_emb = override_item_emb

        item_emb = item_emb * nonpad

        # 位置嵌入
        pos_ids = torch.arange(sequences.size(1), device=sequences.device).unsqueeze(0).expand_as(sequences)
        pos_emb = self.position_embedding(pos_ids)

        x = item_emb + pos_emb
        x = self.layer_norm(x)
        x = self.dropout(x)

        # 仅使用 key padding mask（无因果掩码 -> 双向）
        key_padding_mask = self._key_padding_mask(sequences)

        for block in self.transformer_blocks:
            x = block(x, attn_mask=None, key_padding_mask=key_padding_mask)
            # 对 PAD 位置清零，数值稳定
            x[sequences == 0] = 0.0

        # 池化为用户表示
        if self.pooling == "mean":
            final_user_repr = self._mean_hidden(x, sequences)
        else:
            final_user_repr = self._last_hidden(x, sequences)

        return final_user_repr

    def forward(self, sequences: torch.Tensor, override_item_emb: torch.Tensor = None) -> torch.Tensor:
        """
        输出对所有 items 的打分矩阵 (B, num_items)
        —— 与 Trainer 中的 CrossEntropyLoss(next_item-1) 完全对齐。
        """
        user_repr = self.encode(sequences, override_item_emb=override_item_emb)  # (B, D)
        logits = torch.matmul(user_repr, self.item_embedding.weight.transpose(0, 1))  # (B, N)
        return logits

============================================================
文件位置: src/models/components/__init__.py
============================================================
# datasets package

============================================================
文件位置: src/models/components/augment.py
============================================================
import torch

__all__ = [
    "safe_random_mask",
    "random_crop",
    "local_reorder",
    "random_substitute",
    "compose_augmentation",
]

def _nonzero_pos(row: torch.Tensor):
    return row.nonzero(as_tuple=False).flatten()

@torch.no_grad()
def safe_random_mask(seq: torch.Tensor, mask_ratio: float) -> torch.Tensor:
    """
    保证每个样本至少保留 1 个非零 token 的随机mask（替代原随机伯努利）。
    """
    if mask_ratio <= 0.0:
        return seq
    out = seq.clone()
    (B, L) = out.size()
    dev = out.device
    for i in range(B):
        pos = _nonzero_pos(out[i])
        n = int(pos.numel())
        if n <= 1:
            continue  # 不动，避免全零
        m = int(round(mask_ratio * n))
        m = max(0, min(m, n - 1))  # 至多mask n-1个
        if m == 0:
            continue
        idx = torch.randperm(n, device=dev)[:m]
        out[i, pos[idx]] = 0
    return out

@torch.no_grad()
def random_crop(seq: torch.Tensor, min_keep_ratio: float) -> torch.Tensor:
    if min_keep_ratio >= 1.0:
        return seq
    out = seq.clone()
    (B, L) = out.size()
    dev = out.device
    valid_len = (out != 0).sum(dim=1)
    r = torch.empty(B, device=dev).uniform_(min_keep_ratio, 1.0)
    keep = torch.clamp((valid_len.float() * r).round().long(), min=1)
    for i in range(B):
        k = int(keep[i].item())
        pos = _nonzero_pos(out[i])
        if pos.numel() == 0:
            continue
        pos = pos[-k:]  # 保留尾部k个
        new_row = torch.zeros(L, dtype=out.dtype, device=dev)
        new_row[-k:] = out[i, pos]
        out[i] = new_row
    return out

@torch.no_grad()
def local_reorder(seq: torch.Tensor, reorder_prob: float, max_window: int = 4) -> torch.Tensor:
    if reorder_prob <= 0.0 or max_window <= 1:
        return seq
    out = seq.clone()
    (B, L) = out.size()
    dev = out.device
    for i in range(B):
        if torch.rand(1, device=dev).item() > reorder_prob:
            continue
        pos = _nonzero_pos(out[i])
        if pos.numel() <= 2:
            continue
        w = int(torch.randint(2, min(max_window, pos.numel()) + 1, (1,), device=dev).item())
        start = int(torch.randint(0, pos.numel() - w + 1, (1,), device=dev).item())
        seg = pos[start:start + w]
        vals = out[i, seg].clone()
        out[i, seg] = vals[torch.randperm(w, device=dev)]
    return out

@torch.no_grad()
def random_substitute(seq: torch.Tensor, substitute_prob: float, num_items: int) -> torch.Tensor:
    if substitute_prob <= 0.0 or num_items <= 1:
        return seq
    out = seq.clone()
    (B, L) = out.size()
    dev = out.device
    prob = torch.full((B, L), substitute_prob, device=dev)
    take = (torch.bernoulli(prob).bool()) & out.ne(0)
    if take.any():
        out[take] = torch.randint(1, num_items + 1, (take.sum().item(),), device=dev, dtype=out.dtype)
    return out

@torch.no_grad()
def compose_augmentation(
    seq: torch.Tensor,
    *,
    num_items: int,
    mask_ratio: float = 0.0,
    crop_min_ratio: float = 1.0,
    reorder_prob: float = 0.0,
    substitute_prob: float = 0.0,
) -> torch.Tensor:
    """
    先裁剪再扰动，最后用“安全mask”，并做最终兜底：若某行被意外置零，则把“原序列最后一个非零token”放回末尾。
    """
    orig = seq
    out = seq
    # 顺序：crop -> reorder -> substitute -> safe_mask
    out = random_crop(out, crop_min_ratio)
    out = local_reorder(out, reorder_prob, max_window=4)
    out = random_substitute(out, substitute_prob, num_items)
    out = safe_random_mask(out, mask_ratio)

    # 兜底：防全零（极端情况下）
    (B, L) = out.size()
    dev = out.device
    for i in range(B):
        if out[i].nonzero(as_tuple=False).numel() == 0:
            pos = _nonzero_pos(orig[i])
            if pos.numel() > 0:
                last_val = int(orig[i, pos[-1]].item())
                new_row = torch.zeros(L, dtype=out.dtype, device=dev)
                new_row[-1] = last_val
                out[i] = new_row
    return out

============================================================
文件位置: src/models/components/init_utils.py
============================================================
# src/models/components/init_utils.py
# 统一的权重初始化助手：便于跨模型保持一致且可配置

import torch.nn as nn
from torch.nn.init import xavier_uniform_, xavier_normal_, kaiming_uniform_, kaiming_normal_, constant_

__all__ = [
    "init_linear_xavier_uniform",
    "init_embedding_xavier_uniform",
    "init_layernorm_default",
    "init_gru_xavier_uniform",
]

def init_linear_xavier_uniform(m: nn.Module):
    if isinstance(m, nn.Linear):
        xavier_uniform_(m.weight)
        if m.bias is not None:
            constant_(m.bias, 0)

def init_embedding_xavier_uniform(m: nn.Module):
    if isinstance(m, nn.Embedding):
        xavier_uniform_(m.weight)

def init_layernorm_default(m: nn.Module):
    if isinstance(m, nn.LayerNorm):
        constant_(m.bias, 0)
        constant_(m.weight, 1.0)

def init_gru_xavier_uniform(m: nn.Module):
    if isinstance(m, nn.GRU):
        for name, param in m.named_parameters():
            if "weight_ih" in name or "weight_hh" in name:
                xavier_uniform_(param.data)

============================================================
文件位置: src/models/components/mask_utils.py
============================================================
# src/models/components/mask_utils.py
# 统一的注意力掩码工具：可复用在所有 Transformer 系模型中

import torch

__all__ = [
    "build_causal_mask",
    "build_key_padding_mask",
]

def build_causal_mask(seq_len: int, device: torch.device) -> torch.Tensor:
    """
    严格上三角因果掩码，禁止看未来。
    使用 -1e9 代替 -inf，提升数值稳定性。
    形状: [seq_len, seq_len]，用于 MultiheadAttention(attn_mask=...)
    """
    mask = torch.full((seq_len, seq_len), -1e9, device=device)
    return torch.triu(mask, diagonal=1)

def build_key_padding_mask(sequences: torch.Tensor) -> torch.Tensor:
    """
    Key padding mask：True 表示被 mask（即 padding=0 的位置）。
    形状: [batch, seq_len]
    """
    return sequences.eq(0)

============================================================
文件位置: src/models/components/transformer.py
============================================================
# src/models/components/transformer.py
# 通用 TransformerBlock 与掩码构造工具，供 SASRec / BERT4Rec 复用

import torch
import torch.nn as nn
from src.models.components.init_utils import (
    init_linear_xavier_uniform,
    init_embedding_xavier_uniform,
    init_layernorm_default,
)

_ACT_MAP = {
    "relu": nn.ReLU,
    "gelu": nn.GELU,
}

def get_activation(name: str):
    name = (name or "relu").lower()
    return _ACT_MAP.get(name, nn.ReLU)()

def build_causal_mask(seq_len: int, device: torch.device) -> torch.Tensor:
    mask = torch.full((seq_len, seq_len), -1e9, device=device)
    return torch.triu(mask, diagonal=1)

class TransformerBlock(nn.Module):
    """
    通用 Transformer Block（自注意力 + FFN + 两处残差 LN）
    """
    def __init__(
        self,
        hidden_size: int,
        n_heads: int,
        inner_size: int,
        hidden_dropout_prob: float,
        attn_dropout_prob: float,
        hidden_act: str = "relu",
        layer_norm_eps: float = 1e-12,
    ):
        super().__init__()
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=n_heads,
            dropout=attn_dropout_prob,
            batch_first=True,
        )
        self.ln1 = nn.LayerNorm(hidden_size, eps=layer_norm_eps)
        self.do1 = nn.Dropout(hidden_dropout_prob)

        self.ffn = nn.Sequential(
            nn.Linear(hidden_size, inner_size),
            get_activation(hidden_act),
            nn.Linear(inner_size, hidden_size),
        )
        self.ln2 = nn.LayerNorm(hidden_size, eps=layer_norm_eps)
        self.do2 = nn.Dropout(hidden_dropout_prob)

        # 统一初始化
        self.apply(init_linear_xavier_uniform)
        self.apply(init_embedding_xavier_uniform)
        self.apply(init_layernorm_default)

    def forward(self, x, attn_mask=None, key_padding_mask=None):
        attn_out, _ = self.attention(
            x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask
        )
        x = self.ln1(x + self.do1(attn_out))
        ff = self.ffn(x)
        x = self.ln2(x + self.do2(ff))
        return x

============================================================
文件位置: src/models/gru4rec.py
============================================================
import torch
import torch.nn as nn
from torch.nn.init import xavier_uniform_, xavier_normal_

class GRU4Rec(nn.Module):
    def __init__(self, num_items, embedding_dim, hidden_size, num_layers=1, dropout_prob=0.5):
        super().__init__()
        self.num_items = int(num_items)
        self.embedding_dim = int(embedding_dim)
        self.hidden_size = int(hidden_size)
        self.num_layers = int(num_layers)
        self.dropout_prob = float(dropout_prob)

        # 不含 padding 行
        self.item_embedding = nn.Embedding(self.num_items, self.embedding_dim)

        self.emb_dropout = nn.Dropout(self.dropout_prob)
        self.gru_layers = nn.GRU(
            input_size=self.embedding_dim,
            hidden_size=self.hidden_size,
            num_layers=self.num_layers,
            bias=False,
            batch_first=True,
        )
        self.dense = nn.Linear(self.hidden_size, self.embedding_dim)
        self.out_dropout = nn.Dropout(0.0)

        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Embedding):
            xavier_normal_(module.weight)
        elif isinstance(module, nn.GRU):
            for name, param in module.named_parameters():
                if "weight_ih" in name or "weight_hh" in name:
                    xavier_uniform_(param.data)
        elif isinstance(module, nn.Linear):
            xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)

    @staticmethod
    def _last_hidden_from_outputs(outputs: torch.Tensor, seq_len: torch.Tensor):
        B = outputs.size(0)
        idx = torch.arange(B, device=outputs.device)
        last = outputs[idx, torch.clamp(seq_len - 1, min=0)]
        return last

    def forward(self, sequences: torch.Tensor):
        # 1..N -> 0..N-1，padding=0 做 mask
        nonpad = (sequences > 0).unsqueeze(-1).float()
        seq_idx = torch.clamp(sequences - 1, min=0)

        emb = self.item_embedding(seq_idx)
        emb = emb * nonpad
        emb = self.emb_dropout(emb)

        gru_out, _ = self.gru_layers(emb)
        proj_out = self.dense(gru_out)

        seq_len = (sequences != 0).sum(dim=1)
        seq_output = self._last_hidden_from_outputs(proj_out, seq_len)
        seq_output = self.out_dropout(seq_output)

        logits = torch.matmul(seq_output, self.item_embedding.weight.transpose(0, 1))
        return logits

============================================================
文件位置: src/models/katrec.py
============================================================
import math
import torch
import torch.nn as nn
from torch.nn.init import xavier_uniform_, xavier_normal_
from src.models.components.transformer import TransformerBlock
from src.models.components.mask_utils import build_causal_mask, build_key_padding_mask
from src.models.components.init_utils import init_linear_xavier_uniform, init_embedding_xavier_uniform, init_layernorm_default

class KATRec(nn.Module):

    def __init__(self, num_items: int, config):
        super().__init__()
        self.num_items = int(num_items)
        self.embedding_dim = int(getattr(config, 'embedding_dim', 64))
        self.hidden_size = self.embedding_dim
        self.num_blocks = int(getattr(config, 'num_blocks', 2))
        self.num_heads = int(getattr(config, 'num_attention_heads', 2))
        assert self.embedding_dim % self.num_heads == 0, 'embedding_dim 必须能被 num_attention_heads 整除'
        self.head_dim = self.embedding_dim // self.num_heads
        self.dropout_prob = float(getattr(config, 'dropout_prob', 0.2))
        self.max_len = int(getattr(config, 'window_size', 50))

        # KG 相关
        self.kg_dim = int(getattr(config, 'kg_embedding_dim', getattr(config, 'kg_embedding_size', 64)))
        self.n_gcn_layers = int(getattr(config, 'n_gcn_layers', 2))
        self.mess_dropout_prob = float(getattr(config, 'mess_dropout_prob', 0.1))
        self.kg_margin = float(getattr(config, 'kg_margin', 1.0))
        self.kg_attn_alpha = float(getattr(config, 'kg_attn_alpha', 0.2))

        self.num_users = int(config.num_users)
        self.num_entities = int(config.num_entities)
        self.num_relations = int(config.num_relations)

        # 稀疏邻接（来自 dataloader 构建）
        self._raw_adj = config.adj_matrix.coalesce()
        self.register_buffer('adj_indices', self._raw_adj.indices())
        self.register_buffer('adj_values', self._raw_adj.values())
        self.adj_shape = self._raw_adj.shape

        # item -> entity 对齐
        item_to_ent = torch.full((self.num_items,), -1, dtype=torch.long)
        for (it_1, ent) in getattr(config, 'item_entity_map', {}).items():
            (it_i, ent_i) = (int(it_1) - 1, int(ent))
            if 0 <= it_i < self.num_items and 0 <= ent_i < self.num_entities:
                item_to_ent[it_i] = ent_i
        self.register_buffer('item_to_entity', item_to_ent)

        # 序列编码
        self.item_embedding = nn.Embedding(self.num_items, self.embedding_dim)
        self.position_embedding = nn.Embedding(self.max_len, self.embedding_dim)
        self.dropout = nn.Dropout(self.dropout_prob)
        self.layer_norm = nn.LayerNorm(self.embedding_dim, eps=1e-12)
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(
                hidden_size=self.embedding_dim, n_heads=self.num_heads,
                inner_size=self.embedding_dim * 4, hidden_dropout_prob=self.dropout_prob,
                attn_dropout_prob=self.dropout_prob, hidden_act='relu', layer_norm_eps=1e-12
            ) for _ in range(self.num_blocks)
        ])

        # KG 编码
        self.user_embedding_kg = nn.Embedding(self.num_users, self.kg_dim)
        self.entity_embedding_kg = nn.Embedding(self.num_entities, self.kg_dim)
        self.relation_embedding = nn.Embedding(self.num_relations, self.kg_dim, padding_idx=0)
        self.msg_dropout = nn.Dropout(self.mess_dropout_prob)

        # 融合 & 注意力偏置构建
        self.kg2e_token = nn.Linear(self.kg_dim, self.embedding_dim)
        self.kg_q_proj = nn.Linear(self.embedding_dim, self.embedding_dim, bias=False)
        self.kg_k_proj = nn.Linear(self.embedding_dim, self.embedding_dim, bias=False)

        self.kg2e_item = nn.Linear(self.kg_dim, self.embedding_dim)
        self.item_fuse = nn.Linear(self.embedding_dim + self.embedding_dim, self.embedding_dim)

        self.apply(self._init_weights)
        self.apply(init_linear_xavier_uniform)
        self.apply(init_embedding_xavier_uniform)
        self.apply(init_layernorm_default)

        # 缓存（仅用于推理/评测阶段加速，不持久化）
        self.register_buffer('z_entity_cached', None, persistent=False)

    def _init_weights(self, module):
        if isinstance(module, nn.Embedding):
            xavier_normal_(module.weight.data)
        elif isinstance(module, nn.Linear):
            xavier_uniform_(module.weight.data)
            if module.bias is not None:
                nn.init.zeros_(module.bias)

    def _sparse_adj_on_device(self, device):
        return torch.sparse.FloatTensor(
            self.adj_indices.to(device), self.adj_values.to(device), torch.Size(self.adj_shape)
        ).coalesce()

    def _graph_propagate(self, device):
        adj = self._sparse_adj_on_device(device)
        x0_user = self.user_embedding_kg.weight
        x0_ent = self.entity_embedding_kg.weight
        x = torch.cat([x0_user, x0_ent], dim=0)

        outs = [x]
        for _ in range(self.n_gcn_layers):
            x = torch.sparse.mm(adj, x)
            x = self.msg_dropout(x)
            outs.append(x)

        x_final = torch.stack(outs, dim=0).mean(0)
        z_user = x_final[:self.num_users]
        z_entity = x_final[self.num_users:]
        return (z_user, z_entity)

    # ---------- 新增：KG 表征预计算与缓存（供评测/推理使用） ----------
    def precompute_kg(self, device):
        with torch.no_grad():
            _, z_entity = self._graph_propagate(device)
        # 缓存在 buffer 中（不持久化到 state_dict）
        self.z_entity_cached = z_entity

    def _build_kga_bias(self, sequences, z_entity):
        device = sequences.device
        (B, L) = sequences.size()

        nonpad = (sequences > 0).unsqueeze(-1)
        idx0 = torch.clamp(sequences - 1, min=0)
        ent_ids = self.item_to_entity[idx0]
        valid_mask = (ent_ids >= 0) & nonpad.squeeze(-1)

        ent_ids_clamped = torch.clamp(ent_ids, min=0)
        ent_k = z_entity[ent_ids_clamped]
        ent_k = ent_k * valid_mask.unsqueeze(-1).float()

        ent_e = self.kg2e_token(ent_k)

        q = self.kg_q_proj(ent_e).view(B, L, self.num_heads, self.head_dim)
        k = self.kg_k_proj(ent_e).view(B, L, self.num_heads, self.head_dim)
        q = q.permute(0, 2, 1, 3)
        k = k.permute(0, 2, 1, 3)

        scale = 1.0 / math.sqrt(self.head_dim)
        bias = torch.matmul(q, k.transpose(-2, -1)) * scale

        key_pad = ~valid_mask
        key_pad_exp = key_pad.unsqueeze(1).unsqueeze(1)
        qry_pad_exp = key_pad.unsqueeze(1).unsqueeze(2)
        neg_inf = torch.tensor(-1e9, device=device, dtype=bias.dtype)
        bias = bias.masked_fill(key_pad_exp, neg_inf)
        bias = bias.masked_fill(qry_pad_exp, neg_inf)

        bias = bias.reshape(B * self.num_heads, L, L)
        return bias

    def forward(self, sequences: torch.Tensor):
        device = sequences.device

        # 使用缓存（若不存在则现算）——训练时通常不缓存，以便保持梯度/动态更新
        z_entity = self.z_entity_cached
        if z_entity is None:
            _, z_entity = self._graph_propagate(device)

        (B, L) = sequences.size()
        nonpad = (sequences > 0).unsqueeze(-1).float()
        seq_idx = torch.clamp(sequences - 1, min=0)

        item_emb = self.item_embedding(seq_idx) * nonpad
        pos_ids = torch.arange(L, device=device).unsqueeze(0).expand(B, L)
        pos_emb = self.position_embedding(pos_ids)

        x = self.layer_norm(item_emb + pos_emb)
        x = self.dropout(x)

        causal_mask = build_causal_mask(L, device)
        key_padding_mask = build_key_padding_mask(sequences)

        kg_bias = self._build_kga_bias(sequences, z_entity)
        causal_mask_expanded = causal_mask.unsqueeze(0).expand(B * self.num_heads, -1, -1)
        attn_mask = causal_mask_expanded + self.kg_attn_alpha * kg_bias

        for block in self.transformer_blocks:
            x = block(x, attn_mask=attn_mask, key_padding_mask=key_padding_mask)
            x[sequences == 0] = 0.0

        seq_len = (sequences != 0).sum(dim=1).clamp(min=1)
        b_idx = torch.arange(B, device=device)
        user_vec = x[b_idx, seq_len - 1, :]

        all_item_e = self.item_embedding.weight
        all_item_ent = self.item_to_entity
        all_ent_ids = torch.clamp(all_item_ent, min=0)
        all_valid = (all_item_ent >= 0).float().unsqueeze(-1)

        all_ent_k = z_entity[all_ent_ids] * all_valid
        all_ent_e = self.kg2e_item(all_ent_k)

        all_item_vec = self.item_fuse(torch.cat([all_item_e, all_ent_e], dim=-1))
        logits = torch.matmul(user_vec, all_item_vec.t())
        return logits

    @staticmethod
    def _transE_score(h, r, t):
        return -torch.norm(h + r - t, p=2, dim=-1)

    def calculate_kg_loss(self, h_idx, r_idx, pos_t_idx, neg_t_idx):
        device = h_idx.device
        h = self.entity_embedding_kg(h_idx).to(device)
        r = self.relation_embedding(r_idx).to(device)
        t_pos = self.entity_embedding_kg(pos_t_idx).to(device)
        t_neg = self.entity_embedding_kg(neg_t_idx).to(device)

        pos_score = self._transE_score(h, r, t_pos)
        neg_score = self._transE_score(h, r, t_neg)

        target = torch.ones_like(pos_score, device=device)
        loss_fn = nn.MarginRankingLoss(margin=self.kg_margin)
        loss = loss_fn(pos_score, neg_score, target)
        return loss

============================================================
文件位置: src/models/ksr.py
============================================================
import torch
import torch.nn as nn
from torch.nn.init import xavier_uniform_, xavier_normal_

class KSR(nn.Module):
    def __init__(self, num_items: int, config):
        super().__init__()
        self.n_items = int(num_items)
        self.embedding_size = int(getattr(config, "embedding_dim", 64))
        self.hidden_size = int(getattr(config, "hidden_size", self.embedding_size))
        self.num_layers = int(getattr(config, "num_layers", 1))
        self.dropout_prob = float(getattr(config, "dropout_prob", 0.2))

        self.kg_embedding_size = int(getattr(config, "kg_embedding_size",
                                             getattr(config, "kg_embedding_dim", 64)))
        self.gamma = float(getattr(config, "gamma", 0.5))
        self.freeze_kg = bool(getattr(config, "freeze_kg", False))

        assert hasattr(config, "num_entities"), "KSR needs config.num_entities"
        assert hasattr(config, "num_relations"), "KSR needs config.num_relations (with padding 0)"
        self.n_entities = int(config.num_entities)

        self.n_relations_total = int(config.num_relations)  # 包含 0（pad）
        self.n_relations = self.n_relations_total - 1
        assert self.n_relations > 0

        # 物品不含 padding 行
        self.item_embedding = nn.Embedding(self.n_items, self.embedding_size)

        # KG embedding
        self.entity_embedding = nn.Embedding(self.n_entities, self.kg_embedding_size)
        if self.freeze_kg:
            self.entity_embedding.weight.requires_grad = False
        self.relation_embedding = nn.Embedding(self.n_relations_total, self.kg_embedding_size, padding_idx=0)

        self.emb_dropout = nn.Dropout(self.dropout_prob)
        self.gru_layers = nn.GRU(
            input_size=self.embedding_size,
            hidden_size=self.hidden_size,
            num_layers=self.num_layers,
            bias=False,
            batch_first=True,
        )

        self.dense_seq_to_k = nn.Linear(self.hidden_size, self.kg_embedding_size)
        self.dense_layer_u = nn.Linear(self.hidden_size + self.kg_embedding_size, self.embedding_size)
        self.dense_layer_i = nn.Linear(self.embedding_size + self.kg_embedding_size, self.embedding_size)

        # item -> entity（零基）
        item_to_entity = torch.full((self.n_items,), -1, dtype=torch.long)
        for it_1, ent in getattr(config, "item_entity_map", {}).items():  # it_1: 1..N
            it_i = int(it_1) - 1
            ent_i = int(ent)
            if 0 <= it_i < self.n_items and 0 <= ent_i < self.n_entities:
                item_to_entity[it_i] = ent_i
        self.register_buffer("item_to_entity", item_to_entity)

        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Embedding):
            xavier_normal_(module.weight)
        elif isinstance(module, nn.GRU):
            try:
                nn.init.xavier_uniform_(module.weight_hh_l0)
                nn.init.xavier_uniform_(module.weight_ih_l0)
            except AttributeError:
                for name, param in module.named_parameters():
                    if "weight" in name:
                        xavier_uniform_(param.data)
        elif isinstance(module, nn.Linear):
            xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)

    def _get_kg_embedding(self, head_items: torch.Tensor):
        # head_items: 原序列 token（0 表 pad，1..N 真实）
        nonpad = (head_items > 0).unsqueeze(-1).float()
        idx0 = torch.clamp(head_items - 1, min=0)
        ent_ids = self.item_to_entity[idx0]                    # 零基 item -> 实体
        valid_mask = (ent_ids >= 0).float().unsqueeze(-1)
        ent_ids_clamped = torch.clamp(ent_ids, min=0)

        head_e = self.entity_embedding(ent_ids_clamped) * valid_mask * nonpad

        # tail_M: 针对所有关系（去掉 0）构造
        rel_mat = self.relation_embedding.weight[1:]
        rel_mat = rel_mat.unsqueeze(0).expand(head_e.size(0), -1, -1)
        head_M = head_e.unsqueeze(1).expand(-1, self.n_relations, -1)
        tail_M = head_M + rel_mat
        return head_e, tail_M

    def _memory_update_cell(self, user_memory: torch.Tensor, update_memory: torch.Tensor):
        z = torch.sigmoid((user_memory * update_memory).sum(-1)).unsqueeze(-1)
        updated_user_memory = (1.0 - z) * user_memory + z * update_memory
        return updated_user_memory

    def memory_update(self, item_seq: torch.Tensor, item_seq_len: torch.Tensor):
        B, L = item_seq.size()
        device = item_seq.device
        last_idx = torch.clamp(item_seq_len - 1, min=0)

        user_memory = torch.zeros(B, self.n_relations, self.kg_embedding_size, device=device)
        last_user_memory = torch.zeros_like(user_memory)

        for i in range(L):
            _, upd_mem = self._get_kg_embedding(item_seq[:, i])
            user_memory = self._memory_update_cell(user_memory, upd_mem)
            mask = (last_idx == i)
            if mask.any():
                last_user_memory[mask] = user_memory[mask]
        return last_user_memory

    def memory_read(self, seq_output_H: torch.Tensor, user_memory: torch.Tensor):
        q_k = self.dense_seq_to_k(seq_output_H)
        attrs = self.relation_embedding.weight[1:]
        logits = self.gamma * torch.matmul(q_k, attrs.t()).float()
        attn = torch.softmax(logits, dim=-1)
        u_m = (user_memory * attn.unsqueeze(-1)).sum(1)
        return u_m

    @staticmethod
    def _last_hidden_from_gru(gru_out: torch.Tensor, seq_len: torch.Tensor):
        B = gru_out.size(0)
        idx = torch.arange(B, device=gru_out.device)
        last = gru_out[idx, torch.clamp(seq_len - 1, min=0)]
        return last

    def _get_item_comb_embedding(self, items: torch.Tensor):
        # items: 零基物品索引（0..N-1）
        ent_ids = self.item_to_entity[items]
        valid = (ent_ids >= 0).float().unsqueeze(-1)
        ent_ids_clamped = torch.clamp(ent_ids, min=0)

        h_e = self.entity_embedding(ent_ids_clamped) * valid
        i_e = self.item_embedding(items)
        q_i = self.dense_layer_i(torch.cat((i_e, h_e), dim=-1))
        return q_i

    def forward(self, sequences: torch.Tensor):
        # 1..N -> 0..N-1；padding=0 用 mask
        nonpad = (sequences > 0).unsqueeze(-1).float()
        seq_idx = torch.clamp(sequences - 1, min=0)

        emb = self.item_embedding(seq_idx) * nonpad
        emb = self.emb_dropout(emb)
        gru_out, _ = self.gru_layers(emb)
        seq_len = (sequences != 0).sum(dim=1)
        seq_H = self._last_hidden_from_gru(gru_out, seq_len)

        user_memory = self.memory_update(sequences, seq_len)
        u_m = self.memory_read(seq_H, user_memory)

        p_u = self.dense_layer_u(torch.cat((seq_H, u_m), dim=-1))

        # 所有物品（零基 0..N-1）
        all_item_e = self.item_embedding.weight
        all_item_ent = self.item_to_entity
        all_ent_ids = torch.clamp(all_item_ent, min=0)
        all_valid = (all_item_ent >= 0).float().unsqueeze(-1)
        all_h_e = self.entity_embedding(all_ent_ids) * all_valid
        all_q_i = self.dense_layer_i(torch.cat((all_item_e, all_h_e), dim=-1))

        logits = torch.matmul(p_u, all_q_i.t())
        return logits

============================================================
文件位置: src/models/sasrec.py
============================================================
import torch
import torch.nn as nn
from src.models.components.transformer import TransformerBlock
from src.models.components.mask_utils import build_causal_mask, build_key_padding_mask
from src.models.components.init_utils import (
    init_linear_xavier_uniform,
    init_embedding_xavier_uniform,
    init_layernorm_default,
)

class SASRec(nn.Module):
    def __init__(self, num_items, embedding_dim, max_len, num_blocks, num_attention_heads, dropout_prob):
        super().__init__()
        self.hidden_size = embedding_dim
        self.inner_size = self.hidden_size * 4
        self.max_len = max_len
        self.dropout_prob = dropout_prob
        self.layer_norm_eps = 1e-12

        # 不含 padding 行
        self.item_embedding = nn.Embedding(num_items, self.hidden_size)
        self.position_embedding = nn.Embedding(self.max_len, self.hidden_size)

        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(
                hidden_size=self.hidden_size,
                n_heads=num_attention_heads,
                inner_size=self.inner_size,
                hidden_dropout_prob=self.dropout_prob,
                attn_dropout_prob=self.dropout_prob,
                hidden_act="relu",
                layer_norm_eps=self.layer_norm_eps,
            ) for _ in range(num_blocks)
        ])

        self.layer_norm = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)
        self.dropout = nn.Dropout(self.dropout_prob)

        self.apply(init_linear_xavier_uniform)
        self.apply(init_embedding_xavier_uniform)
        self.apply(init_layernorm_default)

    @staticmethod
    def _generate_attention_mask(sequences):
        key_padding_mask = build_key_padding_mask(sequences)
        causal_mask = build_causal_mask(sequences.size(1), sequences.device)
        return causal_mask, key_padding_mask

    def encode(self, sequences, override_item_emb=None):
        # 将 1..N → 0..N-1；padding 0 单独 mask
        nonpad = (sequences > 0).unsqueeze(-1).float()
        seq_idx = torch.clamp(sequences - 1, min=0)

        if override_item_emb is None:
            item_emb = self.item_embedding(seq_idx)
        else:
            # override_item_emb 已经在外部按 0 基对齐
            item_emb = override_item_emb

        # 抹掉 padding 位置的 embedding 贡献
        item_emb = item_emb * nonpad

        position_ids = torch.arange(sequences.size(1), dtype=torch.long, device=sequences.device)
        position_ids = position_ids.unsqueeze(0).expand_as(sequences)
        position_emb = self.position_embedding(position_ids)

        x = item_emb + position_emb
        x = self.layer_norm(x)
        x = self.dropout(x)

        causal_mask, key_padding_mask = self._generate_attention_mask(sequences)
        for block in self.transformer_blocks:
            x = block(x, attn_mask=causal_mask, key_padding_mask=key_padding_mask)
            x[sequences == 0] = 0.0

        item_seq_len = (sequences != 0).sum(dim=1)
        b_idx = torch.arange(sequences.size(0), device=sequences.device)
        last_idx = torch.clamp(item_seq_len - 1, min=0)
        final_user_repr = x[b_idx, last_idx, :]
        return final_user_repr

    def forward(self, sequences, override_item_emb=None):
        final_user_repr = self.encode(sequences, override_item_emb=override_item_emb)
        # 物品词表是 0..N-1，与 logits 列对齐
        logits = torch.matmul(final_user_repr, self.item_embedding.weight.transpose(0, 1))
        return logits

============================================================
文件位置: src/trainers/__init__.py
============================================================
# datasets package

============================================================
文件位置: src/trainers/trainer.py
============================================================
import torch
import torch.nn as nn
from tqdm import tqdm
from pathlib import Path
import yaml
import gc
import time
import pandas as pd
import random
from collections import defaultdict
from src.utils.metrics import AllRankMetrics
from src.utils.memory_monitor import setup_memory_efficient_environment, MemoryMonitor
from src.utils.logging_utils import setup_logging
from src.utils.gpu_utils import select_device

class Trainer:

    def __init__(self, model, train_loader, valid_loader, test_loader, config):
        setup_memory_efficient_environment()
        self.memory_monitor = MemoryMonitor(config)
        self.memory_monitor.print_memory_stats('Trainer initialization')

        self.config = config
        self.save_dir = Path(config.save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        self.log_file = self.save_dir / 'train.log'
        self.logger = setup_logging(level=getattr(config, 'log_level', 'INFO'), file_path=self.log_file, name='train')

        self.use_amp = bool(getattr(config, 'use_amp', False) and torch.cuda.is_available())
        self.allow_fast_kernels = bool(getattr(config, 'allow_fast_kernels', False) and torch.cuda.is_available())
        if torch.cuda.is_available():
            try:
                torch.backends.cuda.matmul.allow_tf32 = self.allow_fast_kernels
                torch.backends.cudnn.allow_tf32 = self.allow_fast_kernels
            except Exception:
                pass
            try:
                torch.backends.cuda.sdp_kernel(
                    enable_flash=self.allow_fast_kernels,
                    enable_mem_efficient=self.allow_fast_kernels,
                    enable_math=not self.allow_fast_kernels
                )
            except Exception:
                pass
        self.scaler = torch.cuda.amp.GradScaler(enabled=self.use_amp)

        self.device = select_device(
            prefer=getattr(config, 'device_prefer', 'auto'),
            strategy=getattr(config, 'gpu_select', 'max_free'),
            min_free_mem_gb=float(getattr(config, 'min_free_mem_gb', 0.5)),
            allow_cpu=True,
            explicit_id=getattr(config, 'gpu_id') if hasattr(config, 'gpu_id') else None,
            verbose=True
        )
        self.logger.info(f'Using device: {self.device}')

        self.model = model.to(self.device)
        self.train_loader = train_loader
        self.valid_loader = valid_loader
        self.test_loader = test_loader

        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)
        self.criterion = nn.CrossEntropyLoss()

        top_k = int(getattr(config, 'top_k', 10))
        self.metrics_calculator = AllRankMetrics(top_k=top_k)
        self.grad_clip = float(getattr(config, 'grad_clip', 1.0))
        self.best_metric = -1
        self.epochs_no_improve = 0

        # === 仅用训练集构建评估掩码（防泄露） ===
        self._prepare_eval_protocol_masks()

        if self.config.model_name.lower() == 'katrec':
            self._setup_katrec_training()

    def _safe_config_dict(self):
        blacklist = {'adj_matrix', 'item_relation_map_tensor', 'item_entity_map'}
        cfg = {}
        for k, v in vars(self.config).items():
            if k in blacklist:
                continue
            try:
                yaml.safe_dump({k: v})
                cfg[k] = v
            except Exception:
                cfg[k] = f'<non-serializable:{type(v).__name__}>'
        return cfg

    # ========= 评估协议准备：只看 train.csv =========
    def _prepare_eval_protocol_masks(self):
        """
        构建两类掩码（仅使用训练集，绝不读取 valid/test 数据）：
          1) 全局非法候选：所有【未在训练中出现过】的物品（冷启动测试物品）
          2) 每用户已见集合：该用户在训练期间出现过的所有物品
        评估时：
          - 屏蔽(1)与(2)
          - 仅对当前样本的 ground-truth 解除屏蔽（允许其参与排名）
        """
        df = self.train_loader.dataset.data_df  # train.csv
        num_items = int(self.config.num_items)

        valid_items_set = set()
        user_seen = defaultdict(set)

        for _, row in df.iterrows():
            uid = int(row['user_id'])

            seq_str = row['sequence']
            if isinstance(seq_str, str):
                for tok in seq_str.split(','):
                    if not tok:
                        continue
                    it = int(tok)
                    if it > 0:
                        valid_items_set.add(it)
                        user_seen[uid].add(it)

            tgt = int(row['target_id'])
            if tgt > 0:
                valid_items_set.add(tgt)
                user_seen[uid].add(tgt)

        # 1D 全局掩码：True = 非法候选（未在训练集中出现过）
        valid_flag = torch.zeros(num_items, dtype=torch.bool)
        for it in valid_items_set:
            if 1 <= it <= num_items:
                valid_flag[it - 1] = True
        invalid_items_mask_1d = ~valid_flag
        self._invalid_items_mask_1d = invalid_items_mask_1d.to(self.device)

        # 每用户训练期已见（0-based 索引）
        max_uid = int(df['user_id'].max()) if len(df) > 0 else -1
        self._user_seen_idx = [torch.empty(0, dtype=torch.long, device=self.device) for _ in range(max_uid + 1)]
        for uid, items in user_seen.items():
            idxs = [it - 1 for it in items if 1 <= it <= num_items]
            if idxs:
                self._user_seen_idx[uid] = torch.as_tensor(sorted(set(idxs)), dtype=torch.long, device=self.device)

        self._log(f'[EvalProto] valid_items={len(valid_items_set)}, users={len(self._user_seen_idx)} ready.')

    # ========= KATRec KG 训练准备 =========
    def _setup_katrec_training(self):
        self.logger.info('Setting up data for KATRec KG training...')
        from src.datasets.kg_dataloader import get_kg_data
        kg_data = get_kg_data(self.config)
        kg_df = kg_data['kg_triples_df']
        self.kg_triples = kg_df[['head', 'relation', 'tail']].astype(int).values
        self.num_entities = kg_data['num_entities']
        self.kg_dict = defaultdict(list)
        for h, r, t in self.kg_triples:
            self.kg_dict[h, r].append(t)
        self.logger.info(f'Prepared {len(self.kg_triples)} triples for KG loss calculation.')

    def _sample_kg_batch(self, batch_size):
        indices = torch.randint(0, len(self.kg_triples), (batch_size,))
        batch_triples = self.kg_triples[indices]
        h, r, pos_t = (batch_triples[:, 0], batch_triples[:, 1], batch_triples[:, 2])
        neg_t = []
        for i in range(batch_size):
            head, rel = (h[i], r[i])
            tries = 0
            while True:
                neg_tail_candidate = random.randint(0, self.num_entities - 1)
                if neg_tail_candidate not in self.kg_dict[head, rel]:
                    neg_t.append(neg_tail_candidate)
                    break
                tries += 1
                if tries > 50:
                    neg_t.append(neg_tail_candidate)
                    break
        return (
            torch.LongTensor(h).to(self.device),
            torch.LongTensor(r).to(self.device),
            torch.LongTensor(pos_t).to(self.device),
            torch.LongTensor(neg_t).to(self.device)
        )

    def _log(self, message):
        print(message)
        self.logger.info(message)

    def _save_checkpoint(self, epoch, is_best=False):
        state = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_metric': self.best_metric,
            'config': self._safe_config_dict()
        }
        if is_best:
            best_filename = self.save_dir / 'best_model.pth.tar'
            torch.save(state, best_filename)
            self._log(f'-> Found new best model on validation set, saving to {best_filename}')
        else:
            pass

    def _save_results_to_csv(self, final_metrics, best_epoch, duration_str):
        results_dir = Path('./results')
        results_dir.mkdir(parents=True, exist_ok=True)
        results_file = results_dir / 'experiment_results.csv'

        results_data = {
            'timestamp': pd.to_datetime('now').strftime('%Y-%m-%d %H:%M:%S'),
            'model_name': self.config.model_name,
            'dataset_name': self.config.dataset_name,
            f'hit@{self.metrics_calculator.top_k}': final_metrics.get(f'Hit@{self.metrics_calculator.top_k}', -1),
            f'ndcg@{self.metrics_calculator.top_k}': final_metrics.get(f'NDCG@{self.metrics_calculator.top_k}', -1),
            'mrr': final_metrics.get('MRR', -1),
            'best_epoch': best_epoch,
            'total_epochs': self.config.epochs,
            'training_time': duration_str,
            'learning_rate': self.config.learning_rate,
            'batch_size': getattr(self.config, 'optimized_batch_size', self.config.batch_size),
            'embedding_dim': self.config.embedding_dim,
            'dropout_prob': self.config.dropout_prob,
            'window_size': self.config.window_size,
            'save_dir': str(self.save_dir)
        }
        df_results = pd.DataFrame([results_data])
        if not results_file.exists():
            df_results.to_csv(results_file, index=False, header=True)
        else:
            df_results.to_csv(results_file, mode='a', index=False, header=False)

    def _train_epoch(self, epoch):
        self.model.train()
        total_loss, total_rec_loss, total_kg_loss = (0.0, 0.0, 0.0)
        progress_bar = tqdm(self.train_loader, desc=f'Epoch {epoch + 1}/{self.config.epochs} [Training]')

        for batch in progress_bar:
            sequences = batch['sequence'].to(self.device, non_blocking=True)
            targets = batch['target_id'].to(self.device, non_blocking=True)
            targets_0 = torch.clamp_min(targets, 1) - 1

            self.optimizer.zero_grad(set_to_none=True)

            if self.use_amp:
                with torch.cuda.amp.autocast():
                    logits = self.model(sequences)
                    rec_loss = self.criterion(logits, targets_0)
                    if self.config.model_name.lower() == 'katrec':
                        h, r, pos_t, neg_t = self._sample_kg_batch(sequences.size(0))
                        kg_loss = self.model.calculate_kg_loss(h, r, pos_t, neg_t)
                        loss = rec_loss + self.config.kg_loss_lambda * kg_loss
                    else:
                        kg_loss = torch.tensor(0.0, device=self.device)
                        loss = rec_loss
                self.scaler.scale(loss).backward()
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.grad_clip)
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                logits = self.model(sequences)
                rec_loss = self.criterion(logits, targets_0)
                if self.config.model_name.lower() == 'katrec':
                    h, r, pos_t, neg_t = self._sample_kg_batch(sequences.size(0))
                    kg_loss = self.model.calculate_kg_loss(h, r, pos_t, neg_t)
                    loss = rec_loss + self.config.kg_loss_lambda * kg_loss
                else:
                    kg_loss = torch.tensor(0.0, device=self.device)
                    loss = rec_loss

                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.grad_clip)
                self.optimizer.step()

            total_loss += loss.item()
            total_rec_loss += rec_loss.item()
            total_kg_loss += kg_loss.item()

            postfix = {'loss': f'{loss.item():.4f}', 'rec': f'{rec_loss.item():.4f}'}
            if self.config.model_name.lower() == 'katrec':
                postfix['kg'] = f'{kg_loss.item():.4f}'
            progress_bar.set_postfix(postfix)

        avg_loss = total_loss / len(self.train_loader)
        avg_rec_loss = total_rec_loss / len(self.train_loader)
        avg_kg_loss = total_kg_loss / len(self.train_loader)
        log_msg = f'Epoch {epoch + 1} Training -> Avg Loss: {avg_loss:.4f} (Rec: {avg_rec_loss:.4f}'
        if self.config.model_name.lower() == 'katrec':
            log_msg += f', KG: {avg_kg_loss:.4f})'
        else:
            log_msg += ')'
        self._log(log_msg)

    def _evaluate(self, epoch, loader, phase='Validation'):
        if hasattr(self.model, 'precompute_kg'):
            try:
                self.model.precompute_kg(self.device)
            except Exception:
                pass

        self.model.eval()
        self.metrics_calculator.reset()
        progress_bar = tqdm(loader, desc=f'Epoch {epoch + 1}/{self.config.epochs} [{phase}]')

        with torch.inference_mode():
            for batch in progress_bar:
                sequences = batch['sequence'].to(self.device, non_blocking=True)
                targets = batch['target_id']  # CPU 张量
                targets_0 = torch.clamp_min(targets, 1) - 1

                logits = self.model(sequences)
                num_items = logits.size(1)

                # === “几乎全体物品”评估（严格防止泄露） ===
                # 1) 全局屏蔽：所有未在训练出现的物品（测试/验证专属冷启动物品）
                base_mask = self._invalid_items_mask_1d.view(1, -1).expand(sequences.size(0), -1).clone()

                # 2) 每用户再屏蔽：该用户训练期已见过的所有物品
                if hasattr(self, '_user_seen_idx') and self._user_seen_idx:
                    uids = batch['user_id'].tolist()  # 仍在 CPU，逐个索引即可
                    for i, uid in enumerate(uids):
                        if 0 <= uid < len(self._user_seen_idx):
                            seen_idx = self._user_seen_idx[uid]  # 已在 self.device
                            if seen_idx.numel() > 0:
                                base_mask[i, seen_idx] = True

                # 3) 仅解除当前样本的 ground-truth（允许其参与排序；除此之外不解除任何验证/测试物品）
                gt = targets_0.to(logits.device).long()
                valid = (gt >= 0) & (gt < num_items)
                if valid.any():
                    bs_idx = torch.arange(gt.size(0), device=logits.device)[valid]
                    base_mask[bs_idx, gt[valid]] = False

                # 4) 应用掩码
                logits = logits.masked_fill(base_mask, float('-inf')).detach().cpu()

                # 只统计 target > 0 的样本
                keep = targets > 0
                if keep.any():
                    self.metrics_calculator(logits[keep], (targets_0[keep]).to(torch.long))

        metrics = self.metrics_calculator.summary()
        k = self.metrics_calculator.top_k
        log_str = (
            f"Epoch {epoch + 1} {phase} Results -> "
            f"Hit@{k}: {metrics[f'Hit@{k}']:.4f}, "
            f"NDCG@{k}: {metrics[f'NDCG@{k}']:.4f}, "
            f"MRR: {metrics['MRR']:.4f}"
        )
        self._log(log_str)
        return metrics

    def fit(self):
        self._log('=' * 20 + ' Starting Training ' + '=' * 20)

        startup_cfg = getattr(self.config, '_startup_config', None)
        to_dump = startup_cfg if isinstance(startup_cfg, dict) else self._safe_config_dict()
        cfg_path = self.save_dir / 'config.yaml'
        with open(cfg_path, 'w', encoding='utf-8') as f:
            yaml.safe_dump(to_dump, f, default_flow_style=False, sort_keys=False, allow_unicode=True)

        start_time = time.time()
        best_epoch_num = 0
        try:
            for epoch in range(self.config.epochs):
                self._train_epoch(epoch)
                metrics = self._evaluate(epoch, loader=self.valid_loader, phase='Validation')
                current_metric = metrics[f'NDCG@{self.metrics_calculator.top_k}']

                if current_metric > self.best_metric:
                    self.best_metric = current_metric
                    self.epochs_no_improve = 0
                    best_epoch_num = epoch + 1
                    self._save_checkpoint(epoch, is_best=True)
                else:
                    self.epochs_no_improve += 1
                    self._log(f'-> No improvement in {self.epochs_no_improve} epochs.')
                    self._save_checkpoint(epoch, is_best=False)

                if self.epochs_no_improve >= self.config.patience:
                    self._log(f'Early stopping triggered after {self.config.patience} epochs.')
                    break
        finally:
            gc.collect()

        duration_str = time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))
        self._log(f'Total Training Time: {duration_str}')
        self._log('=' * 20 + ' Training Finished ' + '=' * 20)

        self._log('\n' + '=' * 20 + ' Final Evaluation on Test Set ' + '=' * 20)
        best_model_path = self.save_dir / 'best_model.pth.tar'
        if best_model_path.exists():
            checkpoint = torch.load(best_model_path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self._log(f'Loaded best model from epoch {best_epoch_num}')
            if hasattr(self.model, 'precompute_kg'):
                try:
                    self.model.precompute_kg(self.device)
                except Exception:
                    pass
            final_metrics = self._evaluate(checkpoint['epoch'], loader=self.test_loader, phase='Testing')
            self._save_results_to_csv(final_metrics, best_epoch_num, duration_str)
        else:
            self._log('No best model found to evaluate.')

============================================================
文件位置: src/utils/__init__.py
============================================================
# datasets package

============================================================
文件位置: src/utils/code_snapshot.py
============================================================
# src/utils/code_snapshot.py
from __future__ import annotations
import os, sys, subprocess
from pathlib import Path
from datetime import datetime
from typing import Iterable

DEFAULT_INCLUDE_EXTS = {".py", ".yaml", ".yml", ".sh", ".txt", ".md", ".cfg", ".ini"}
DEFAULT_PARTIAL_TEXT_EXTS = {".csv", ".tsv", ".json", ".log", ".inter", ".kg", ".link"}
DEFAULT_EXCLUDE_DIRS = {
    ".git", ".idea", ".vscode", "__pycache__", ".pytest_cache",
    ".mypy_cache", "dataset", "saved", "results", "wandb", "logs", ".DS_Store"
}
DEFAULT_BINARY_EXTS = {".pth", ".tar", ".pkl", ".npy", ".npz", ".pyc", ".so", ".dll", ".exe", ".bin", ".dat"}

def _is_binary_path(p: Path) -> bool:
    suf = "".join(p.suffixes).lower()
    if suf.endswith(".pth.tar") or suf.endswith(".pkl.gz"):
        return True
    return p.suffix.lower() in DEFAULT_BINARY_EXTS

def _try_read_bytes(p: Path):
    try: return p.read_bytes()
    except Exception: return None

def _decode_text(b: bytes) -> str:
    for enc in ("utf-8","utf-16","gbk","gb2312","latin-1"):
        try: return b.decode(enc)
        except Exception: pass
    return b.decode("utf-8", errors="replace")

def _iter_project_files(root: Path, include_exts, partial_text_exts, exclude_dirs) -> Iterable[Path]:
    for dirpath, dirnames, filenames in os.walk(root):
        dirnames[:] = sorted([d for d in dirnames if d not in exclude_dirs and not d.startswith(".") or d=="configs"], key=str.lower)
        for fname in sorted(filenames, key=str.lower):
            p = Path(dirpath)/fname
            if p.name=="main.py" or ("configs" in p.parts):
                yield p; continue
            suf = p.suffix.lower()
            if suf in include_exts or suf in partial_text_exts: yield p

def _build_tree(root: Path, exclude_dirs) -> str:
    lines = ["文件夹结构:", f"└── {root.name or '/'}"]
    def kids(path: Path):
        try:
            d=[c for c in path.iterdir() if c.is_dir() and c.name not in exclude_dirs and not c.name.startswith(".")]
            f=[c for c in path.iterdir() if c.is_file()]
            return sorted(d,key=lambda x:x.name.lower()), sorted(f,key=lambda x:x.name.lower())
        except PermissionError:
            return [],[]
    def walk(path: Path, prefix: str):
        d,f = kids(path); all_=d+f
        for i,ch in enumerate(all_):
            last = i==len(all_)-1
            lines.append(f"{prefix}{'└── ' if last else '├── '}{ch.name}")
            if ch.is_dir():
                walk(ch, prefix + ("    " if last else "│   "))
    walk(root,"    ")
    return "\n".join(lines)

def _gather_env_info(project_root: Path) -> str:
    info=[ "环境信息:",
           f"- 生成时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
           f"- Python: {sys.version.split()[0]} on {sys.platform}" ]
    try:
        out = subprocess.check_output([sys.executable,"-m","pip","freeze"], stderr=subprocess.STDOUT, timeout=10)
        info += ["- pip freeze (partial):", _decode_text(out)[:5000].strip()]
    except Exception: info.append("- pip freeze: N/A")
    try:
        commit = subprocess.check_output(["git","rev-parse","HEAD"], cwd=project_root, stderr=subprocess.STDOUT, timeout=3)
        status = subprocess.check_output(["git","status","--porcelain"], cwd=project_root, stderr=subprocess.STDOUT, timeout=3)
        info.append(f"- git commit: {_decode_text(commit).strip()}")
        info.append(f"- git status: {'clean' if not _decode_text(status).strip() else 'dirty'}")
    except Exception: info.append("- git: N/A")
    try:
        out = subprocess.check_output(["nvidia-smi","-L"], stderr=subprocess.STDOUT, timeout=2)
        info.append("- GPU: " + _decode_text(out).strip())
    except Exception: info.append("- GPU: N/A")
    return "\n".join(info)

def write_code_snapshot(
    save_dir: Path,
    project_root: Path|None=None,
    include_exts=None,
    partial_text_exts=None,
    exclude_dirs=None,
    max_bytes_per_file: int = 1_000_000,
    partial_head_lines: int = 50,
) -> Path:
    save_dir = Path(save_dir); save_dir.mkdir(parents=True, exist_ok=True)
    out_path = save_dir/"code.txt"
    project_root = Path(project_root) if project_root else Path.cwd()
    include_exts = include_exts or DEFAULT_INCLUDE_EXTS
    partial_text_exts = partial_text_exts or DEFAULT_PARTIAL_TEXT_EXTS
    exclude_dirs = exclude_dirs or DEFAULT_EXCLUDE_DIRS

    files = sorted(_iter_project_files(project_root, include_exts, partial_text_exts, exclude_dirs),
                   key=lambda p: "/".join(p.relative_to(project_root).parts).lower())
    tree_text = _build_tree(project_root, exclude_dirs)
    env_text = _gather_env_info(project_root)

    with out_path.open("w", encoding="utf-8", errors="replace") as f:
        f.write(f"分析文件夹: {project_root.resolve()}\n")
        f.write(f"输出文件: {out_path.name}\n")
        f.write(f"分析时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("="*60 + "\n\n")
        f.write(env_text + "\n")
        f.write("\n" + "="*60 + "\n")
        f.write(tree_text + "\n")
        f.write("\n" + "="*60 + "\n")
        f.write("文件内容分析:\n")
        f.write("="*60 + "\n")

        for fp in files:
            if _is_binary_path(fp): continue
            rel = fp.relative_to(project_root)
            f.write("\n" + "="*60 + "\n")
            f.write(f"文件位置: {rel.as_posix()}\n")
            f.write("="*60 + "\n")

            raw = _try_read_bytes(fp)
            if raw is None:
                f.write("无法读取文件内容\n"); continue

            truncated = False
            if len(raw) > max_bytes_per_file:
                raw = raw[:max_bytes_per_file]; truncated = True

            text = _decode_text(raw)
            suf = fp.suffix.lower()
            if suf in include_exts:
                if truncated: f.write("[TRUNCATED]\n")
                f.write(text + "\n")
            else:
                lines = text.splitlines()
                for i, ln in enumerate(lines[:partial_head_lines], 1):
                    f.write(f"{i:3d}: {ln}\n")
                if len(lines) > partial_head_lines:
                    f.write(f"... (文件共 {len(lines)} 行，仅显示前{partial_head_lines}行)\n")

        f.write("\n" + "="*60 + "\n")
        f.write("分析完成!\n")
        f.write(f"结果已保存到: {out_path.name}\n")
    return out_path

============================================================
文件位置: src/utils/gpu_utils.py
============================================================
# src/utils/gpu_utils.py
from __future__ import annotations
import os
import math
from typing import List, Dict, Optional, Tuple
import torch

__all__ = ["get_gpu_memory", "select_device", "describe_devices"]

def _try_init_nvml():
    """尝试初始化 NVML，成功则返回 pynvml 模块，否则返回 None。"""
    try:
        import pynvml  # type: ignore
        pynvml.nvmlInit()
        return pynvml
    except Exception:
        return None

def _shutdown_nvml(pynvml_mod) -> None:
    try:
        if pynvml_mod is not None:
            pynvml_mod.nvmlShutdown()
    except Exception:
        pass

def _parse_visible_device_indices() -> Optional[List[int]]:
    """
    解析 CUDA_VISIBLE_DEVICES，返回可见的物理 GPU 索引列表（整数）。
    若未设置或不可解析，返回 None。
    """
    cvd = os.environ.get("CUDA_VISIBLE_DEVICES", "").strip()
    if not cvd:
        return None
    parts = [p.strip() for p in cvd.split(",") if p.strip() != ""]
    idxs: List[int] = []
    for p in parts:
        # 仅支持数字索引（不处理 UUID/MIG 的复杂场景）
        if p.isdigit():
            idxs.append(int(p))
        else:
            return None
    return idxs if idxs else None

def get_gpu_memory() -> List[Dict[str, float]]:
    """
    返回当前“可见” GPU 的资源/负载信息列表（按 PyTorch 可见索引编号）：
    [
      {
        'id': torch 可见索引（int）,
        'total': bytes,
        'used': bytes,
        'free': bytes,
        'util': used/total（显存占用比例，0~1，向后兼容）,
        'gpu_util_pct': Volatile GPU-Util（0~100，若 NVML 不可用则为 None）,
        'mem_ctrl_util_pct': 显存控制器利用率（0~100，若 NVML 不可用则为 None）
      }, ...
    ]
    若无 CUDA 或无可见 GPU，返回 []。
    """
    if not torch.cuda.is_available():
        return []

    infos: List[Dict[str, float]] = []
    pynvml = _try_init_nvml()
    try:
        visible_phys = _parse_visible_device_indices()  # 物理索引列表或 None
        if pynvml is not None:
            if visible_phys is None:
                # 未设置 CVD，则假定 NVML 顺序 == PyTorch 顺序
                phys_indices = list(range(pynvml.nvmlDeviceGetCount()))
                visible_map = list(range(len(phys_indices)))  # torch 可见索引
            else:
                phys_indices = visible_phys
                visible_map = list(range(len(phys_indices)))  # 0..N-1 映射为 torch 可见索引

            # 限制到 PyTorch 实际可见数量，避免 cuda:<id> 超界
            torch_visible_cnt = torch.cuda.device_count()
            phys_indices = phys_indices[:torch_visible_cnt]
            visible_map = visible_map[:torch_visible_cnt]

            for torch_idx, phys_idx in zip(visible_map, phys_indices):
                handle = pynvml.nvmlDeviceGetHandleByIndex(phys_idx)
                mem = pynvml.nvmlDeviceGetMemoryInfo(handle)
                total = float(mem.total)
                used = float(mem.used)
                free = float(mem.free)
                util_ratio = (used / total) if total > 0 else 1.0

                # Volatile Utilization（过去一个采样窗口的平均）
                try:
                    ur = pynvml.nvmlDeviceGetUtilizationRates(handle)
                    gpu_util_pct = float(ur.gpu)           # SM/核心利用率
                    mem_ctrl_util_pct = float(ur.memory)   # 显存控制器
                except Exception:
                    gpu_util_pct = float("nan")
                    mem_ctrl_util_pct = float("nan")

                infos.append({
                    "id": torch_idx,
                    "total": total,
                    "used": used,
                    "free": free,
                    "util": util_ratio,
                    "gpu_util_pct": gpu_util_pct,
                    "mem_ctrl_util_pct": mem_ctrl_util_pct,
                })
        else:
            # 无 NVML：退化到“显存信息 + 显存占用比例”
            count = torch.cuda.device_count()
            for i in range(count):
                props = torch.cuda.get_device_properties(i)
                total = float(props.total_memory)
                try:
                    reserved = float(torch.cuda.memory_reserved(i))
                except Exception:
                    reserved = 0.0
                used = reserved
                free = max(0.0, total - reserved)
                util_ratio = (used / total) if total > 0 else 1.0
                infos.append({
                    "id": i,
                    "total": total,
                    "used": used,
                    "free": free,
                    "util": util_ratio,
                    "gpu_util_pct": float("nan"),
                    "mem_ctrl_util_pct": float("nan"),
                })
    finally:
        _shutdown_nvml(pynvml)
    return infos

def _gpu_util_for_sort(g: Dict[str, float]) -> float:
    """用于排序的 GPU 利用率值；NVML 不可用时退化为显存占用比例（*100）。"""
    v = g.get("gpu_util_pct", float("nan"))
    if isinstance(v, float) and math.isfinite(v):
        return v
    # 退化：用显存占用比例推测忙碌程度
    return float(g.get("util", 1.0)) * 100.0

def select_device(
    prefer: str = "auto",
    strategy: str = "min_gpu_util",  # 默认改为挑“最闲”的
    min_free_mem_gb: float = 1.0,
    allow_cpu: bool = True,
    explicit_id: Optional[int] = None,
    verbose: bool = True,
) -> torch.device:
    """
    选择最合适的计算设备：
    - prefer: 'auto' | 'cuda' | 'cpu'
    - strategy:
        'max_free'     -> 空闲显存最大的卡
        'min_util'     -> 显存占用比例最低（向后兼容）
        'min_gpu_util' -> Volatile GPU-Util 最低（NVML），失败则退化到 'min_util'
    - min_free_mem_gb: 至少需要的空闲显存（不足则放宽）
    - explicit_id: 显式指定 GPU id（torch 可见索引，优先级最高）
    - allow_cpu: 没有可用 GPU 时是否回退到 CPU
    返回 torch.device('cuda:{id}') 或 torch.device('cpu')
    """
    if prefer == "cpu" or not torch.cuda.is_available():
        if verbose:
            print("[GPU-Select] Using CPU (prefer=cpu or CUDA unavailable).")
        return torch.device("cpu")

    if explicit_id is not None:
        if verbose:
            print(f"[GPU-Select] Using explicit cuda:{explicit_id}.")
        return torch.device(f"cuda:{explicit_id}")

    gpus = get_gpu_memory()
    if not gpus:
        if allow_cpu:
            if verbose:
                print("[GPU-Select] No visible GPUs. Falling back to CPU.")
            return torch.device("cpu")
        return torch.device("cuda:0")

    min_free_bytes = int(min_free_mem_gb * (1024 ** 3))
    candidates = [g for g in gpus if g["free"] >= min_free_bytes] or gpus

    strat = strategy.lower()
    if strat in ("max_free", "max_free_mem"):
        best = max(candidates, key=lambda g: g["free"])
        strat_name = "max_free"
    elif strat in ("min_gpu_util", "min_volatile", "least_busy"):
        best = min(candidates, key=_gpu_util_for_sort)
        strat_name = "min_gpu_util"
    else:
        best = min(candidates, key=lambda g: g.get("util", 1.0))
        strat_name = "min_util"

    device = torch.device(f"cuda:{int(best['id'])}")
    if verbose:
        tot = best["total"] / 1024**3
        fre = best["free"] / 1024**3
        usd = best["used"] / 1024**3
        gpuu = best.get("gpu_util_pct", float("nan"))
        if math.isfinite(gpuu):
            extra = f", gpu_util={gpuu:.0f}%"
        else:
            extra = ""
        print(f"[GPU-Select] Strategy={strat_name} -> pick cuda:{int(best['id'])} "
              f"(free={fre:.2f}GB, used={usd:.2f}GB, total={tot:.2f}GB{extra}).")
    return device

def describe_devices() -> str:
    """返回一段可打印字符串，描述当前可见 GPU 负载/显存状况。"""
    infos = get_gpu_memory()
    if not infos:
        return "No CUDA GPUs visible."
    lines = ["Visible GPUs:"]
    for g in infos:
        free_gb = g['free']/1024**3
        tot_gb = g['total']/1024**3
        used_gb = g['used']/1024**3
        util_pct = g.get("gpu_util_pct", float("nan"))
        memc_pct = g.get("mem_ctrl_util_pct", float("nan"))
        util_str = f"{util_pct:.0f}%" if math.isfinite(util_pct) else "N/A"
        memc_str = f"{memc_pct:.0f}%" if math.isfinite(memc_pct) else "N/A"
        lines.append(
            f"  cuda:{int(g['id'])}: free {free_gb:.2f}GB / total {tot_gb:.2f}GB "
            f"(used {used_gb:.2f}GB), GPU-Util {util_str}, MemCtrl {memc_str}"
        )
    return "\n".join(lines)

============================================================
文件位置: src/utils/logging_utils.py
============================================================
# src/utils/logging_utils.py

import logging
from pathlib import Path
from typing import Optional

def setup_logging(level: str = "INFO", file_path: Optional[Path] = None, name: str = "train"):
    """
    设置日志记录器，兼容 Python 3.9 及以下。
    - **修复**：避免重复添加 handlers 导致日志重复。
    """
    logger = logging.getLogger(name)

    # 清理旧的 handlers，防止重复输出
    if logger.handlers:
        for h in list(logger.handlers):
            logger.removeHandler(h)

    logger.propagate = False
    logger.setLevel(getattr(logging, level.upper(), logging.INFO))

    formatter = logging.Formatter(
        fmt="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )

    # 控制台输出
    ch = logging.StreamHandler()
    ch.setFormatter(formatter)
    logger.addHandler(ch)

    # 文件输出（可选）
    if file_path is not None:
        fh = logging.FileHandler(file_path)
        fh.setFormatter(formatter)
        logger.addHandler(fh)

    return logger

============================================================
文件位置: src/utils/memory_monitor.py
============================================================
# src/utils/memory_monitor.py

import psutil
import torch
import os
import gc
from pathlib import Path

class MemoryMonitor:
    """轻量内存监控与建议/自适应调参（保持对外 API 不变）"""
    def __init__(self, config):
        self.config = config
        self.initial_memory = self.get_memory_usage()

    def get_memory_usage(self):
        mem = psutil.virtual_memory()
        info = {
            'total_gb': mem.total / (1024**3),
            'available_gb': mem.available / (1024**3),
            'used_gb': mem.used / (1024**3),
            'percent': mem.percent
        }
        if torch.cuda.is_available():
            total = torch.cuda.get_device_properties(0).total_memory
            info.update({
                'gpu_total_gb': total / (1024**3),
                'gpu_allocated_gb': torch.cuda.memory_allocated(0) / (1024**3),
                'gpu_cached_gb': torch.cuda.memory_reserved(0) / (1024**3),
                'gpu_free_gb': (total - torch.cuda.memory_reserved(0)) / (1024**3)
            })
        return info

    def print_memory_stats(self, stage=""):
        m = self.get_memory_usage()
        print(f"\n=== 内存使用 {stage} ===")
        print(f"系统: {m['used_gb']:.2f}/{m['total_gb']:.2f}GB ({m['percent']:.1f}%), 可用 {m['available_gb']:.2f}GB")
        if 'gpu_total_gb' in m:
            print(f"GPU: alloc {m['gpu_allocated_gb']:.2f}GB, cached {m['gpu_cached_gb']:.2f}GB, "
                  f"free {m['gpu_free_gb']:.2f}/{m['gpu_total_gb']:.2f}GB")

    def suggest_config_adjustments(self):
        m = self.get_memory_usage()
        tips = []
        if m['percent'] > 80:
            cur_bs = getattr(self.config, 'batch_size', 256)
            tips += [
                "系统内存紧张，建议：",
                f"- 减少 batch_size（当前 {cur_bs}）",
                "- 将 num_workers 调低为 0~1",
                "- 关闭 pin_memory",
                f"- 建议 batch_size ≈ {max(16, cur_bs // 2)}"
            ]
        if 'gpu_free_gb' in m and m['gpu_free_gb'] < 2.0:
            tips += [
                "GPU 内存紧张，建议：",
                "- 进一步减少 batch_size",
                "- 降低 embedding_dim",
                "- 使用 AMP 或梯度累积"
            ]
        return tips or ["内存使用正常"]

    def auto_adjust_config(self):
        m = self.get_memory_usage()
        # 系统内存
        if m['percent'] > 80 and hasattr(self.config, 'batch_size'):
            orig = self.config.batch_size
            self.config.optimized_batch_size = max(16, orig // 2)
            self.config.num_workers = min(getattr(self.config, 'num_workers', 0), 1)
            print(f"自动调整: batch_size {orig} -> {self.config.optimized_batch_size}, num_workers -> {self.config.num_workers}")
        # GPU 内存
        if 'gpu_free_gb' in m and m['gpu_free_gb'] < 2.0:
            cur = getattr(self.config, 'optimized_batch_size', getattr(self.config, 'batch_size', 256))
            self.config.optimized_batch_size = max(8, cur // 2)
            print(f"GPU 紧张: 进一步减少 batch_size -> {self.config.optimized_batch_size}")

def setup_memory_efficient_environment():
    """设置简洁的内存友好环境变量"""
    torch.set_num_threads(min(4, os.cpu_count() or 4))
    if torch.cuda.is_available():
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
        torch.cuda.empty_cache()
    os.environ['OMP_NUM_THREADS'] = '2'
    os.environ['MKL_NUM_THREADS'] = '2'
    print("已应用内存优化环境变量")

def monitor_training_memory(model, train_loader, device, max_batches=5):
    """快速批次内显存观测，不影响训练逻辑"""
    print("\n=== 训练内存监控 ===")
    model.eval()
    with torch.no_grad():
        for i, batch in enumerate(train_loader):
            if i >= max_batches:
                break
            if torch.cuda.is_available():
                torch.cuda.synchronize()
                before = torch.cuda.memory_allocated() / 1024**3
            seq = batch['sequence'].to(device, non_blocking=True)
            tgt = batch['target_id'].to(device, non_blocking=True)
            _ = model(seq)
            if torch.cuda.is_available():
                torch.cuda.synchronize()
                after = torch.cuda.memory_allocated() / 1024**3
                print(f"Batch {i+1}: ΔGPU {after - before:.3f}GB")
            del seq, tgt
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    print("内存监控完成")

============================================================
文件位置: src/utils/metrics.py
============================================================
# src/utils/metrics.py

import torch

class AllRankMetrics:
    """
    Calculates all-ranking metrics (Hit@K, NDCG@K, MRR) by accumulating
    statistics batch by batch, avoiding high memory usage.
    The evaluation protocol is to rank the ground truth item against all other items.
    """
    def __init__(self, top_k=10):
        self.top_k = top_k
        # 初始化统计累加值，而不是列表
        self.reset()

    def reset(self):
        """Resets the internal accumulators."""
        self._total_hits = 0.0
        self._total_ndcgs = 0.0
        self._total_mrrs = 0.0
        self._count = 0

    def __call__(self, logits, targets):
        """
        Updates metrics with a new batch of predictions and targets.
        It calculates metrics for the batch and adds them to the running totals.

        Args:
            logits (torch.Tensor): The model output scores. Shape: (batch_size, num_items)
            targets (torch.Tensor): The ground truth item ids. Shape: (batch_size,)
        """
        # 确保 targets 和 logits 在同一设备上进行比较
        targets = targets.to(logits.device)

        # 获取每个样本中目标物品的分数
        target_scores = logits.gather(1, targets.view(-1, 1))

        # 计算目标物品的排名 (有多少个物品的分数比目标高 + 1)
        ranks = (logits > target_scores).sum(dim=1) + 1
        
        # --- 逐批次计算指标并累加 ---
        
        # Hit Rate @ K: 排名是否在前 K
        hits_at_k = (ranks <= self.top_k).float()
        
        # NDCG @ K: DCG = 1 / log2(rank + 1) for ranks in top K. IDCG is 1.
        in_top_k_mask = (ranks <= self.top_k).float()
        ndcg_at_k = in_top_k_mask * (1.0 / torch.log2(ranks.float() + 1))

        # Mean Reciprocal Rank (MRR)
        mrr = 1.0 / ranks.float()

        # 累加当前批次的结果到总和中
        # 使用 .sum().item() 将批次总和转换为Python标量，避免张量累积
        self._total_hits += hits_at_k.sum().item()
        self._total_ndcgs += ndcg_at_k.sum().item()
        self._total_mrrs += mrr.sum().item()
        self._count += targets.size(0)

    def summary(self):
        """
        Calculates and returns the final average metrics based on accumulated stats.
        
        Returns:
            dict: A dictionary containing the average Hit@K, NDCG@K, and MRR.
        """
        if self._count == 0:
            return {f'Hit@{self.top_k}': 0, f'NDCG@{self.top_k}': 0, 'MRR': 0}

        # 计算最终的平均值
        avg_hit = self._total_hits / self._count
        avg_ndcg = self._total_ndcgs / self._count
        avg_mrr = self._total_mrrs / self._count
        
        return {
            f'Hit@{self.top_k}': avg_hit,
            f'NDCG@{self.top_k}': avg_ndcg,
            'MRR': avg_mrr
        }

============================================================
文件位置: src/utils/path_finder.py
============================================================
from __future__ import annotations
from dataclasses import dataclass
from pathlib import Path
from typing import NamedTuple, Optional, Tuple

class DataPaths(NamedTuple):
    base_processed: Path
    split_dir: Path
    train_csv: Path
    valid_csv: Path
    test_csv: Path
    processed_link: Path
    processed_user: Path
    raw_inter: Path
    raw_link: Path
    raw_item: Path
    raw_kg: Path

@dataclass(frozen=True)
class PathFinder:
    dataset_name: str
    window_size: int
    project_root: Optional[Path] = None

    def root(self) -> Path:
        if self.project_root is not None:
            return self.project_root.resolve()
        return Path('.').resolve()

    def data_dir(self) -> Path:
        return self.root() / 'dataset'

    def raw_dir(self) -> Path:
        return self.data_dir() / 'raw' / self.dataset_name

    def processed_dir(self) -> Path:
        return self.data_dir() / 'processed' / self.dataset_name

    def split_dir_path(self) -> Path:
        return self.processed_dir() / f'L_{self.window_size}'

    def raw_inter_path(self) -> Path:
        return self.raw_dir() / f'{self.dataset_name}.inter'

    def raw_link_path(self) -> Path:
        return self.raw_dir() / f'{self.dataset_name}.link'

    def raw_item_path(self) -> Path:
        return self.raw_dir() / f'{self.dataset_name}.item'

    def raw_kg_path(self) -> Path:
        return self.raw_dir() / f'{self.dataset_name}.kg'

    # 新增：处理后产物路径
    def processed_link_path(self) -> Path:
        return self.processed_dir() / f'{self.dataset_name}.link'

    def processed_user_path(self) -> Path:
        return self.processed_dir() / f'{self.dataset_name}.user'

    def train_valid_test_paths(
        self,
        use_corrected_train: bool=False,
        corrected_train_filename: str='corrected_train.csv'
    ) -> Tuple[Path, Path, Path]:
        split = self.split_dir_path()
        corrected = split / corrected_train_filename
        train_default = split / 'train.csv'
        train = corrected if use_corrected_train and corrected.exists() else train_default
        return (train, split / 'valid.csv', split / 'test.csv')

    def data_paths(
        self,
        use_corrected_train: bool=False,
        corrected_train_filename: str='corrected_train.csv',
        ensure_dirs: bool=True
    ) -> DataPaths:
        base_processed = self.processed_dir()
        split_dir = self.split_dir_path()
        if ensure_dirs:
            base_processed.mkdir(parents=True, exist_ok=True)
            split_dir.mkdir(parents=True, exist_ok=True)
        (train_csv, valid_csv, test_csv) = self.train_valid_test_paths(
            use_corrected_train=use_corrected_train,
            corrected_train_filename=corrected_train_filename
        )
        return DataPaths(
            base_processed=base_processed,
            split_dir=split_dir,
            train_csv=train_csv,
            valid_csv=valid_csv,
            test_csv=test_csv,
            processed_link=self.processed_link_path(),
            processed_user=self.processed_user_path(),
            raw_inter=self.raw_inter_path(),
            raw_link=self.raw_link_path(),
            raw_item=self.raw_item_path(),
            raw_kg=self.raw_kg_path()
        )

    def save_dir(self, model_name: str) -> Path:
        out = Path('./saved') / self.dataset_name / model_name / f'L_{self.window_size}'
        out.mkdir(parents=True, exist_ok=True)
        return out

============================================================
分析完成!
结果已保存到: code.txt
